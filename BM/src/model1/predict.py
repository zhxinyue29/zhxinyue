#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
from pathlib import Path
from datetime import datetime
import logging
import json

import torch
import pandas as pd
import yaml

# =========================
# å¯è‡ªå®šä¹‰çš„é»˜è®¤é…ç½®
# =========================
DEFAULT_CONFIG = "/home/liyakun/twitter-stock-prediction/configs/model1.yaml"
# å¦‚æœä½ å¸Œæœ›ç›´æ¥åœ¨è„šæœ¬é‡Œç»™å‡ æ¡ç¤ºä¾‹æ–‡æœ¬ï¼Œå°±å¡«åˆ°è¿™é‡Œï¼›å¦åˆ™è®¾ä¸º Noneï¼Œä¼šè‡ªåŠ¨ä» test_data_path è¯»å–
SAMPLE_TEXTS = None
# ä»æµ‹è¯•é›†å–å¤šå°‘æ¡åšæ¼”ç¤ºï¼ˆå½“ SAMPLE_TEXTS=None æ—¶ç”Ÿæ•ˆï¼‰
TEST_TAKE_N = 5
# è¾“å‡ºç»“æœä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
DEFAULT_OUTPUT_JSON = "/home/liyakun/twitter-stock-prediction/results/model1_predict_preview.json"

# =========================
# ç®€å•æ—¥å¿—
# =========================
logger = logging.getLogger("predict")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(ch)

# =========================
# ä½ ä¹‹å‰ç»™çš„æ¨¡å‹ä¸€å ä½ç±»
# =========================
class DeepSeekTextModel(torch.nn.Module):
    def __init__(self, model_weights_path: str, device='cuda'):
        super().__init__()
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        # ç¤ºä¾‹ï¼šç®€å•çº¿æ€§å±‚æ›¿ä»£çœŸå®æ¨¡å‹
        self.model = torch.nn.Linear(100, 50).to(self.device)
        # åŠ è½½æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if model_weights_path and Path(model_weights_path).exists():
            try:
                state_dict = torch.load(model_weights_path, map_location=self.device)
                # å…¼å®¹åªå­˜äº† state_dict æˆ–æ•´ä¸ª checkpoint çš„æƒ…å†µ
                if isinstance(state_dict, dict) and "state_dict" in state_dict:
                    state_dict = state_dict["state_dict"]
                self.model.load_state_dict(state_dict, strict=False)
                logger.info(f"âœ… æ¨¡å‹ä¸€æƒé‡å·²åŠ è½½: {model_weights_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½æƒé‡å¤±è´¥ï¼ˆå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰: {model_weights_path} | {e}")
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹æƒé‡ä¸å­˜åœ¨ï¼ˆå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰: {model_weights_path}")

    def forward(self, x):
        return self.model(x)

    def generate_text(self, input_tensor):
        """
        ç”Ÿæˆå¯è¯»æ–‡æœ¬è¾“å‡ºï¼ˆæ¼”ç¤ºç”¨ï¼‰
        """
        summaries = []
        for row in input_tensor:
            text = f"çŸ©é˜µè¡Œå’Œ: {row.sum().item():.4f}"
            summaries.append(text)
        return summaries


# =========================
# å·¥å…·å‡½æ•°
# =========================
def load_config(config_path: str) -> dict:
    cfg_path = Path(config_path)
    if not cfg_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg or {}

def pick_model1_weights_from_config(cfg: dict) -> str:
    # å…¼å®¹ä¸¤ç§å¸¸è§å†™æ³•ï¼špaths.output_model æˆ– models.model1_path
    paths = cfg.get("paths", {})
    if "output_model" in paths:
        return paths["output_model"]
    models = cfg.get("models", {})
    if "model1_path" in models:
        return models["model1_path"]
    raise KeyError("æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°æ¨¡å‹ä¸€æƒé‡è·¯å¾„ï¼ˆæœŸæœ› paths.output_model æˆ– models.model1_pathï¼‰ã€‚")

def pick_test_csv_from_config(cfg: dict) -> str:
    paths = cfg.get("paths", {})
    # å…¼å®¹ test_data_path / test_dir / {"test":[...]}
    if "test_data_path" in paths:
        return paths["test_data_path"]
    if "test_dir" in paths:
        test_src = paths["test_dir"]
        if isinstance(test_src, str):
            return test_src
        if isinstance(test_src, dict) and "test" in test_src and test_src["test"]:
            return test_src["test"][0]
        if isinstance(test_src, (list, tuple)) and len(test_src) > 0:
            return test_src[0]
    raise KeyError("æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆæœŸæœ› paths.test_data_path æˆ– paths.test_dirï¼‰ã€‚")

def take_texts_from_csv(csv_path: str, n: int = 5) -> list:
    df = pd.read_csv(csv_path)
    # å…¼å®¹åˆ—å
    cand_cols = ["text", "tweet", "content", "message", "body"]
    text_col = None
    for c in cand_cols:
        if c in df.columns:
            text_col = c
            break
    if text_col is None:
        raise ValueError(f"æµ‹è¯•CSVä¸­æ‰¾ä¸åˆ°æ–‡æœ¬åˆ—ï¼ˆå°è¯• {cand_cols}ï¼‰ã€‚ç°æœ‰åˆ—={list(df.columns)}")
    texts = df[text_col].astype(str).fillna("").tolist()
    texts = [t for t in texts if t.strip()]
    if not texts:
        raise ValueError("æµ‹è¯•CSVä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ã€‚")
    return texts[:n]

def texts_to_embeddings(texts: list, device: torch.device, dim: int = 100) -> torch.Tensor:
    """
    âš ï¸ æ¼”ç¤ºç”¨ï¼šæŠŠæ–‡æœ¬â€œä¼ªè£…â€ä¸ºå‘é‡ã€‚
    çœŸå®æƒ…å†µè¯·ç”¨ tokenizer + model.encode() å¾—åˆ°å‘é‡ã€‚
    """
    torch.manual_seed(42)
    x = torch.randn(len(texts), dim, device=device)
    return x

def save_preview(results: list, out_path: str):
    os.makedirs(Path(out_path).parent, exist_ok=True)
    payload = []
    for r in results:
        row = {
            "input_text": r["input_text"],
            "output_text": r["output_text"],
            # çŸ©é˜µå¤§å¯¹è±¡ä¸ç›´æ¥å…¨é‡å†™å…¥ï¼Œç»™ä¸ªæ‘˜è¦
            "output_matrix_shape": list(r["output_matrix"].shape),
            "output_matrix_sum": float(r["output_matrix"].sum().item()),
        }
        payload.append(row)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“ ç»“æœé¢„è§ˆå·²ä¿å­˜: {out_path}")


# =========================
# ä¸»æµç¨‹ï¼ˆæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼‰
# =========================
def main():
    config_path = DEFAULT_CONFIG
    logger.info(f"ä½¿ç”¨é…ç½®: {config_path}")

    cfg = load_config(config_path)
    weights_path = pick_model1_weights_from_config(cfg)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"è®¾å¤‡: {device}")

    # 1) æ„é€ è¾“å…¥æ–‡æœ¬
    if SAMPLE_TEXTS and len(SAMPLE_TEXTS) > 0:
        input_texts = SAMPLE_TEXTS
        logger.info(f"ä½¿ç”¨è„šæœ¬å†…ç½® SAMPLE_TEXTSï¼Œå…± {len(input_texts)} æ¡")
    else:
        test_csv = pick_test_csv_from_config(cfg)
        logger.info(f"ä»æµ‹è¯•é›†è¯»å–æ–‡æœ¬: {test_csv}ï¼ˆå–å‰ {TEST_TAKE_N} æ¡ï¼‰")
        input_texts = take_texts_from_csv(test_csv, n=TEST_TAKE_N)

    # 2) åˆå§‹åŒ–æ¨¡å‹ä¸€
    model1 = DeepSeekTextModel(weights_path, device=device)
    model1.eval()

    # 3) æ–‡æœ¬ -> å‘é‡ï¼ˆæ¼”ç¤ºï¼‰
    inputs = texts_to_embeddings(input_texts, device=device, dim=100)

    # 4) å‰å‘å¾—åˆ°â€œçŸ©é˜µè¡¨ç¤ºâ€ & ç”Ÿæˆå¯è¯»æ–‡æœ¬
    results = []
    with torch.no_grad():
        output_matrix = model1(inputs)                      # [B, 50]
        output_texts = model1.generate_text(output_matrix)  # list[str]

        for text, mat, txt in zip(input_texts, output_matrix, output_texts):
            results.append({
                "input_text": text,
                "output_matrix": mat.detach().cpu(),
                "output_text": txt
            })
            # ç®€è¦æ‰“å°
            print("â€”â€”â€”")
            print(f"è¾“å…¥: {text[:120]}{'...' if len(text) > 120 else ''}")
            print(f"è¾“å‡ºçŸ©é˜µå½¢çŠ¶: {tuple(mat.shape)} | è¡Œå’Œæ‘˜è¦æ–‡æœ¬: {txt}")

    # 5) å¯é€‰ä¿å­˜ä¸€ä¸ªé¢„è§ˆjsonï¼ˆä¸ä¿å­˜å¤§çŸ©é˜µæœ¬ä½“ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
    try:
        save_preview(results, DEFAULT_OUTPUT_JSON)
    except Exception as e:
        logger.warning(f"ç»“æœé¢„è§ˆä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")

    logger.info("âœ… é¢„æµ‹å®Œæˆ")
    return results


if __name__ == "__main__":
    main()
