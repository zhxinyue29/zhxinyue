from torch.utils.data import DataLoader
import os
import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime

import yaml
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset

# è§£å†³ import è·¯å¾„
THIS_FILE = Path(__file__).resolve()
PROJECT_ROOT = THIS_FILE.parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# ------------------ é»˜è®¤é…ç½®è·¯å¾„ï¼ˆå†™æ­»ï¼Œä¾¿äºç›´æ¥è¿è¡Œï¼‰ ------------------
DEFAULT_CONFIG_PATH = Path("/home/liyakun/twitter-stock-prediction/configs/model2.yaml")


# ------------------ æ—¥å¿— ------------------
def setup_logger(name: str,
                 log_file: Path,
                 console_level=logging.INFO,
                 file_level=logging.DEBUG,
                 also_timestamp_file: bool = True) -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(logging.DEBUG)

    # æ§åˆ¶å°
    ch = logging.StreamHandler()
    ch.setLevel(console_level)
    ch.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(ch)

    # å›ºå®šæ—¥å¿—æ–‡ä»¶
    log_file = Path(log_file)
    log_file.parent.mkdir(parents=True, exist_ok=True)
    fh = logging.FileHandler(str(log_file), encoding="utf-8")
    fh.setLevel(file_level)
    fh.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
    logger.addHandler(fh)

    # æ—¶é—´æˆ³æ—¥å¿—æ–‡ä»¶ï¼ˆé¢å¤–ï¼‰
    if also_timestamp_file:
        ts_name = f"{log_file.stem}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{log_file.suffix}"
        ts_path = log_file.parent / ts_name
        fh2 = logging.FileHandler(str(ts_path), encoding="utf-8")
        fh2.setLevel(file_level)
        fh2.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
        logger.addHandler(fh2)

    logger.debug(f"æ—¥å¿—å†™å…¥: {log_file}")
    return logger


# ------------------ é…ç½® ------------------
def load_config(path: Path):
    if not Path(path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# ------------------ æ•°æ®é›† ------------------
class TextTargetDataset(Dataset):
    """
    è¯»å– CSVï¼ˆå¿…é¡»åŒ…å« text, targetï¼›å¯é€‰ dateï¼‰
    """
    def __init__(self, csv_path: str):
        df = pd.read_csv(csv_path)
        cols = [c.lower() for c in df.columns]
        df.columns = cols

        if "text" not in df.columns:
            raise ValueError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ° 'text' åˆ—ï¼Œç°æœ‰åˆ—ï¼š{list(df.columns)}")
        if "target" not in df.columns:
            raise ValueError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ° 'target' åˆ—ï¼Œç°æœ‰åˆ—ï¼š{list(df.columns)}")

        self.texts = df["text"].astype(str).tolist()
        self.targets = torch.tensor(df["target"].astype(float).values, dtype=torch.float32)
        self.dates = df["date"].astype(str).tolist() if "date" in df.columns else [""] * len(df)

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, i):
        return self.texts[i], self.targets[i], self.dates[i]


# ------------------ ç‰¹å¾æŠ½å– ------------------
class EmbeddingExtractor:
    """
    ä¼˜å…ˆè°ƒç”¨ä½ è‡ªå·±çš„ src.model1.predict.infer_embeddings(texts, tokenizer_dir, device) -> torch.FloatTensor [N,D]
    å›é€€ï¼šç”¨ HuggingFace çš„ tokenizer+AutoModelï¼Œä»æœ€åéšå±‚å– CLS å‘é‡ï¼ˆæˆ–å¹³å‡æ± åŒ–ï¼‰
    """
    def __init__(self, tokenizer_dir: str, device: torch.device, logger: logging.Logger):
        self.tokenizer_dir = tokenizer_dir
        self.device = device
        self.logger = logger
        self.logger.info(f"ğŸ§© æŠ½ç‰¹å¾ï¼šå›é€€ HuggingFace @ {tokenizer_dir}")
        self._build_hf_stack()

    def _build_hf_stack(self):
        from transformers import AutoTokenizer, AutoModel
        self.hf_tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_dir, use_fast=True)
        self.hf_model = AutoModel.from_pretrained(self.tokenizer_dir).to(self.device)
        self.hf_model.eval()

    @torch.no_grad()
    def encode(self, texts: list, max_length: int = 128, batch_size: int = 32) -> torch.Tensor:
        # 1) ä½ çš„è‡ªå®šä¹‰å‡½æ•°
        if self._user_infer is not None:
            try:
                embs = self._user_infer(texts=texts,
                                        tokenizer_dir=self.tokenizer_dir,
                                        device=str(self.device),
                                        max_length=max_length,
                                        batch_size=batch_size)
                if isinstance(embs, np.ndarray):
                    embs = torch.tensor(embs, dtype=torch.float32)
                return embs.to(self.device)
            except Exception as e:
                self.logger.warning(f"âš ï¸ è‡ªå®šä¹‰ infer_embeddings å¤±è´¥ï¼Œæ”¹ç”¨ HFï¼š{e}")
                self._user_infer = None  # åç»­ç›´æ¥èµ° HF

        # 2) HuggingFace å›é€€ï¼šCLS
        from transformers import BatchEncoding
        embs = []
        for i in range(0, len(texts), batch_size):
            chunk = texts[i:i + batch_size]
            enc = self.hf_tokenizer(
                chunk,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(self.device)

            outputs = self.hf_model(**enc, output_hidden_states=True)
            # å–æœ€åä¸€å±‚çš„ç¬¬ 1 ä¸ª token (CLS) å‘é‡
            last_hidden = outputs.last_hidden_state  # [B, L, H]
            cls_vec = last_hidden[:, 0, :]           # [B, H]
            embs.append(cls_vec.float().detach().cpu())

        embs = torch.cat(embs, dim=0)  # [N, H]
        return embs.to(self.device)


# ------------------ æ¨¡å‹æ„å»ºï¼ˆä»ç›®å½•è¯»å–ç»“æ„ï¼‰------------------
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_size: int = 512, output_dim: int = 1, num_layers: int = 2):
        super().__init__()
        layers = []
        d = input_dim
        for _ in range(max(1, num_layers - 1)):
            layers += [nn.Linear(d, hidden_size), nn.ReLU()]
            d = hidden_size
        layers += [nn.Linear(d, output_dim)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


def build_model_from_dir(model_dir: str, logger: logging.Logger, device: torch.device):
    """
    ä¼˜å…ˆå°è¯• HuggingFace AutoConfig/AutoModelï¼›
    å¦åˆ™æŒ‰ç®€å• config.json é‡Œ input_dim/hidden_size/output_dim é€ ä¸€ä¸ª MLPã€‚
    è¿”å›ï¼š(model, model_kind, output_dim)
    """
    model_dir = Path(model_dir)
    cfg_path = model_dir / "config.json"
    if not cfg_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json: {model_dir}")

    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg_json = json.load(f)

    # 1) HuggingFace é£æ ¼
    try:
        from transformers import AutoConfig, AutoModel
        hf_config = AutoConfig.from_pretrained(str(model_dir))
        model = AutoModel.from_pretrained(str(model_dir), config=hf_config).to(device)
        logger.info(f"ğŸ§  ä½¿ç”¨ HF æ¨¡å‹: {getattr(hf_config, 'architectures', ['AutoModel'])[0]}")
        # æˆ‘ä»¬åšçš„æ˜¯å›å½’å¤´è®­ç»ƒï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦ä¸€ä¸ªçº¿æ€§å¤´å°†éšè—å‘é‡ -> æ ‡é‡
        head = nn.Linear(hf_config.hidden_size, 1).to(device)
        return (model, "hf_backbone+linear", 1, head)
    except Exception:
        logger.info("â„¹ï¸ é HF æ¶æ„ï¼ŒæŒ‰ç®€å• MLP é…ç½®æ„å»º")

    # 2) ç®€å• MLP
    input_dim = int(cfg_json.get("input_dim", 768))
    hidden_size = int(cfg_json.get("hidden_size", 512))
    output_dim = int(cfg_json.get("output_dim", 1))
    num_layers = int(cfg_json.get("num_layers", 2))

    model = MLP(input_dim=input_dim,
                hidden_size=hidden_size,
                output_dim=output_dim,
                num_layers=num_layers).to(device)
    logger.info(f"ğŸ§  ä½¿ç”¨ MLP: in={input_dim}, hidden={hidden_size}, out={output_dim}, layers={num_layers}")
    return (model, "mlp", output_dim, None)


# ------------------ æŸå¤±ï¼ˆå•ç‹¬å†™ä¸€ä¸ªç±»ï¼‰------------------
class SafeSmoothL1Loss(nn.Module):
    def __init__(self):
        super().__init__()
        self._crit = nn.SmoothL1Loss()

    def forward(self, pred, target):
        if not torch.isfinite(pred).all():
            pred = torch.nan_to_num(pred, nan=0.0, posinf=1e6, neginf=-1e6)
        if not torch.isfinite(target).all():
            target = torch.nan_to_num(target, nan=0.0, posinf=1e6, neginf=-1e6)
        return self._crit(pred, target)


# ------------------ è®­ç»ƒä¸»æµç¨‹ ------------------
def main(config_path: Path = DEFAULT_CONFIG_PATH):
    cfg = load_config(config_path)

    # è·¯å¾„é…ç½®
    train_csv = cfg["paths"].get("train_data_path", "/home/liyakun/twitter-stock-prediction/data/processed/train_dataset.csv")
    val_csv   = cfg["paths"].get("val_data_path",   "/home/liyakun/twitter-stock-prediction/data/processed/val_dataset.csv")
    out_model = Path(cfg["paths"]["output_model"])
    log_file  = Path(cfg["paths"]["log_file"])
    model2_dir = cfg["paths"].get("model2_dir", "/home/liyakun/twitter-stock-prediction/models/model2")
    tokenizer_dir = cfg["env"].get("tokenizer_dir", "/home/liyakun/LLaMA-Factory-main/deepseek1.5B")

    # è¶…å‚
    bs      = int(cfg["training"].get("batch_size", 16))
    lr      = float(cfg["training"].get("learning_rate", 1e-4))
    epochs  = int(cfg["training"].get("epochs", 10))
    max_len = int(cfg["training"].get("max_length", 128))

    # Debugï¼ˆä¸æ”¹é…ç½®ï¼Œç”¨å¼€å…³ï¼‰
    DEBUG_SMALL_TRAIN = cfg["training"].get("debug_small_train", True)
    DEBUG_TRAIN_SAMPLES = int(cfg["training"].get("debug_train_samples", 100))
    DEBUG_MAX_BATCHES = cfg["training"].get("debug_max_batches", None)  # None æˆ– æ­£æ•´æ•°

    # æ—¥å¿—
    logger = setup_logger("model2_train", log_file,
                          console_level=logging.INFO,
                          file_level=logging.DEBUG,
                          also_timestamp_file=True)
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®: {config_path}")
    logger.info(f"ğŸ“ è®­ç»ƒé›†: {train_csv}")
    logger.info(f"ğŸ“ éªŒè¯é›†: {val_csv}")
    logger.info(f"ğŸ§­ æ¨¡å‹ç›®å½•ï¼ˆè¯»å–ç»“æ„ï¼‰: {model2_dir}")
    logger.info(f"ğŸ§© ç‰¹å¾æŠ½å–: ä¼˜å…ˆ src.model1.predict.infer_embeddingsï¼›å›é€€ HF @ {tokenizer_dir}")

    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"âš™ï¸ è®¾å¤‡: {device}")

    # æ•°æ®é›†
    train_ds = TextTargetDataset(train_csv)
    val_ds   = TextTargetDataset(val_csv)

    if DEBUG_SMALL_TRAIN:
        n = min(DEBUG_TRAIN_SAMPLES, len(train_ds))
        train_ds_loader = Subset(train_ds, range(n))
        logger.warning(f"âš ï¸ Debug æ¨¡å¼ï¼šä»…ä½¿ç”¨å‰ {n} æ¡è®­ç»ƒæ ·æœ¬")
    else:
        train_ds_loader = train_ds

    # ç‰¹å¾æŠ½å–å™¨
    extractor = EmbeddingExtractor(tokenizer_dir=tokenizer_dir, device=device, logger=logger)

    # å…ˆæŠ½ä¸€å°æ‰¹ï¼Œå¾—åˆ°ç‰¹å¾ç»´åº¦
    probe_texts = [train_ds[0][0]]
    probe_emb = extractor.encode(probe_texts, max_length=max_len, batch_size=1)  # [1, D]
    feat_dim = probe_emb.shape[-1]
    logger.info(f"ğŸ§· è®­ç»ƒç‰¹å¾ç»´åº¦: {feat_dim}")

    # æ„å»ºæ¨¡å‹äºŒ
    model2, model_kind, out_dim, hf_head = build_model_from_dir(model2_dir, logger, device)

    # å¦‚æœæ˜¯ HF backboneï¼Œå°±åŠ ä¸€ä¸ªçº¿æ€§å¤´æŠŠéšè—å‘é‡ -> æ ‡é‡ï¼›è®­ç»ƒæ—¶è¾“å…¥å¿…é¡»æ˜¯â€œç‰¹å¾å‘é‡â€ï¼Œ
    # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç®€å•åŒ…è£…ä¸€ä¸‹ï¼šæŠŠç‰¹å¾é€åˆ°çº¿æ€§å¤´ï¼›HF backboneåªåœ¨â€œåŠ è½½ç»“æ„/ä¿æŒç›®å½•ä¸€è‡´æ€§â€å±‚é¢å ä½
    use_linear_head_only = (model_kind == "hf_backbone+linear") and (hf_head is not None)
    if use_linear_head_only:
        # å†»ç»“ HF ä¸»ä½“ï¼ˆåªè®­ç»ƒçº¿æ€§å¤´ï¼‰
        for p in model2.parameters():
            p.requires_grad = False
        reg_head = hf_head  # nn.Linear(hidden_size, 1)
        params = list(reg_head.parameters())
        logger.info("ğŸ§± è®­ç»ƒç­–ç•¥ï¼šå†»ç»“ HF ä¸»ä½“ï¼Œä»…è®­ç»ƒçº¿æ€§å›å½’å¤´")
    else:
        reg_head = None
        params = list(model2.parameters())
        # æ ¡éªŒè¾“å…¥ç»´åº¦åŒ¹é…ï¼ˆä»…å¯¹ MLP æœ‰æ„ä¹‰ï¼‰
        if model_kind == "mlp":
            try:
                # ç®€å•å‰ä¼ ä¸€æ¬¡æ£€æŸ¥
                _ = model2(torch.zeros(1, feat_dim, device=device))
            except Exception as e:
                logger.error(f"âŒ MLP å‰å‘æ£€æŸ¥å¤±è´¥ï¼Œå¯èƒ½æ˜¯ input_dim ä¸åŒ¹é…ï¼ˆæœŸæœ›ä¸ç‰¹å¾ç»´åº¦ {feat_dim} ä¸€è‡´ï¼‰: {e}")
                raise

    # ä¼˜åŒ–å™¨ & æŸå¤±
    opt = torch.optim.AdamW(params, lr=lr)
    crit = SafeSmoothL1Loss()

    # DataLoaderï¼ˆæŒ‰æ‰¹æŠ½ç‰¹å¾ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    # ========= DataLoader =========
    def collate(batch):
        texts, targets, dates = zip(*batch)
        return list(texts), torch.stack(targets), list(dates)

    train_loader = DataLoader(train_ds_loader, batch_size=bs, shuffle=True,
                            num_workers=0, pin_memory=True, collate_fn=collate)
    val_loader   = DataLoader(val_ds, batch_size=bs, shuffle=False,
                            num_workers=0, pin_memory=True, collate_fn=collate)

    # --- ç»Ÿè®¡ y åˆ†å¸ƒï¼ˆå…¼å®¹ (text, target, date) ç»“æ„ï¼‰---
    target_only_loader = DataLoader(
        train_ds_loader,  # ç”¨ä¸ä½ è®­ç»ƒä¸€è‡´çš„æ•°æ®æºï¼ˆå¯èƒ½æ˜¯ Subsetï¼‰
        batch_size=4096,
        shuffle=False,
        num_workers=0,
        collate_fn=lambda batch: torch.stack([b[1] for b in batch])  # åªå †å  target
    )
    with torch.no_grad():
        y_chunks = [yb.float().view(-1).cpu() for yb in target_only_loader]
        y_all = torch.cat(y_chunks, dim=0)
    logger.info(
        f"[y ç»Ÿè®¡] mean={y_all.mean().item():.4f}, "
        f"std={y_all.std().item():.4f}, "
        f"min={y_all.min().item():.4f}, "
        f"max={y_all.max().item():.4f}, "
        f"n={y_all.numel()}"
    )

    # ========= è®­ç»ƒé…ç½® =========
    out_model.parent.mkdir(parents=True, exist_ok=True)
    SAVE_ONLY_LAST = True       # åªåœ¨è®­ç»ƒç»“æŸæ—¶æŠŠâ€œæœ€ä½³â€ä¸€æ¬¡æ€§è½ç›˜ï¼›æƒ³æ¯æ¬¡åˆ·æœ€ä½³å°±ä¿å­˜ï¼Œæ”¹ False
    best_val = float("inf")
    best_state = None           # ç¼“å­˜æœ€ä½³æƒé‡ï¼ˆæŒ‰ç­–ç•¥å†³å®šç¼“å­˜ reg_head è¿˜æ˜¯ model2ï¼‰

    # ========= è®­ç»ƒå¾ªç¯ =========
    for epoch in range(1, epochs + 1):
        t0 = time.time()
        model2.train()
        if reg_head is not None:
            reg_head.train()

        run_loss = 0.0
        seen = 0

        logger.info(f"\nã€è®­ç»ƒã€‘å¼€å§‹ç¬¬ {epoch} è½®")
        for b, (texts, y, _) in enumerate(train_loader, start=1):
            if DEBUG_MAX_BATCHES is not None and b > int(DEBUG_MAX_BATCHES):
                logger.info(f"ğŸ”§ Debug: ä»…è·‘å‰ {DEBUG_MAX_BATCHES} ä¸ª batch")
                break

            with torch.no_grad():
                X = extractor.encode(texts, max_length=max_len, batch_size=bs)  # [B, D]
            y = y.to(device)

            opt.zero_grad(set_to_none=True)
            if use_linear_head_only:
                pred = reg_head(X).squeeze(-1)    # [B,1] -> [B]
            else:
                pred = model2(X).squeeze(-1)      # [B,1] -> [B]

            loss = crit(pred, y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(params, 1.0)
            opt.step()

            run_loss += loss.item() * y.size(0)
            seen += y.size(0)

            if b % 20 == 0:
                logger.info(f"[Epoch {epoch}] batch {b} | loss={loss.item():.6f}")

        # â€”â€” æœ¬è½®è®­ç»ƒå‡å€¼ & æ—¶é—´
        avg_train = run_loss / max(1, seen)
        dt = time.time() - t0
        logger.info(f"âœ… Epoch {epoch} å®Œæˆ | è®­ç»ƒæŸå¤±: {avg_train:.6f} | ç”¨æ—¶ {dt:.1f}s")

        # ========= éªŒè¯ =========
        model2.eval()
        if reg_head is not None:
            reg_head.eval()

        val_loss = 0.0
        vseen = 0
        with torch.no_grad():
            for texts, y, _ in val_loader:
                X = extractor.encode(texts, max_length=max_len, batch_size=bs)
                y = y.to(device)
                if use_linear_head_only:
                    pred = reg_head(X).squeeze(-1)
                else:
                    pred = model2(X).squeeze(-1)
                l = crit(pred, y)
                val_loss += l.item() * y.size(0)
                vseen += y.size(0)
        avg_val = val_loss / max(1, vseen)
        logger.info(f"ğŸ§ª éªŒè¯æŸå¤±: {avg_val:.6f}")

        # ========= å½“å‰ vs å†å²æœ€ä½³ + ä¿å­˜ç­–ç•¥ =========
        new_best = avg_val < best_val
        logger.info(
            f"Epoch {epoch}/{epochs} | è®­ç»ƒæŸå¤±={avg_train:.6f} | éªŒè¯æŸå¤±={avg_val:.6f} "
            f"| æœ€ä½³={ (best_val if best_val < float('inf') else float('nan')) :.6f}"
            f"{' âœ… æ–°æœ€ä½³' if new_best else ''}"
        )

        if new_best:
            best_val = avg_val
            if SAVE_ONLY_LAST:
                # åªåœ¨æœ€åä¿å­˜ï¼šè¿™é‡Œç¼“å­˜æƒé‡
                best_state = {"head": reg_head.state_dict()} if use_linear_head_only \
                            else {"model": model2.state_dict()}
            else:
                # ç«‹åˆ»è½ç›˜
                if use_linear_head_only:
                    torch.save(reg_head.state_dict(), str(out_model))
                else:
                    torch.save(model2.state_dict(), str(out_model))
                logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ä½³æ¨¡å‹ -> {out_model} (val={avg_val:.6f})")

    # ========= è®­ç»ƒç»“æŸï¼šä¸€æ¬¡æ€§ä¿å­˜å…¨ç¨‹æœ€ä½³ï¼ˆè‹¥å¯ç”¨ï¼‰=========
    if SAVE_ONLY_LAST and best_state is not None:
        if use_linear_head_only and "head" in best_state:
            torch.save(best_state["head"], str(out_model))
        elif not use_linear_head_only and "model" in best_state:
            torch.save(best_state["model"], str(out_model))
        logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜å…¨ç¨‹æœ€ä½³æ¨¡å‹ -> {out_model} (best val={best_val:.6f})")
    else:
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ")


if __name__ == "__main__":
    main(DEFAULT_CONFIG_PATH)
