data:
  train_file: data/processed/model2_input.csv
  model1_path: models/model1/best_model.pt
training:
  batch_size: 16
  epochs: 50
  learning_rate: 1e-5
paths:
  output_model: /home/liyakun/twitter-stock-prediction/models/model2/best_model.pt
  log_file: /home/liyakun/twitter-stock-prediction/logs/model2_train.log
env:
  prediction_model_path: /home/liyakun/twitter-stock-prediction/models/model2/best_model.pt
  tokenizer_dir: /home/liyakun/LLaMA-Factory-main/deepseek1.5B
model:
  type: TransformerModel
  params:
    input_dim: 773
    hidden_size: 1536
    num_layers: 28
    num_attention_heads: 6
    seq_len: 30
    output_dim: 1
    head_dim: 256
    vocab_size: 151936
    layer_norm_eps: 1e-5
    use_bias: false
deepseek:
  num_key_value_heads: 1
  intermediate_size: 5504
  max_position_embeddings: 4096
  head_dim: 256
  rms_norm_eps: 1e-6
  attention_dropout: 0.0
  hidden_dropout: 0.0
  initializer_range: 0.02
