import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import yaml
import os
import logging
from typing import Optional, Tuple, Dict, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("StockPredictor")
def remap_deepseek_keys(state_dict):
    """å®Œæ•´çš„DeepSeekæƒé‡æ˜ å°„å‡½æ•°"""
    mapping = {
        # Embedding å’Œè¾“å‡ºå±‚
        'embed_tokens.weight': 'embed_tokens.weight',
        'lm_head.weight': 'lm_head.weight',
        'norm.weight': 'norm.weight',
    }
    
    # ä¸ºæ¯ä¸€å±‚æ·»åŠ æ˜ å°„ (å‡è®¾28å±‚)
    for i in range(28):
        # æ³¨æ„å±‚ç´¢å¼•ä»0å¼€å§‹
        mapping.update({
            # å±‚å½’ä¸€åŒ–
            f'layers.{i}.input_layernorm.weight': f'layers.{i}.input_norm.weight',
            f'layers.{i}.post_attention_layernorm.weight': f'layers.{i}.post_attention_norm.weight',
            
            # æ³¨æ„åŠ›å±‚ - æƒé‡
            f'layers.{i}.self_attn.q_proj.weight': f'layers.{i}.attention.q_proj.weight',
            f'layers.{i}.self_attn.k_proj.weight': f'layers.{i}.attention.k_proj.weight',
            f'layers.{i}.self_attn.v_proj.weight': f'layers.{i}.attention.v_proj.weight',
            f'layers.{i}.self_attn.o_proj.weight': f'layers.{i}.attention.o_proj.weight',
            
            # æ³¨æ„åŠ›å±‚ - åç½® (å¦‚æœå­˜åœ¨)
            f'layers.{i}.self_attn.q_proj.bias': f'layers.{i}.attention.q_proj.bias',
            f'layers.{i}.self_attn.k_proj.bias': f'layers.{i}.attention.k_proj.bias',
            f'layers.{i}.self_attn.v_proj.bias': f'layers.{i}.attention.v_proj.bias',
            f'layers.{i}.self_attn.o_proj.bias': f'layers.{i}.attention.o_proj.bias',
            
            # MLPå±‚ - æƒé‡
            f'layers.{i}.mlp.gate_proj.weight': f'layers.{i}.mlp.gate_proj.weight',
            f'layers.{i}.mlp.up_proj.weight': f'layers.{i}.mlp.up_proj.weight',
            f'layers.{i}.mlp.down_proj.weight': f'layers.{i}.mlp.down_proj.weight',
            
            # MLPå±‚ - åç½® (å¦‚æœå­˜åœ¨)
            f'layers.{i}.mlp.gate_proj.bias': f'layers.{i}.mlp.gate_proj.bias',
            f'layers.{i}.mlp.up_proj.bias': f'layers.{i}.mlp.up_proj.bias',
            f'layers.{i}.mlp.down_proj.bias': f'layers.{i}.mlp.down_proj.bias',
        })
    
    new_state_dict = {}
    for old_key, value in state_dict.items():
        new_key = mapping.get(old_key, old_key)
        new_state_dict[new_key] = value
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ“Š æƒé‡æ˜ å°„å®Œæˆ: {len(state_dict)} -> {len(new_state_dict)} ä¸ªå‚æ•°")
    
    return new_state_dict
def remap_state_dict_keys(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
    """
    é‡æ˜ å°„çŠ¶æ€å­—å…¸é”®å - ç®€åŒ–ç‰ˆæœ¬
    """
    new_state_dict = {}
    
    for old_key, param in state_dict.items():
        new_key = old_key
        
        # åªéœ€è¦åšç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢
        if "self_attn" in new_key:
            new_key = new_key.replace("self_attn", "attention")
        
        if "input_layernorm" in new_key:
            new_key = new_key.replace("input_layernorm", "input_norm")
        
        if "post_attention_layernorm" in new_key:
            new_key = new_key.replace("post_attention_layernorm", "post_attention_norm")
        
        new_state_dict[new_key] = param
    
    return new_state_dict
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return norm_x * self.weight

class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç 
        self._precompute_rotary_emb()
        
    def _precompute_rotary_emb(self):
        # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=self.device) / self.dim))
        t = torch.arange(self.max_position_embeddings, device=self.device, dtype=torch.float32)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)
        
    def forward(self, x, seq_len=None):
        return self.cos_cached, self.sin_cached

def rotate_half(x: torch.Tensor) -> torch.Tensor:
    """æ—‹è½¬ä¸€åŠçš„ç»´åº¦"""
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat((-x2, x1), dim=-1)
class DeepSeekMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config['hidden_size']
        self.intermediate_size = config['intermediate_size']
        
        self.gate_proj = nn.Linear(config["hidden_size"], config["intermediate_size"], bias=True)
        self.up_proj = nn.Linear(config["hidden_size"], config["intermediate_size"], bias=True)
        self.down_proj = nn.Linear(config["intermediate_size"], config["hidden_size"], bias=True) 
    def forward(self, x):
        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))
def _prepare_4d_causal_attention_mask(attention_mask, input_shape, inputs_embeds, past_key_values_length):
    """å‡†å¤‡å› æœæ³¨æ„åŠ›æ©ç """
    batch_size, seq_length = input_shape
    mask = torch.full(
        (seq_length, seq_length),
        torch.finfo(inputs_embeds.dtype).min,
        device=inputs_embeds.device,
    )
    mask_cond = torch.arange(mask.size(-1), device=mask.device)
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
    
    if past_key_values_length > 0:
        mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=mask.dtype, device=mask.device), mask], dim=-1)
    
    return mask[None, None, :, :].expand(batch_size, 1, seq_length, seq_length + past_key_values_length)
class DeepSeekDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config['hidden_size']
        
        # å½’ä¸€åŒ–å±‚
        self.input_norm = nn.LayerNorm(config["hidden_size"], eps=config["rms_norm_eps"],bias=False)
        self.post_attention_norm = nn.LayerNorm(config["hidden_size"], eps=config["rms_norm_eps"],bias=False)
        
        # æ³¨æ„åŠ›å±‚å’ŒMLPå±‚
        self.attention = DeepSeekAttention(config)
        self.mlp = DeepSeekMLP(config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€ [batch_size, seq_len, hidden_size]
            attention_mask: æ³¨æ„åŠ›æ©ç 
            position_ids: ä½ç½®ç¼–ç 
            past_key_value: è¿‡å»çš„é”®å€¼å¯¹ï¼ˆç”¨äºKVç¼“å­˜ï¼‰
            output_attentions: æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
            
        Returns:
            è¾“å‡ºå…ƒç»„: (hidden_states, attentions, present_key_value)
        """
        # è‡ªæ³¨æ„åŠ›éƒ¨åˆ†
        residual = hidden_states
        hidden_states = self.input_norm(hidden_states)
        
        attention_outputs = self.attention(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        
        hidden_states = attention_outputs[0]
        hidden_states = residual + hidden_states  # æ®‹å·®è¿æ¥
        
        # MLPéƒ¨åˆ†
        residual = hidden_states
        hidden_states = self.post_attention_norm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states  # æ®‹å·®è¿æ¥
        
        # ç»„ç»‡è¾“å‡º
        outputs = (hidden_states,)
        
        # å¦‚æœéœ€è¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡
        if output_attentions:
            outputs += (attention_outputs[1],)
        
        # å¦‚æœéœ€è¦ä½¿ç”¨ç¼“å­˜
        if use_cache:
            outputs += (attention_outputs[2],)  # present_key_value
        
        return outputs

def adapt_deepseek_weights(pretrained_weights, num_heads=6):
    """
    æ­£ç¡®é€‚é…DeepSeekæƒé‡æ ¼å¼
    """
    adapted_weights = {}
    
    for key, value in pretrained_weights.items():
        # å¤„ç†k_projå’Œv_projçš„æƒé‡ (éœ€è¦è½¬ç½®å¹¶é‡å¤)
        if ('k_proj.weight' in key or 'v_proj.weight' in key) and value.shape == torch.Size([256, 1536]):
            # å…ˆè½¬ç½®: [256, 1536] -> [1536, 256]
            value = value.t()
            # ç„¶åé‡å¤6æ¬¡: [1536, 256] -> [1536, 1536]
            value = value.repeat(1, num_heads)
            logger.debug(f"ğŸ”„ é€‚é…æƒé‡: {key} {value.shape}")
        
        # å¤„ç†k_projå’Œv_projçš„åç½® (éœ€è¦é‡å¤)
        elif ('k_proj.bias' in key or 'v_proj.bias' in key) and value.shape == torch.Size([256]):
            # é‡å¤6æ¬¡: [256] -> [1536]
            value = value.repeat(num_heads)
            logger.debug(f"ğŸ“ é€‚é…åç½®: {key} {value.shape}")
        
        else:
            # å…¶ä»–æƒé‡ä¿æŒä¸å˜
            adapted_weights[key] = value
            logger.debug(f"âœ… ä¿æŒåŸæ ·: {key} {value.shape}")
    
    logger.info(f"âœ… æƒé‡é€‚é…å®Œæˆï¼Œå…±å¤„ç† {len(adapted_weights)} ä¸ªå‚æ•°")
    return adapted_weights
class DeepSeekAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config['hidden_size']
        self.num_heads = config['num_attention_heads']
        self.head_dim = self.hidden_size // self.num_heads
        
        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=True)
        self._init_rope()
        assert self.hidden_size == self.num_heads * self.head_dim, "éšè—å¤§å°å¿…é¡»æ˜¯å¤´æ•°Ã—å¤´ç»´åº¦çš„æ•´æ•°å€"
    def _init_rope(self):
        # æ—‹è½¬ä½ç½®ç¼–ç åˆå§‹åŒ–
        pass
    
    def forward(self, x, attention_mask=None, position_ids=None):
        batch_size, seq_length, _ = x.shape
        
        # æŸ¥è¯¢æŠ•å½± - ä¿æŒæ•´ä½“æŠ•å½±
        query_states = self.q_proj(x)  # [batch, seq_len, hidden_size]
        
        # å…³é”®ä¿®æ”¹ï¼šé”®å€¼æŠ•å½± - ç›´æ¥æŠ•å½±åˆ°å¤šå¤´æ ¼å¼
        key_states = self.k_proj(x)    # [batch, seq_len, num_heads * head_dim]
        value_states = self.v_proj(x)   # [batch, seq_len, num_heads * head_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆå¦‚æœæœ‰ï¼‰
        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, position_ids)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value_states)
        
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_length, self.num_heads * self.head_dim)
        
        # è¾“å‡ºæŠ•å½±
        attn_output = self.o_proj(attn_output)
        
        return attn_output, attn_weights, (key_states, value_states)
def apply_rotary_pos_emb(x, cos, sin):
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    rotated = torch.cat([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=-1)
    return rotated
class DeepSeekTransformerBlock(nn.Module):
    def __init__(self, config: dict, layer_idx: int):
        super().__init__()
        self.layer_idx = layer_idx
        
        # DeepSeek ç‰¹æœ‰çš„æ®‹å·®æƒé‡å‚æ•°
        self.attention_residual_weight = nn.Parameter(torch.ones(1))
        self.mlp_residual_weight = nn.Parameter(torch.ones(1))
    
        # âš ï¸ å…³é”®ä¿®æ”¹ï¼šé‡å‘½åå½’ä¸€åŒ–å±‚ä»¥åŒ¹é…æƒé‡æ–‡ä»¶
        self.input_layernorm = nn.LayerNorm(config["hidden_size"], eps=config["rms_norm_eps"], bias=False)
        self.post_attention_layernorm = nn.LayerNorm(config["hidden_size"], eps=config["rms_norm_eps"], bias=False)
        
        # æ³¨æ„åŠ›å±‚
        self.attention = DeepSeekAttention(config)
        
        # MLPå±‚
        self.mlp = DeepSeekMLP(config)

    def forward(self, x, attention_mask=None):
        # ä¿æŒåŸæœ‰çš„å‰å‘ä¼ æ’­é€»è¾‘
        residual = x
        x = self.input_norm(x)
        x = self.attention(x, attention_mask)
        x = residual + self.attention_residual_weight * x
        
        residual = x
        x = self.post_attention_norm(x)
        x = self.mlp(x)
        x = residual + self.mlp_residual_weight * x
        
        return x

class DeepSeekCompatibleModel(nn.Module):
    def __init__(self, config, device="cuda"):
        super().__init__()
        self.config = config
        self.device = device
        self.padding_idx = getattr(config, 'pad_token_id', 0)
        
        # 1. è¯åµŒå…¥å±‚ (æ‚¨ä¹‹å‰ç¼ºå°‘çš„)
        self.embed_tokens = nn.Embedding(
            config['vocab_size'], 
            config['hidden_size'], 
            self.padding_idx
        )
        
        # 2. Transformerå±‚ (éœ€è¦å®Œå…¨é‡æ–°å®ç°)
        self.layers = nn.ModuleList([
            DeepSeekDecoderLayer(config) for _ in range(config['num_layers'])
        ])
        
        # 3. å½’ä¸€åŒ–å±‚
        self.norm = nn.LayerNorm(config['hidden_size'], eps=config.get('rms_norm_eps', 1e-6),bias=False)
        
        # 4. è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.Linear(config['hidden_size'], config['output_dim'],bias=False )
        
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        # ç‰¹æ®Šåˆå§‹åŒ–
        self._tie_weights()
    
    def _init_weights(self, module):
        """DeepSeeké£æ ¼çš„æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def _tie_weights(self):
        """æƒé‡ç»‘å®šï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"""
        pass
    
    def forward(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ):
        # åµŒå…¥æŸ¥æ‰¾
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        
        # å‡†å¤‡æ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            attention_mask = _prepare_4d_causal_attention_mask(
                attention_mask, inputs_embeds.shape[:2], inputs_embeds, 0
            )
        
        hidden_states = inputs_embeds
        presents = () if use_cache else None
        all_self_attns = () if output_attentions else None
        
        # é€šè¿‡æ‰€æœ‰å±‚
        for idx, decoder_layer in enumerate(self.layers):
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_values[idx] if past_key_values is not None else None,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )
            
            hidden_states = layer_outputs[0]
            
            if use_cache:
                presents = presents + (layer_outputs[2 if output_attentions else 1],)
            
            if output_attentions:
                all_self_attns = all_self_attns + (layer_outputs[1],)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        hidden_states = self.norm(hidden_states)
        
        # è‚¡ç¥¨é¢„æµ‹è¾“å‡º
        logits = self.output_proj(hidden_states)  # å–æœ€åä¸€ä¸ªtoken
        
        return logits

class DeepSeekPredictor:
    CONFIG_PATH = "/home/liyakun/twitter-stock-prediction/configs/model2.yaml"
    
    def __init__(self, device=None, model_path=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device is None else device
        logger.info(f"ğŸ“Š ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(self.CONFIG_PATH)
        
        # æå–æ¨¡å‹å‚æ•°
        self.model_params = self.config["model"]["params"]
        self.deepseek_params = self.config["deepseek"]
       
        # å…³é”®å‚æ•° - éœ€è¦ç¡®ä¿ä¸DeepSeekæ¶æ„åŒ¹é…
        self.input_dim = self.model_params["input_dim"]
        self.fixed_seq_len = self.model_params["seq_len"]
        self.model_params['use_bias'] = False
        # DeepSeekæ¶æ„å‚æ•°
        self.hidden_size = self.model_params["hidden_size"]
        self.num_layers = self.model_params["num_layers"]
        self.num_attention_heads = self.model_params["num_attention_heads"]
        self.vocab_size = self.model_params["vocab_size"]
        self.head_dim = self.model_params["head_dim"]
        
        # DeepSeekç‰¹å®šå‚æ•°
        self.num_key_value_heads = self.deepseek_params["num_key_value_heads"]
        self.intermediate_size = self.deepseek_params["intermediate_size"]
        self.max_position_embeddings = self.deepseek_params["max_position_embeddings"]
        
        # æ„å»ºæ¨¡å‹ - ä½¿ç”¨æ–°çš„DeepSeekCompatibleModel
        self.model = self._build_model_from_config()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if model_path:
            self.load_model(model_path)
        else:
            default_path = "/home/liyakun/twitter-stock-prediction/models/model2/best_model.pt"
            if os.path.exists(default_path):
                self.load_model(default_path)

    def _load_config(self, config_path):
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def _build_model_from_config(self):
        """æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡"""
        
        # è°ƒè¯•ï¼šæ‰“å°ä¼ é€’ç»™DeepSeekCompatibleModelçš„é…ç½®
        print("=== ä¼ é€’ç»™ DeepSeekCompatibleModel çš„é…ç½® ===")
        print("é…ç½®ç±»å‹:", type(self.model_params))
        print("é…ç½®é”®:", list(self.model_params.keys()))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å‚æ•°
        required_params = ["num_layers", "hidden_size", "num_attention_heads", "vocab_size"]
        missing_params = [p for p in required_params if p not in self.model_params]
        
        if missing_params:
            print(f"âŒ ç¼ºå°‘å‚æ•°: {missing_params}")
            print("å½“å‰é…ç½®å†…å®¹:", self.model_params)
            raise ValueError(f"é…ç½®ç¼ºå°‘å‚æ•°: {missing_params}")
        
        # åˆ›å»ºå®Œæ•´çš„é…ç½®
        full_config = {
            **self.model_params,  # åŒ…å« num_layers ç­‰å‚æ•°
            "num_key_value_heads": self.deepseek_params["num_key_value_heads"],
            "intermediate_size": self.deepseek_params["intermediate_size"],
            "max_position_embeddings": self.deepseek_params["max_position_embeddings"],
            "rms_norm_eps": 1e-6,
        }
        
        print("âœ… å®Œæ•´é…ç½®:", full_config)
        
        # ç°åœ¨åˆ›å»ºæ¨¡å‹
        model = DeepSeekCompatibleModel(full_config, self.device)
        return model
    def load_model(self, model_path):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        try:
            if os.path.exists(model_path):
                logger.info(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæƒé‡: {model_path}")
                
                # åŠ è½½æƒé‡æ–‡ä»¶
                if model_path.endswith('.safetensors'):
                    from safetensors import safe_open
                    state_dict = {}
                    with safe_open(model_path, framework="pt", device="cpu") as f:
                        for key in f.keys():
                            state_dict[key] = f.get_tensor(key)
                else:
                    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
                    
                    # æå–çŠ¶æ€å­—å…¸
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    else:
                        state_dict = checkpoint
                
                logger.info(f"ğŸ“Š æƒé‡æ–‡ä»¶åŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
                
                # é‡æ–°æ˜ å°„æƒé‡
                remapped_state_dict = remap_deepseek_keys(state_dict)
                
                # åŠ è½½æƒé‡ (ä½¿ç”¨ strict=False å…è®¸éƒ¨åˆ†å‚æ•°ä¸åŒ¹é…)
                missing_keys, unexpected_keys = self.model.load_state_dict(
                    remapped_state_dict, strict=False
                )
                
                # è®°å½•ä¸åŒ¹é…çš„å‚æ•°
                if missing_keys:
                    logger.warning(f"âš ï¸  ç¼ºå¤± {len(missing_keys)} ä¸ªå‚æ•°:")
                    for key in missing_keys[:10]:
                        logger.warning(f"  - {key}")
                    if len(missing_keys) > 10:
                        logger.warning(f"  ... è¿˜æœ‰ {len(missing_keys) - 10} ä¸ªç¼ºå¤±å‚æ•°")
                
                if unexpected_keys:
                    logger.warning(f"âš ï¸  å‘ç° {len(unexpected_keys)} ä¸ªæ„å¤–å‚æ•°:")
                    for key in unexpected_keys[:10]:
                        logger.warning(f"  - {key}")
                    if len(unexpected_keys) > 10:
                        logger.warning(f"  ... è¿˜æœ‰ {len(unexpected_keys) - 10} ä¸ªæ„å¤–å‚æ•°")
                
                logger.info("âœ… æƒé‡åŠ è½½å®Œæˆ")
            else:
                logger.warning("â„¹ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def predict(self, input_data):
        """é¢„æµ‹æ–¹æ³• - éœ€è¦é€‚é…æ–°çš„æ¨¡å‹è¾“å‡ºæ ¼å¼"""
        self.model.eval()
        with torch.no_grad():
            if isinstance(input_data, np.ndarray):
                input_data = torch.from_numpy(input_data).float().to(self.device)
            
            if len(input_data.shape) == 2:
                input_data = input_data.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            if input_data.shape[1] != self.fixed_seq_len:
                raise ValueError(f"âŒ è¾“å…¥åºåˆ—é•¿åº¦åº”ä¸º {self.fixed_seq_len}, ä½†å¾—åˆ° {input_data.shape[1]}")
            
            # æ–°çš„æ¨¡å‹è¾“å‡ºæ˜¯å­—å…¸æ ¼å¼
            output = self.model(input_data)
            
            # æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©è¾“å‡º
            # å¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œå¯èƒ½ä½¿ç”¨ logits
            # å¦‚æœæ˜¯å›å½’ä»»åŠ¡ï¼Œå¯èƒ½ä½¿ç”¨ hidden_states æˆ– last_hidden_state
            if isinstance(output, dict):
                # è¿”å›éšè—çŠ¶æ€ï¼ˆé€‚ç”¨äºç‰¹å¾æå–ï¼‰
                return output["last_hidden_state"].cpu().numpy()
            else:
                # ç›´æ¥è¿”å›è¾“å‡ºï¼ˆé€‚ç”¨äºåˆ†ç±»å¤´ï¼‰
                return output.cpu().numpy()

    def get_embeddings(self, input_data):
        """è·å–è¾“å…¥æ•°æ®çš„åµŒå…¥è¡¨ç¤º"""
        self.model.eval()
        with torch.no_grad():
            if isinstance(input_data, np.ndarray):
                input_data = torch.from_numpy(input_data).float().to(self.device)
            
            if len(input_data.shape) == 2:
                input_data = input_data.unsqueeze(0)
            
            # è·å–è¯åµŒå…¥
            embeddings = self.model.embed_tokens(input_data.long())
            return embeddings.cpu().numpy()

    def save_model(self, save_path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save(self.model.state_dict(), save_path)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "num_layers": self.model.num_layers,
            "hidden_size": self.model.config["hidden_size"],
            "device": str(self.device)
        }


if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨é¢„æµ‹å™¨æµ‹è¯•...")
    predictor = DeepSeekPredictor()
    
    # 2. åŠ è½½å¹¶æ¸…ç†é¢„è®­ç»ƒæƒé‡
    pretrained_path = "/home/liyakun/twitter-stock-prediction/models/model2/best_model.pt"
    if os.path.exists(pretrained_path):
        logger.info("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæƒé‡...")
        pretrained_state_dict = torch.load(pretrained_path, map_location='cpu')
        
        # ğŸ¯ åœ¨è¿™é‡Œè°ƒç”¨é€‚é…å‡½æ•°ï¼
        logger.info("ğŸ”„ é€‚é…DeepSeekæƒé‡æ ¼å¼...")
        adapted_state_dict = adapt_deepseek_weights(pretrained_state_dict, num_heads=6)
        
        # è·å–å½“å‰æ¨¡å‹çš„state_dict
        model_state_dict = predictor.model.state_dict()
        
        # åªåŠ è½½åŒ¹é…çš„å‚æ•°ï¼ˆè¿‡æ»¤æ‰biaså‚æ•°ï¼‰
        filtered_state_dict = {}
        for key in model_state_dict.keys():
            if key in adapted_state_dict:  # ä½¿ç”¨é€‚é…åçš„æƒé‡å­—å…¸
                # åªåŠ è½½weightå‚æ•°ï¼Œè·³è¿‡biaså‚æ•°
                if 'bias' not in key:
                    filtered_state_dict[key] = adapted_state_dict[key]
                    logger.debug(f"âœ… åŠ è½½å‚æ•°: {key}")
                else:
                    logger.debug(f"â­ï¸  è·³è¿‡biaså‚æ•°: {key}")
            else:
                logger.warning(f"âš ï¸  ç¼ºå¤±å‚æ•°: {key}")
        
        # åŠ è½½è¿‡æ»¤åçš„æƒé‡
        predictor.model.load_state_dict(filtered_state_dict, strict=False)
        logger.info(f"âœ… æƒé‡åŠ è½½å®Œæˆï¼Œè·³è¿‡äº†æ‰€æœ‰biaså‚æ•°")
    
    # å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    input_dim = predictor.model_params["input_dim"]
    seq_len = predictor.model_params["seq_len"]
    mock_data = np.random.randn(seq_len, input_dim).astype(np.float32)
    
    logger.info(f"ğŸ§ª æµ‹è¯•è¾“å…¥: shape={mock_data.shape}")

    # æ‰§è¡Œé¢„æµ‹
    try:
        prediction = predictor.predict(mock_data)
        logger.info(f"ğŸ“ˆ é¢„æµ‹ç»“æœ: shape={prediction.shape}, å€¼èŒƒå›´: [{prediction.min():.4f}, {prediction.max():.4f}]")
        
        # æµ‹è¯•æ‰¹é‡é¢„æµ‹
        batch_size = 3
        batch_data = np.random.randn(batch_size, seq_len, input_dim).astype(np.float32)
        batch_prediction = predictor.predict(batch_data)
        logger.info(f"ğŸ“Š æ‰¹é‡é¢„æµ‹ç»“æœ: shape={batch_prediction.shape}")
        
    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
    
    logger.info("âœ… æµ‹è¯•å®Œæˆ")