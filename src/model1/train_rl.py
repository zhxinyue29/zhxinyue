import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import ConcatDataset, DataLoader, Dataset
from pathlib import Path
import numpy as np
import pandas as pd
import yaml                                                                    
import logging
import json
import argparse
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from torch.utils.data import DataLoader, Dataset, random_split
import torch.nn.functional as F 
from contextlib import contextmanager
import inspect
import warnings
from ..model2.inference import load_model, predict, DeepSeekModel
from transformers import AutoTokenizer, AutoModel  


if torch.cuda.is_available():
    device = torch.device("cuda:0")  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPU")
class SafetyModule(nn.Module):  # ğŸ”´ å…³é”®ï¼šç»§æ‰¿nn.Module
    def __init__(self, input_size=773):
        super().__init__()
        self.protector = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.Linear(256, 256)
        )
    
    def forward(self, x):
        return self.protector(x)
    
safety = SafetyModule()

config_path = '/home/liyakun/twitter-stock-prediction/configs/model1.yaml'
def load_model(config_path):
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if not config:
            print("é…ç½®ä¸ºç©ºæˆ–æ ¼å¼æœ‰è¯¯")
            return None
        return config
    except Exception as e:
        print("è¯»å–é…ç½®å¤±è´¥ï¼š", e)
        return None
config = load_model(config_path)


try:
    model_path = config['env']['prediction_model_path']
    print("æ¨¡å‹è·¯å¾„ï¼š", model_path)
except KeyError:
    print("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° prediction_model_path å­—æ®µã€‚")
    # ä½ å¯ä»¥è®¾ç½®é»˜è®¤è·¯å¾„æˆ–æŠ›å‡ºå¼‚å¸¸
if not config:
    raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥ï¼")

# æå–å‚æ•°å’Œæ¨¡å‹è·¯å¾„
try:
    hidden_size = config['env']['hidden_size']
    num_layers = config['env']['num_layers']
    num_heads = config['env']['num_heads']
    model_path = config['env']['prediction_model_path']
    input_dim=config['model']['params']['input_dim']
except KeyError as e:
    raise RuntimeError(f"é…ç½®ç¼ºå°‘å­—æ®µ: {e}")
# æ ¹æ®é…ç½®è·¯å¾„åŠ è½½æ¨¡å‹
if model_path:
    model2 = DeepSeekModel(input_dim,hidden_size, num_layers, num_heads)
    model2 = model2.to(device)
    safety = safety.to(device)
    print("åŠ è½½çš„model2ç±»å‹ï¼š", type(model2))
    assert model2 is not None, "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel2ä¸ºNoneï¼"
else:
    print("æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚")

class ConfigLoader:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config

def setup_logger(log_file: str) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_dir = Path(log_file).parent
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logger = logging.getLogger("training")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ä¹‹å‰çš„handlersé¿å…é‡å¤
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_file)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    return logger

def create_output_directories(config: Dict[str, Any]) -> None:
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•"""
    paths = config['paths']
    # æ¨¡å‹è¾“å‡ºç›®å½• (model1_output/)
    output_dir = Path(paths.get('processed_output_dir', 'data/processed/model1_output'))
    output_dir.mkdir(parents=True, exist_ok=True)
    # è¯„ä¼°ç»“æœç›®å½• (model1_eval/)
    eval_dir = Path(paths.get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_dir = Path(config['paths']['output_model']).parent
    model_save_dir.mkdir(parents=True, exist_ok=True)
    # æ—¥å¿—æ–‡ä»¶ç›®å½•
    log_dir = Path(config['paths']['log_file']).parent
    log_dir.mkdir(parents=True, exist_ok=True)

def convert_to_serializable(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(obj)
    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.detach().cpu().tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]
    else:
        return obj

def save_structured_data(data: pd.DataFrame, config: Dict[str, Any], filename: str = "processed_data.csv") -> Path:
    """
    ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_outputç›®å½•
    """
    output_dir = Path(config['paths'].get('processed_output_dir', 'data/processed/model1_output'))
    output_path = output_dir / filename 
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    # æ·»åŠ æ—¥æœŸåˆ—ä½œä¸ºç»“æ„åŒ–æ•°æ®çš„ä¸€éƒ¨åˆ†
    if 'date' not in data.columns:
        if 'timestamp' in data.columns:
            data['date'] = pd.to_datetime(data['timestamp']).dt.date
        else:
            data['date'] = pd.Timestamp.now().strftime("%Y-%m-%d")
    data.to_csv(output_path, index=False)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°: {output_path}")
    return output_path

def save_evaluation_results(results: Dict[str, Any], config: Dict[str, Any], filename: str = "evaluation_results.json") -> Tuple[Path, Path]:
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_evalç›®å½•
    """
    eval_dir = Path(config['paths'].get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    output_path = eval_dir / filename
    results_serializable = convert_to_serializable(results)
    with open(output_path, 'w') as f:
        json.dump(results_serializable, f, indent=4)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ä¾¿äºåˆ†æ
    csv_path = output_path.with_suffix('.csv')
    eval_df = pd.DataFrame([results])
    eval_df.to_csv(csv_path, index=False)
    logger.info(f"âœ… åŒæ—¶ä¿å­˜CSVæ ¼å¼è¯„ä¼°ç»“æœ: {csv_path}")
    
    return output_path, csv_path

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    """è®¡ç®—å¤æ™®æ¯”ç‡"""
    excess_returns = returns - risk_free_rate
    return excess_returns.mean() / (excess_returns.std() + 1e-8)

def max_drawdown(values: np.ndarray) -> float:
    """è®¡ç®—æœ€å¤§å›æ’¤"""
    peak = np.maximum.accumulate(values)
    drawdown = (values - peak) / peak
    return np.min(drawdown)
class SafetyController:
    def __init__(self, model, optimizer, config, logger):
        self.model = model
        self.optimizer = optimizer
        self.config = config['training']['stability']
        self.logger = logger
        self.epoch_backup_count = 0
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        self.backup_dir = Path(f"{config['paths']['log_file']}_safety_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        # å­¦ä¹ ç‡å¤‡ä»½
        self.original_lr = optimizer.param_groups[0]['lr']
    
    def check_inputs(self, inputs, targets):
        """è¾“å…¥æ•°æ®å¥åº·æ£€æŸ¥"""
        nan_count = torch.isnan(inputs).sum().item() + torch.isnan(targets).sum().item()
        inf_count = torch.isinf(inputs).sum().item() + torch.isinf(targets).sum().item()
        
        if nan_count > self.config.get('max_nan_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«NaNæ•°æ®çš„æ‰¹æ¬¡ (NaNæ•°é‡: {nan_count})")
            return False
        
        if inf_count > self.config.get('max_inf_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«Infæ•°æ®çš„æ‰¹æ¬¡ (Infæ•°é‡: {inf_count})")
            return False 
        return True
    
    def protect_outputs(self, outputs):
        """æ¨¡å‹è¾“å‡ºé˜²æŠ¤"""
        if torch.isnan(outputs).any():
            self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œæ‰§è¡Œä¿®å¤...")
            outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1e4, neginf=-1e4)
          # ä¿®å¤NaN/Inféœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆä½¿ç”¨torch.whereå¯å¯¼ï¼‰
        outputs = torch.where(
       torch.isnan(outputs) | torch.isinf(outputs),
       torch.zeros_like(outputs),
       outputs
   )    
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        outputs = torch.clamp(outputs, 
                              min=self.config['output_clip_range'][0], 
                              max=self.config['output_clip_range'][1]) 
        return outputs
    
    def check_gradients(self):
        """æ¢¯åº¦å¥åº·æ£€æŸ¥å’Œä¿®å¤"""
        max_grad_norm = self.config.get('max_grad_norm', 1.0)
        problematic_layers = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad              
                # ä¿®å¤NaN/Infæ¢¯åº¦
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    problematic_layers.append(name)
                    param.grad = torch.where(
                        torch.isnan(grad) | torch.isinf(grad),
                        torch.zeros_like(grad),
                        grad
                    )            
                # æ¢¯åº¦è£å‰ª
                torch.clamp_(param.grad, -self.config['gradient_clip_value'], 
                            self.config['gradient_clip_value'])
        
        # å…¨å±€æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_grad_norm) 
        if problematic_layers:
            self.logger.warning(f"ä¿®å¤æ¢¯åº¦å¼‚å¸¸å±‚: {problematic_layers[:3]}{'...' if len(problematic_layers)>3 else ''}")
        
        return len(problematic_layers) == 0
    
    def create_backup(self, epoch):
        """åˆ›å»ºå®‰å…¨æ¢å¤ç‚¹"""
        if epoch % self.config.get('backup_interval', 5) == 0:
            backup_path = self.backup_dir / f"epoch_{epoch}_safety_backup.pt"
            torch.save({
                'model_state': self.model.state_dict(),
                'optimizer_state': self.optimizer.state_dict(),
                'epoch': epoch
            }, backup_path)
            
            # ä¿å­˜æœ€è¿‘çš„5ä¸ªå¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if len(backup_files) > 5:
                os.remove(backup_files[0])
    
    def recover(self, current_epoch):
        """ä»é”™è¯¯ä¸­æ¢å¤"""
        recovery_type = self.config.get('recovery_strategy', 'backoff')    
        if recovery_type == 'backoff':
            # å­¦ä¹ ç‡é€€é¿
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= self.config.get('lr_backoff_factor', 0.5)            
            self.logger.warning(f"é‡‡ç”¨å­¦ä¹ ç‡é€€é¿ç­–ç•¥ï¼Œæ–°å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")            
            # å°è¯•æ¢å¤æœ€è¿‘çš„å¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if backup_files:
                latest_backup = backup_files[-1]
                checkpoint = torch.load(latest_backup)
                self.model.load_state_dict(checkpoint['model_state'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                self.logger.warning(f"ä»å¤‡ä»½æ¢å¤: {latest_backup.name} (epoch {checkpoint['epoch']})")
                return checkpoint['epoch']  # è¿”å›æ¢å¤åˆ°çš„epoch
            
        elif recovery_type == 'reset':
            # å®Œå…¨é‡ç½®æ¨¡å‹
            self.logger.error("æ‰§è¡Œæ¨¡å‹å®Œå…¨é‡ç½®!")            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            for module in self.model.modules():
                if hasattr(module, 'reset_parameters'):
                    module.reset_parameters()            
            # é‡ç½®ä¼˜åŒ–å™¨
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.original_lr            
            return 0  # ä»å¤´å¼€å§‹è®­ç»ƒ
        
        return current_epoch
    
    @contextmanager
    def safe_forward_context(self):
        """å®‰å…¨å‰å‘ä¼ æ’­ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # å¯ç”¨PyTorchå¼‚å¸¸æ£€æµ‹
            torch.autograd.set_detect_anomaly(True)
            yield
        except RuntimeError as e:
            self.logger.error(f"å‰å‘ä¼ æ’­å¼‚å¸¸: {str(e)}")
            # æå–å¼‚å¸¸ä½ç½®
            stack = inspect.stack()
            caller = stack[1]  # è°ƒç”¨safe_forward_contextçš„å‡½æ•°
            self.logger.error(f"å¼‚å¸¸ä½ç½®: {caller.filename}:{caller.lineno}")
            raise
        finally:
            torch.autograd.set_detect_anomaly(False)

# === æ–°å¢: æ¢¯åº¦ç›‘æ§å™¨ ===
class GradientMonitor:
    def __init__(self, model, layers_to_watch=None):
        self.model = model
        self.layers = layers_to_watch or self._identify_critical_layers()
        self.handles = []
        self.logger = logging.getLogger("gradient_monitor")
        self.reset_stats()
    
    def _identify_critical_layers(self):
        """è¯†åˆ«å…³é”®å±‚ï¼ˆæœ€åä¸€å±‚å’Œå«LayerNormçš„å±‚ï¼‰"""
        critical_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
                critical_layers.append(name)
            if 'out' in name or 'final' in name or 'proj' in name:
                critical_layers.append(name)
        return list(set(critical_layers))
    
    def reset_stats(self):
        self.max_grad = 0.0
        self.max_grad_layer = None
        self.nan_count = 0
    
    def _hook_fn(self, module, grad_input, grad_output, layer_name):
        # æ£€æŸ¥NaNå’ŒInf
        for g in grad_output:
            if g is not None:
                if torch.isnan(g).any():
                    self.nan_count += torch.isnan(g).sum().item()
                if torch.isinf(g).any():
                    self.nan_count += torch.isinf(g).sum().item()        
        # è®°å½•æœ€å¤§æ¢¯åº¦
        for g in grad_output:
            if g is not None:
                grad_norm = torch.norm(g)
                if grad_norm > self.max_grad:
                    self.max_grad = grad_norm.item()
                    self.max_grad_layer = layer_name
    
    def attach(self):
        """é™„åŠ æ¢¯åº¦é’©å­"""
        self.reset_stats()
        for name in self.layers:
            module = self._get_module(name)
            hook = lambda m, gi, go, n=name: self._hook_fn(m, gi, go, n)
            handle = module.register_full_backward_hook(hook)
            self.handles.append(handle)
    
    def detach(self):
        """ç§»é™¤æ¢¯åº¦é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles = []
    
    def _get_module(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å—"""
        names = name.split('.')
        module = self.model
        for n in names:
            module = getattr(module, n)
        return module
    
    def report(self, batch_idx, epoch):
        """ç”Ÿæˆæ¢¯åº¦æŠ¥å‘Š"""
        report = f"æ¢¯åº¦ç›‘æ§ - Epoch {epoch} Batch {batch_idx}:\n"
        report += f"  ğŸš© æœ€å¤§æ¢¯åº¦: {self.max_grad:.2e} ({self.max_grad_layer})\n"
        report += f"  âš ï¸ å¼‚å¸¸æ¢¯åº¦è®¡æ•°: {self.nan_count}"
        
        if self.nan_count > 0:
            self.logger.warning(report)
        elif batch_idx % 10 == 0:
            self.logger.info(report)


# === æ–°å¢: SmoothL1æŸå¤±å‡½æ•° ===
class RLoss(nn.Module):
    def __init__(self, supervised_criterion, base_loss_weight=0.5):
        super().__init__()
        self.base_loss_weight = base_loss_weight
        self.supervised_criterion = supervised_criterion
    
    def forward(self, model_outputs, targets, reward):
        supervised_loss = self.supervised_criterion(model_outputs, targets)
        # 3. åˆ†æƒ…å†µå¤„ç†çš„ç­–ç•¥æŸå¤±
        policy_loss, match_rate = self.calculate_policy_loss(
            model_outputs, 
            targets,
            reward,
        )
        print("ç›‘ç£æŸå¤± grad_fn:",supervised_loss.grad_fn)
        print("ç­–ç•¥æŸå¤± grad_fn:", policy_loss.grad_fn) 
        # 4. åŠ¨æ€è°ƒæ•´æƒé‡
        volatility = torch.abs(targets).mean()
        current_weight = self.dynamic_weight_adjust(volatility)

        # 5. æ··åˆæŸå¤± - ç»¼åˆæ‰€æœ‰æˆåˆ†
        total_loss = (
            current_weight * supervised_loss +
            (1 - current_weight) * 0.7 * policy_loss
        )
        # è¿”å›æŸå¤±å’Œç›¸å…³æŒ‡æ ‡
        return {
            "total_loss": total_loss,
            "supervised_loss": supervised_loss,
            "policy_loss": policy_loss,
            "weight": current_weight,
            "mean_reward": reward.mean(),
            "match_rate": match_rate
        }

    def calculate_policy_loss(self, model_outputs, targets, reward):
        logits = model_outputs["logits"]        
        # ç¡®ä¿å¤„ç†æ‰¹é‡æ•°æ®
        if logits.dim() == 1:
            logits = logits.unsqueeze(0)  
        # å¯¹ç‰¹å¾ç»´åº¦åº”ç”¨softmax (dim=1)
        action_probs = F.softmax(logits, dim=1)        
        # æ¯ä¸ªæ ·æœ¬é€‰æ‹©æœ€å¤§æ¦‚ç‡åŠ¨ä½œ
        position_direction = torch.argmax(action_probs, dim=1)  # [batch_size]        
        # é£é™©åˆ¤æ–­ - ç¡®ä¿targetsé€‚å½“å½¢çŠ¶
        if targets.dim() > 1:
            targets = targets.squeeze()
        risk_mask = targets < -0.05  # [batch_size]        
        # æœ‰æ•ˆåŠ¨ä½œè®¾ç½®
        valid_actions = torch.zeros_like(position_direction)
        valid_actions[risk_mask] = 3        
        # è®¡ç®—åŒ¹é…ç‡
        directional_match = (position_direction == valid_actions).float()        
        # å®‰å…¨é€‰æ‹©æ¦‚ç‡ï¼ˆé¿å…in-placeæ“ä½œï¼‰
        batch_indices = torch.arange(logits.size(0))
        chosen_probs = action_probs[batch_indices, position_direction]
        log_probs = torch.log(chosen_probs + 1e-8)        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±
        with torch.no_grad():
            advantage = reward * (1.0 + 0.5 * directional_match)
        
        rl_loss = -torch.mean(log_probs * advantage)
        
        return rl_loss, directional_match.mean()
    
    def dynamic_weight_adjust(self, volatility):
        """æ ¹æ®å¸‚åœºæ³¢åŠ¨ç‡è°ƒæ•´ç›‘ç£æŸå¤±æƒé‡ (å®Œå…¨ä¿ç•™)"""
        return torch.clamp(0.65 - volatility * 25, min=0.2, max=0.7).item()

def get_rl_weight(epoch):
    """æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ç›‘ç£æŸå¤±æƒé‡"""
    # ç¬¬ä¸€é˜¶æ®µï¼šä¾§é‡ç›‘ç£å­¦ä¹ ï¼ˆepoch 0-9ï¼‰
    if epoch < 10:
        return 0.7
    # ç¬¬äºŒé˜¶æ®µï¼šå¹³è¡¡å­¦ä¹ ï¼ˆepoch 10-19ï¼‰
    elif epoch < 20:
        # çº¿æ€§è¿‡æ¸¡ï¼š0.7 â†’ 0.5
        return 0.7 - 0.2 * (epoch - 9) / 10
    # ç¬¬ä¸‰é˜¶æ®µï¼šä¾§é‡å¼ºåŒ–å­¦ä¹ ï¼ˆepoch â‰¥20ï¼‰
    else:
        return 0.3
    
class SafeSmoothL1Loss(nn.Module):
    """æ•°å€¼ç¨³å®šçš„SmoothL1æŸå¤±"""
    def __init__(self, beta=1.0):
        super().__init__()
        self.beta = beta
     
    def forward(self, model_outputs, target):
        if isinstance(model_outputs, dict):
        # å°è¯•å¸¸è§é”®åï¼ˆæ ¹æ®å®é™…æ¨¡å‹è¾“å‡ºé”®åè°ƒæ•´ï¼‰
            if "logits" in model_outputs:
                input_tensor = model_outputs["logits"]
            elif "value" in model_outputs:
                input_tensor = model_outputs["value"]
            else:
                # å¦‚æœæ²¡æœ‰æ ‡å‡†é”®åï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
                input_tensor = next(iter(model_outputs.values()))
        else:
            input_tensor = model_outputs
        print("model_outputs",input_tensor)
        safe_input = torch.clamp(input_tensor, -50, 50)
        safe_target = torch.clamp(target, -50, 50)
        diff = safe_input - safe_target
        diff = torch.where(torch.isinf(diff), torch.zeros_like(diff), diff)
        diff = torch.where(torch.isnan(diff), torch.zeros_like(diff), diff)
        
        abs_diff = torch.abs(diff)
        mask = abs_diff < self.beta        
        # L2éƒ¨åˆ†: 0.5 * x^2 / beta
        l2_loss = 0.5 * torch.pow(diff, 2) / self.beta
        # L1éƒ¨åˆ†: |x| - 0.5 * beta
        l1_loss = abs_diff - 0.5 * self.beta        
        loss = torch.where(mask, l2_loss, l1_loss)
        loss = loss.mean()
        if torch.isnan(loss) or torch.isinf(loss):
            warnings.warn("æ£€æµ‹åˆ°NaN/InfæŸå¤±å€¼ï¼Œæ‰§è¡Œä¿®å¤!")
            return torch.zeros_like(loss)
            
        return loss

class CSVDataset(Dataset):
    def __init__(self, x_file: str, y_file: str, dates_file: str, 
                 filter_nan=True, 
                 sequence_length=30,
                 feature_names=None):
        """
        é’ˆå¯¹å¸‚åœºæ¨ç‰¹æ•°æ®ä¼˜åŒ–çš„CSVDataset
        
        å‚æ•°:
            x_file: åŒ…å«ç‰¹å¾çš„CSVæ–‡ä»¶è·¯å¾„
            y_file: åŒ…å«æ ‡ç­¾çš„CSVæ–‡ä»¶è·¯å¾„
            dates_file: åŒ…å«æ—¥æœŸçš„CSVæ–‡ä»¶è·¯å¾„
            filter_nan: æ˜¯å¦è¿‡æ»¤åŒ…å«NaNçš„æ ·æœ¬ (é»˜è®¤ä¸ºTrue)
            sequence_length: æ—¶é—´åºåˆ—é•¿åº¦ (é»˜è®¤ä¸º30)
            feature_names: å¯é€‰ç‰¹å¾åç§°åˆ—è¡¨
        """
        print("Loading data files:")
        print("x_file:", x_file)
        print("y_file:", y_file)
        print("dates_file:", dates_file)        
        try:
            # 1. åŠ è½½æ•°æ®æ–‡ä»¶
            self.df = pd.read_csv(x_file)
            self.label_df = pd.read_csv(y_file) if y_file else None
            self.dates_df = pd.read_csv(dates_file) if dates_file else None            
            # æ‰“å°åŸºæœ¬ä¿¡æ¯
            print("Data shapes:")
            print("x_data:", self.df.shape)
            print("y_data:", self.label_df.shape if self.label_df is not None else "None")
            print("dates:", self.dates_df.shape if self.dates_df is not None else "None")            
        except Exception as e:
            print(f"Error loading CSV files: {e}")
            raise
            
        # 2. è§£æJSONå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._parse_json_columns()
        
        # 3. åˆå¹¶data, label, date
        self._merge_dataframes()
        
        # 4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        assert len(self.x_data) == len(self.y_data), "xå’Œyè¡Œæ•°ä¸ä¸€è‡´"
        if self.dates_df is not None:
            assert len(self.x_data) == len(self.dates_df), "xå’Œdatesè¡Œæ•°ä¸ä¸€è‡´"
            
        # 5. åˆ›å»ºæ—¶é—´åºåˆ—
        self._create_sequences(sequence_length)
        
        # 6. è¿‡æ»¤NaNå€¼
        if filter_nan:
            self._filter_nan_samples()
            
        self.feature_names = feature_names
        if self.feature_names is None:
            self.feature_names = [f'Feature_{i}' for i in range(self.x_data.shape[1])]
        
        # 7. æ‰“å°æœ€ç»ˆå½¢çŠ¶
        print(f"åˆ›å»ºåºåˆ—åå½¢çŠ¶ - x_data: {self.x_data.shape}, y_data: {self.y_data.shape}")
        
    def _parse_json_columns(self):
        """è§£æCSVä¸­çš„JSONæ ¼å¼åˆ—"""
        print("è§£æJSONæ ¼å¼åˆ—...")
        json_columns = []
        
        # æ£€æŸ¥dfä¸­çš„JSONåˆ—
        for col in self.df.columns:
            if self.df[col].dtype == 'object' and self.df[col].str.startswith('{').any():
                json_columns.append(col)
                self.df[col] = self.df[col].apply(self._safe_parse_json)
                print(f"è§£æJSONåˆ—: {col}")
        
        # åˆ›å»ºç‰¹å¾åˆ—
        self._create_feature_columns()
    
    def _safe_parse_json(self, json_str):
        """å®‰å…¨è§£æJSONå­—ç¬¦ä¸²"""
        if pd.isnull(json_str):
            return {}
        try:
            # å¤„ç†ä¸æ ‡å‡†çš„å¼•å·
            json_str = json_str.replace('""', '"')
            return json.loads(json_str)
        except (json.JSONDecodeError, TypeError):
            try:
                # å°è¯•æ›¿æ¢å•å¼•å·
                json_str = json_str.replace("'", '"')
                return json.loads(json_str)
            except:
                return {}
    
    def _create_feature_columns(self):
        """ä»JSONç»“æ„ä¸­åˆ›å»ºç‰¹å¾åˆ—"""
        # å¸‚åœºæ•°æ®ç‰¹å¾
        if 'market_context' in self.df.columns:
            market_data = self.df['market_context']
            self.df['open'] = market_data.apply(lambda x: x.get('open', np.nan))
            self.df['high'] = market_data.apply(lambda x: x.get('high', np.nan))
            self.df['low'] = market_data.apply(lambda x: x.get('low', np.nan))
            self.df['close'] = market_data.apply(lambda x: x.get('close', np.nan))
            self.df['volume'] = market_data.apply(lambda x: x.get('volume', np.nan))
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        if 'technical_indicators' in self.df.columns:
            tech_data = self.df['technical_indicators']
            self.df['sma'] = tech_data.apply(lambda x: x.get('sma', np.nan))
            self.df['ema'] = tech_data.apply(lambda x: x.get('ema', np.nan))
            self.df['rsi'] = tech_data.apply(lambda x: x.get('rsi', np.nan))
            self.df['bollinger_upper'] = tech_data.apply(lambda x: x.get('bollinger_upper', np.nan))
            self.df['bollinger_lower'] = tech_data.apply(lambda x: x.get('bollinger_lower', np.nan))
        
        # æ¨ç‰¹å…ƒæ•°æ®ç‰¹å¾
        if 'tweet_metadata' in self.df.columns:
            tweet_data = self.df['tweet_metadata']
            self.df['retweet_count'] = tweet_data.apply(lambda x: x.get('retweet_count', 0))
            self.df['favorite_count'] = tweet_data.apply(lambda x: x.get('favorite_count', 0))
            self.df['mention_count'] = tweet_data.apply(lambda x: len(x.get('mentioned_symbols', [])))
    
    def _merge_dataframes(self):
        """åˆå¹¶data, labelå’Œdateæ•°æ®å¸§"""
        # æå–åŸºç¡€ç‰¹å¾åˆ—
        base_features = ['symbol', 'timestamp', 'sentiment', 'text',
                         'open', 'high', 'low', 'close', 'volume',
                         'sma', 'ema', 'rsi', 'bollinger_upper', 'bollinger_lower',
                         'retweet_count', 'favorite_count', 'mention_count']
        
        # åˆ›å»ºæœ€ç»ˆç‰¹å¾çŸ©é˜µ
        feature_cols = [col for col in base_features if col in self.df.columns]
        self.x_data = self.df[feature_cols].values
        
        # åˆ›å»ºæ ‡ç­¾å‘é‡
        if self.label_df is not None:
            # å¦‚æœæ˜¯å•åˆ—æ ‡ç­¾æ–‡ä»¶
            if len(self.label_df.columns) == 1:
                self.y_data = self.label_df.values
            else:
                # å°è¯•ä»æ ‡ç­¾æ–‡ä»¶è¯†åˆ«ç›®æ ‡åˆ—
                for col in ['target', 'label', 'close']:
                    if col in self.label_df.columns:
                        self.y_data = self.label_df[col].values
                        break
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ ‡ç­¾åˆ—ï¼ŒæŠ›å‡ºä¸€ä¸ªè¯¦ç»†çš„é”™è¯¯
                    available_cols = list(self.label_df.columns)
                    if len(available_cols) > 5:
                        available_cols = available_cols[:5] + ["..."]
                    
                    raise ValueError(
                        f"âŒ æ— æ³•è¯†åˆ«æ ‡ç­¾åˆ—ã€‚å€™é€‰åˆ—å {self.label_df.columns} å‡ä¸å­˜åœ¨äºæ ‡ç­¾æ–‡ä»¶ä¸­ã€‚\n"
                        f"æ ‡ç­¾æ–‡ä»¶åŒ…å«çš„åˆ—: {available_cols}\n"
                        f"è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è§£å†³:\n"
                        f"1. åœ¨æ ‡ç­¾æ–‡ä»¶ä¸­æ·»åŠ  'target' æˆ– 'label' åˆ—\n"
                        f"2. æŒ‡å®šä»£ç ä¸­ä½¿ç”¨çš„å®é™…æ ‡ç­¾åˆ—å"
                )
        else:
            # å¦‚æœæ²¡æä¾›æ ‡ç­¾æ–‡ä»¶ï¼Œå°è¯•ä»ç‰¹å¾æ•°æ®ä¸­æå–æ ‡ç­¾
            if 'close' in self.df.columns:
                print("è­¦å‘Šï¼šä½¿ç”¨ç‰¹å¾æ•°æ®ä¸­çš„'close'ä½œä¸ºæ ‡ç­¾")
                self.y_data = self.df['close'].values
            else:
                raise ValueError("æ— æ³•ç¡®å®šæ ‡ç­¾åˆ—")
        
        # åˆ›å»ºæ—¥æœŸå‘é‡
        if self.dates_df is not None:
            # å°è¯•è¯†åˆ«æ—¥æœŸåˆ—
            for col in ['date', 'timestamp', 'time']:
                if col in self.dates_df.columns:
                    self.dates = self.dates_df[col].values
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå–ç¬¬ä¸€åˆ—ä½œä¸ºæ—¥æœŸ
                    available_cols = list(self.dates_df.columns)
                    if len(available_cols) > 5:
                        available_cols = available_cols[:5] + ["..."]
                    
                    raise ValueError(
                        f"âŒ æ— æ³•è¯†åˆ«æ ‡ç­¾åˆ—ã€‚å€™é€‰åˆ—å {self.label_df.columns} å‡ä¸å­˜åœ¨äºæ ‡ç­¾æ–‡ä»¶ä¸­ã€‚\n"
                        f"æ ‡ç­¾æ–‡ä»¶åŒ…å«çš„åˆ—: {available_cols}\n"
                        f"è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è§£å†³:\n"
                        f"1. åœ¨æ ‡ç­¾æ–‡ä»¶ä¸­æ·»åŠ  'target' æˆ– 'label' åˆ—\n"
                        f"2. æŒ‡å®šä»£ç ä¸­ä½¿ç”¨çš„å®é™…æ ‡ç­¾åˆ—å"
                )
        else:
            # å¦‚æœæ²¡æä¾›æ—¥æœŸæ–‡ä»¶ï¼Œå°è¯•ä»ç‰¹å¾æ•°æ®ä¸­æå–æ—¥æœŸ
            if 'timestamp' in self.df.columns:
                print("è­¦å‘Šï¼šä½¿ç”¨ç‰¹å¾æ•°æ®ä¸­çš„'timestamp'ä½œä¸ºæ—¥æœŸ")
                self.dates = self.df['timestamp'].values
            else:
                self.dates = np.arange(len(self.x_data))
    
    def _create_sequences(self, sequence_length):
        """åˆ›å»ºæ—¶é—´åºåˆ—åºåˆ—"""
        sequences = []
        labels = []
        date_sequences = []
        
        # æŒ‰æ—¶é—´åºåˆ—é•¿åº¦åˆ›å»ºåºåˆ—
        for i in range(len(self.x_data) - sequence_length):
            sequences.append(self.x_data[i:i+sequence_length])
            labels.append(self.y_data[i+sequence_length])  # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æ ‡ç­¾
            date_sequences.append(self.dates[i:i+sequence_length])
        
        self.x_data = np.array(sequences)
        self.y_data = np.array(labels)
        self.dates = np.array(date_sequences)
        
        print(f"åˆ›å»ºæ—¶é—´åºåˆ—: {len(self.x_data)} ä¸ªåºåˆ— (é•¿åº¦: {sequence_length})")
        print(f"åºåˆ—å½¢çŠ¶: x_data {self.x_data.shape}, y_data {self.y_data.shape}")
    
    def _filter_nan_samples(self):
        """è¿‡æ»¤åŒ…å«NaNçš„æ ·æœ¬"""
        original_count = len(self.x_data)
        
        # åˆ›å»ºæœ‰æ•ˆç´¢å¼•åˆ—è¡¨
        valid_indices = [
            i for i in range(original_count)
            if not np.isnan(self.x_data[i]).any() and not np.isnan(self.y_data[i]).any()
        ]
        
        # åº”ç”¨è¿‡æ»¤
        self.x_data = self.x_data[valid_indices]
        self.y_data = self.y_data[valid_indices]
        self.dates = self.dates[valid_indices] if self.dates is not None else None
        
        filtered_count = original_count - len(valid_indices)
        print(f"è¿‡æ»¤æ‰åŒ…å«NaNçš„æ ·æœ¬: {filtered_count}/{original_count}")
    
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, idx):
        data = self.x_data[idx]
        label = self.y_data[idx]
        date = self.dates[idx]
        
        # è½¬æ¢ä¸ºTensor
        data = torch.tensor(data, dtype=torch.float32) 
        label = torch.tensor(label, dtype=torch.float32)
        
        # æ•°æ®æ¸…æ´—
        if torch.isnan(data).any():
            data = torch.nan_to_num(data, nan=0.0)
        if torch.isinf(data).any():
            data = torch.nan_to_num(data, posinf=1e6, neginf=-1e6)
        
        # è¿”å›åŒ…å«äº‹ä»¶æ ‡ç­¾çš„ä¸‰å…ƒç»„
        return data, label, date
    
    def get_event_names(self):
        """è·å–æ‰€æœ‰äº‹ä»¶åç§°åˆ—è¡¨"""
        return []  # æ‚¨çš„åŸå§‹ä»£ç ä¸­æœ‰æ­¤å‡½æ•°
    
    def get_special_event_id(self):
        """è·å–ç‰¹æ®Šäº‹ä»¶ID"""
        return 3  # æ‚¨çš„åŸå§‹ä»£ç ä¸­æœ‰æ­¤å‡½æ•°

class TransformerModel(nn.Module):
    """Transformeræ¨¡å‹å®šä¹‰ï¼ŒåŒ¹é…é¢„è®­ç»ƒæƒé‡ç»“æ„"""
    def __init__(self, model_name: str, output_size: int = 4):
        super().__init__()
        self.model_name = model_name
        self.output_size = output_size
        
        # 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.encoder = AutoModel.from_pretrained(model_name)
        
        # 2. æ·»åŠ é€‚é…å±‚è¾“å‡ºå‘é‡
        encoder_hidden_size = self.encoder.config.hidden_size
        self.vector_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(encoder_hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, output_size)
        )
        self.output_layer = None
        self.set_output_size(output_size)
        # 3. è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token or '[PAD]'
    def set_output_size(self, size: int = None):
        """åŠ¨æ€è®¾ç½®è¾“å‡ºç»´åº¦(ä¹Ÿå¯ä¸è®¾ç½®)"""
        if size is None:
            # ä¿ç•™åŸå§‹ç»´åº¦
            self.output_size = self.hidden_size
            if hasattr(self, 'output_layer'):
                del self.output_layer  # ç§»é™¤é€‚é…å±‚
            self.output_layer = nn.Identity()
            print(f"è¾“å‡ºå±‚ï¼šä¿ç•™åŸå§‹ç»´åº¦ {self.hidden_size}")
        else:
            # æ·»åŠ /è°ƒæ•´é€‚é…å±‚
            self.output_size = size
            if hasattr(self, 'output_layer') and isinstance(self.output_layer, nn.Linear):
                # è°ƒæ•´ç°æœ‰é€‚é…å±‚
                self.output_layer.out_features = size
            else:
                # æ–°å»ºé€‚é…å±‚
                self.output_layer = nn.Linear(self.hidden_size, size)
            print(f"è¾“å‡ºå±‚ï¼šé€‚é…åˆ° {size} ç»´")

    def forward(self, inputs) -> torch.Tensor:  # ç›´æ¥è¿”å›å¼ é‡
        """ç›´æ¥è¿”å›åä¸º outputs çš„å‘é‡ï¼ˆä¸æ˜¯å­—å…¸ï¼ï¼‰"""
        # å¤„ç†æ–‡æœ¬è¾“å…¥æˆ–å¼ é‡è¾“å…¥
        if isinstance(inputs, dict) and 'text' in inputs:
            # æ–‡æœ¬è¾“å…¥æ¨¡å¼
            tokenized = self.tokenizer(
                inputs['text'],
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=512
            ).to(self.encoder.device)
            input_ids = tokenized.input_ids
            attention_mask = tokenized.attention_mask
        else:
            # å¼ é‡è¾“å…¥æ¨¡å¼
            input_ids = inputs['input_ids'] if isinstance(inputs, dict) else inputs
            attention_mask = inputs['attention_mask'] if isinstance(inputs, dict) else None
        
        # æå–æ–‡æœ¬è¡¨ç¤º
        encoder_outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # ä½¿ç”¨[CLS]æ ‡è®°ä½œä¸ºåºåˆ—è¡¨ç¤º
        last_hidden_state = encoder_outputs.last_hidden_state
        cls_token = last_hidden_state[:, 0, :] if last_hidden_state.dim() == 3 else last_hidden_state
        
        # ç”Ÿæˆè¾“å‡ºå‘é‡ - å‘½åä¸º outputs
        outputs = self.vector_head(cls_token)  # <-- ç›´æ¥è¿”å›å¼ é‡è€Œéå­—å…¸
        
        return outputs  # è¿”å›åä¸º outputs çš„å‘é‡ï¼
    
    @classmethod
    def load_pretrained(cls, config: Dict[str, Any]) -> 'TransformerModel':
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        logger = logging.getLogger("training")
        prediction_model_path = config['paths']['output_model']
        model_params = config['model']['params']
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(
            model_name=model_params['model_name'],
            output_size=model_params.get('output_size')
        )
        model = model.to(device)
        safety = SafetyModule()
        safety = safety.to(device)
        # å¦‚æœé¢„è®­ç»ƒæ¨¡å‹å­˜åœ¨ï¼ŒåŠ è½½æƒé‡
        if os.path.exists(prediction_model_path):
            logger.info(f"âš™ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {prediction_model_path}")
            pretrained_dict = torch.load(prediction_model_path)
            model.load_state_dict(pretrained_dict, strict=False)
            logger.info(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡")
        else:
            logger.warning(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æœªæ‰¾åˆ°: {prediction_model_path}")
        
        return model

def compute_reward(model2_result, targets,lookback=5):
    """æ¨¡æ‹Ÿå®é™…äº¤æ˜“æ•ˆæœçš„å¥–åŠ±"""
    # è·å–æ¨¡å‹é¢„æµ‹åºåˆ—
    predictions = model2_result[:, :lookback]
    
    # æ„å»ºæ¨¡æ‹ŸæŒä»“
    portfolio = torch.zeros_like(targets)
    total_return = torch.zeros_like(targets)
    
    # åŸºäºè¿ç»­é¢„æµ‹çš„ä»“ä½å˜åŒ–
    for t in range(1, lookback):
        # åŸºäºé¢„æµ‹ä¿¡å·è°ƒæ•´ä»“ä½
        position_change = predictions[:, t] - predictions[:, t-1]
        portfolio += position_change
        
        # è®¡ç®—æ¯æ—¥æ”¶ç›Š
        daily_return = portfolio * targets[:, t]
        total_return += daily_return
        
        # é£é™©ç®¡ç†: å¼ºåˆ¶å¹³ä»“è§„åˆ™
        portfolio = torch.where(torch.abs(portfolio) > 2, 
                                torch.clamp(portfolio, -1, 1), 
                                portfolio)
    
    # æœ€ç»ˆç»„åˆä»·å€¼ä½œä¸ºå¥–åŠ±
    final_value = 1 + total_return
    reward = final_value - 1  # ç»„åˆæ”¶ç›Šä½œä¸ºå¥–åŠ±
    return reward

def get_actual_price(stock_code, date):
    # æ–‡ä»¶è·¯å¾„ç¤ºä¾‹
    csv_file = os.path.join('data', 'stock_prices', f'{stock_code}.csv')
    df = pd.read_csv(csv_file)

    # å‡è®¾csvæœ‰åˆ—ï¼š'date', 'close'
    row = df[df['date'] == date]
    if row.empty:
        print(f"æœªæ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„è‚¡ä»·ï¼š{date}")
        return None
    return float(row['close'].values[0])

def get_data_files_from_dir(dir_path: str) -> Dict[str, str]:
    """å®‰å…¨åœ°ä»ç›®å½•è¯†åˆ«CSVæ•°æ®æ–‡ä»¶ï¼ˆç‰¹å¾/Xï¼Œæ ‡ç­¾/Yå’Œæ—¥æœŸ/datesæ–‡ä»¶ï¼‰"""
    path_obj = Path(dir_path)
    if not path_obj.exists() or not path_obj.is_dir():
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶å¤¹: {dir_path}")
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = list(path_obj.glob('*.csv'))
    if not csv_files:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶: {dir_path}")
    
    # 2. åˆ†ç±»æ–‡ä»¶
    result = {'train': [], 'val': [], 'test': []}
    
    # ä¼˜å…ˆçº§1: æŒ‰æ ‡å‡†å‘½åæ¨¡å¼
    for file in csv_files:
        fname = file.stem.lower()
        
        # åŒ¹é…è®­ç»ƒé›†æ–‡ä»¶
        if re.search(r'(train|training|train_data|training_data|train_set)', fname):
            result['train'].append(str(file))
        
        # åŒ¹é…éªŒè¯é›†æ–‡ä»¶
        elif re.search(r'(val|validation|valid|valid_data|val_data)', fname):
            result['val'].append(str(file))
        
        # åŒ¹é…æµ‹è¯•é›†æ–‡ä»¶
        elif re.search(r'(test|testing|test_data|testing_data|test_set)', fname):
            result['test'].append(str(file))
    
    # ä¼˜å…ˆçº§2: æŒ‰æ•°æ®é›†ç›®å½•ç»“æ„
    if not any(result.values()):
        for file in csv_files:
            parent_dir = file.parent.stem.lower()
            
            if 'train' in parent_dir:
                result['train'].append(str(file))
            elif 'val' in parent_dir or 'valid' in parent_dir:
                result['val'].append(str(file))
            elif 'test' in parent_dir:
                result['test'].append(str(file))
    
    # ä¼˜å…ˆçº§3: æŒ‰ç®€å•å‘½å
    if not any(result.values()):
        for file in csv_files:
            fname = file.stem.lower()
            
            if fname == 'data' or fname == 'dataset' or fname == 'full':
                # å¦‚æœåªæœ‰ä¸€ä¸ªæ•°æ®æ–‡ä»¶ï¼Œåˆ™ç”¨äºæ‰€æœ‰é›†
                result['train'].append(str(file))
                result['val'].append(str(file))
                result['test'].append(str(file))
                break
    
    # 4. éªŒè¯ç»“æœ
    if not result['train']:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®é›†æ–‡ä»¶: {dir_path}")
    
    return result

def main(config_path: str):
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config_loader = ConfigLoader(config_path)
    config = config_loader.config
    config['training']['stability'] = config.get('stability', {
        'max_nan_allowed': 0,
        'output_clip_range': [-5.0, 5.0],
        'max_grad_norm': 1.0,
        'gradient_clip_value': 10000.0,
        'add_output_noise': True,
        'noise_std': 0.01,
        'backup_interval': 5,
        'recovery_strategy': 'backoff',
        'lr_backoff_factor': 0.5
    })
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = setup_logger(config['paths']['log_file'])
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œè®­ç»ƒè„šæœ¬")
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•
    create_output_directories(config)
    logger.info("ğŸ“ æ‰€æœ‰è¾“å‡ºç›®å½•å·²åˆ›å»º")
    
    # è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è·¯å¾„
    train_dir = config['env'].get('train_data_path')
    val_dir = config['env'].get('val_data_path', None)  
    test_dir = config['env'].get('test_data_path')
    
    if not train_dir or not os.path.exists(train_dir):
        raise RuntimeError(f"è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–æœªå®šä¹‰ï¼š{train_dir}")
    if not test_dir or not os.path.exists(test_dir):
        raise RuntimeError(f"æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–æœªå®šä¹‰ï¼š{test_dir}")
    
    # è·å–æ ‡ç­¾åˆ—å’Œæ—¥æœŸåˆ—å
    label_col = config['data'].get('label_col', 'label')
    date_col = config['data'].get('date_col', 'date')
    seq_length = config['training'].get('sequence_length', 30)
    
    logger.info(f"ğŸ“Š æ•°æ®é…ç½®: æ ‡ç­¾åˆ—='{label_col}', æ—¥æœŸåˆ—='{date_col}', åºåˆ—é•¿åº¦={seq_length}")
    
    # è·å–æ•°æ®é›†æ–‡ä»¶
    train_files = get_data_files_from_dir(train_dir)
    logger.info(f"è®­ç»ƒæ•°æ®é›†æ–‡ä»¶: train={train_files['train']}, val={train_files['val']}, test={train_files['test']}")
    
    # å¦‚æœæœ‰å•ç‹¬çš„éªŒè¯ç›®å½•
    val_files = get_data_files_from_dir(val_dir) if val_dir else {'train': [], 'val': [], 'test': []}
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_datasets = []
    for path in train_files['train']:
        logger.info(f"ğŸ” åŠ è½½è®­ç»ƒæ•°æ®é›†: {path}")
        ds = CSVDataset(
            data_path=path,
            label_col=label_col,
            date_col=date_col,
            sequence_length=seq_length
        )
        train_datasets.append(ds)
    
    # å¤„ç†éªŒè¯æ•°æ®é›†
    val_datasets = []
    for path in train_files['val'] + val_files['val']:  # å…ˆä½¿ç”¨è®­ç»ƒç›®å½•ä¸­çš„éªŒè¯é›†ï¼Œå†ä½¿ç”¨éªŒè¯ç›®å½•ä¸­çš„
        if path:  # ç¡®ä¿è·¯å¾„æœ‰æ•ˆ
            logger.info(f"ğŸ” åŠ è½½éªŒè¯æ•°æ®é›†: {path}")
            ds = CSVDataset(
                data_path=path,
                label_col=label_col,
                date_col=date_col,
                sequence_length=seq_length
            )
            val_datasets.append(ds)
    
    # å¦‚æœæ²¡æœ‰æ˜¾å¼çš„éªŒè¯é›†ï¼Œå°è¯•ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†
    if not val_datasets and train_datasets:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°æ˜¾å¼éªŒè¯é›†ï¼Œå°†ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†20%ä½œä¸ºéªŒè¯é›†")
        total_size = len(train_datasets[0])
        val_size = int(total_size * 0.2)
        train_size = total_size - val_size
        
        # åˆ†å‰²ç¬¬ä¸€ä¸ªè®­ç»ƒé›†ä½œä¸ºéªŒè¯é›†
        train_ds, val_ds = torch.utils.data.random_split(
            train_datasets[0], 
            [train_size, val_size]
        )
        
        # æ›¿æ¢åŸæ¥çš„è®­ç»ƒé›†åˆ—è¡¨
        train_datasets = [train_ds] + train_datasets[1:]
        val_datasets = [val_ds]
    # ç»„åˆæ•°æ®é›†
    if len(train_datasets) > 1:
        train_dataset = ConcatDataset(train_datasets)
        logger.info(f"âœ… åˆå¹¶äº† {len(train_datasets)} ä¸ªè®­ç»ƒæ•°æ®é›†")
    else:
        train_dataset = train_datasets[0] if train_datasets else None
    
    if len(val_datasets) > 1:
        val_dataset = ConcatDataset(val_datasets)
        logger.info(f"âœ… åˆå¹¶äº† {len(val_datasets)} ä¸ªéªŒè¯æ•°æ®é›†")
    else:
        val_dataset = val_datasets[0] if val_datasets else None
    
    if not train_dataset:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®!")
    
    # åˆ›å»ºæ¨¡å‹
    if hasattr(train_dataset, 'get_sample'):
        sample = train_dataset.get_sample(0)
        input_dim = sample[0].shape[1] if isinstance(sample, tuple) else None
    else:
        # å¦‚æœdatasetæ²¡æœ‰get_sampleæ–¹æ³•ï¼Œå°è¯•å…¶ä»–æ–¹æ³•è·å–è¾“å…¥ç»´åº¦
        try:
            first_item = next(iter(train_dataset))
            input_dim = first_item[0].shape[1]  # (batch, seq_len, features)
        except:
            raise RuntimeError("æ— æ³•ç¡®å®šè¾“å…¥ç»´åº¦")
    
    logger.info(f"ğŸ§  æ¨¡å‹è¾“å…¥ç»´åº¦: {input_dim}")
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    pretrained_path = config['paths'].get('pretrained_model')
    if pretrained_path and os.path.exists(pretrained_path):
        logger.info(f"â¬‡ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path}")
        model = TransformerModel.load_pretrained(config)
    else:
        logger.info("ğŸ†• åˆ›å»ºæ–°æ¨¡å‹")
        model = TransformerModel(
            input_dim=input_dim,
            output_dim=1,  # å•è¾“å‡º
            config=config
        )
    
    logger.info(f"ğŸ”„ æ¨¡å‹æ¶æ„:\n{model}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    logger.info(f"âš™ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config['training'].get('batch_size', 32)
    logger.info(f"ğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨ (batch_size={batch_size})")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=min(4, os.cpu_count())  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=min(2, os.cpu_count())  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
        )
        logger.info(f"âœ… è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    else:
        logger.warning("âš ï¸ æœªåˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨")
     # è®­ç»ƒæ¨¡å‹
    logger.info("â³ å¼€å§‹è®­ç»ƒ...")
    trained_model, stats = model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device=device
    )
    logger.info("ğŸ è®­ç»ƒå®Œæˆ!")
    
    # ä¿å­˜æ¨¡å‹
    model_save_path = Path(config['paths']['model_save_dir']) / "trained_model.pth"
    torch.save(trained_model.state_dict(), model_save_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
    
    # å‡†å¤‡ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(
        model.parameters(), 
        lr=float(config['training']['learning_rate']),
        eps=1e-6  # é˜²æ­¢é™¤é›¶
    )

    max_grad_norm = config['training'].get('max_grad_norm', 1.0) 
    logger.info("â„¹ï¸ ä½¿ç”¨SmoothL1Lossä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’")
    # åˆ›å»ºå®‰å…¨æ§åˆ¶å™¨
    safety = SafetyController(model, optimizer, config, logger)

# ä½¿ç”¨æ–°æŸå¤±å‡½æ•°
    loss_fn = RLoss(
    supervised_criterion=SafeSmoothL1Loss(beta=1.0).to(device),
    base_loss_weight=config['training']['base_loss_weight'],
).to(device)
    grad_monitor = GradientMonitor(model)
    grad_monitor.attach()
    torch.autograd.set_detect_anomaly(True)
    for epoch in range(config['training']['epochs']):
        new_weight = get_rl_weight(epoch)
        loss_fn.base_loss_weight = new_weight  # åŠ¨æ€è°ƒæ•´æƒé‡
        safety.create_backup(epoch)
        model.train()
        epoch_loss = 0.0
        nan_batch_count = 0
        valid_batch_count = 0
            # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶è¾“å‡º
        print(f"\nã€è°ƒè¯•ã€‘å¼€å§‹ç¬¬ {epoch+1} è½®è®­ç»ƒ")
        for batch_idx, (inputs, targets, dates) in enumerate(train_loader):
            print(f"\nã€è°ƒè¯•ã€‘ç¬¬ {batch_idx+1} æ‰¹æ¬¡")
            # print("batch_data å†…å®¹ï¼š", batch_data)123456
            print("åŸå§‹inputs.shape:", inputs.shape)
            print("åŸå§‹targets.shape:", targets.shape)
            # è½¬ç§»åˆ°è®¾å¤‡
            inputs = inputs.to(device)
            targets = targets.to(device)  
            print("inputsè®¾å¤‡:", inputs.device)
            print("targetsè®¾å¤‡:", targets.device)   
           
            # 1. è¾“å…¥æ•°æ®æ£€æŸ¥
            if not safety.check_inputs(inputs, targets):
                nan_batch_count += 1
                continue
                           
            # 2. åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œå‰å‘ä¼ æ’­
            loss = None
            with safety.safe_forward_context():
                outputs = model(inputs)
                print("outputså½¢çŠ¶")
                print(outputs)
                print("outputsçš„å½¢çŠ¶ï¼š", outputs.shape)
                print(torch.min(outputs).item(), torch.max(outputs).item())
                
                if outputs is None:
                    print("æ¨¡å‹è¾“å‡ºä¸ºNoneï¼Œè·³è¿‡ä¿æŠ¤å’Œlossè®¡ç®—")
                    # å¯ä»¥å®šä¹‰è·³å‡ºå½“å‰batchæˆ–ç»ˆæ­¢
                    continue
                if model2 is None:
                    raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œä¸èƒ½è¿›è¡Œé¢„æµ‹ï¼")
                model2_result = predict(model2, outputs)
                print("æ¨¡å‹äºŒæ¨ç†ç»“æœï¼š", model2_result)
                # è®¡ç®—å¥–åŠ±ï¼ˆç¤ºä¾‹ï¼‰
                direction_match = (torch.sign(model2_result) == torch.sign(targets)).float()
                magnitude_error = torch.abs(model2_result - targets)
                accuracy_reward = direction_match * 0.8
                
                # å¹…åº¦è¯¯å·®å¥–åŠ±ï¼ˆè¯¯å·®è¶Šå°å¥–åŠ±è¶Šé«˜ï¼‰
                error_reward = torch.exp(-2 * magnitude_error) * 0.2
                reward = (accuracy_reward + error_reward)
                print(f"å¹³å‡æ–¹å‘åŒ¹é…ç‡: {direction_match.mean().item():.4f}")
                print(f"å¹³å‡è¯¯å·®å¥–åŠ±: {error_reward.mean().item():.4f}")
                print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {reward.mean().item():.4f}")
                protected_logits = safety.protect_outputs(outputs)
                print(f"logitsä¿æŠ¤å: min={protected_logits.min().item():.4f}, max={protected_logits.max().item():.4f}")
                protected_outputs = {'logits': protected_logits}
            # 6. ä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆä¿æŒåŸæ¥å£ä¸å˜ï¼‰
                loss_dict = loss_fn(
                    model_outputs=protected_outputs,
                    targets=targets,
                    reward=reward,
                )
                
                loss = loss_dict["total_loss"]
                print(f"æ€»æŸå¤±: {loss.item():.4f} | "
                    f"ç›‘ç£æŸå¤±: {loss_dict['supervised_loss'].item():.4f} | "
                    f"ç­–ç•¥æŸå¤±: {loss_dict['policy_loss'].item():.4f} | "
                    f"åŒ¹é…ç‡: {loss_dict['match_rate'].item():.4f}")
                
            
            # åå‘ä¼ æ’­
            print("logits.requires_grad:", protected_logits.requires_grad)
            print("loss.requires_grad:", loss.requires_grad)
            optimizer.zero_grad()
            loss.backward()
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_max = param.grad.abs().max().item()
                    print(f"å±‚ {name} æ¢¯åº¦æœ€å¤§å€¼: {grad_max}")
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
            safety.check_gradients()
            optimizer.step()
            batch_reward = reward.mean().item()
            epoch_loss += loss.item()
            valid_batch_count += 1
            # æ¯10æ‰¹æ¬¡æŠ¥å‘Šæ¢¯åº¦çŠ¶æ€
            if batch_idx % 10 == 0:
                grad_monitor.report(batch_idx, epoch)
        # ç§»é™¤æ¢¯åº¦ç›‘æ§é’©å­
        grad_monitor.detach()
        
        # æŠ¥å‘Šå½“å‰epochçŠ¶æ€
        if valid_batch_count > 0:
            avg_loss = epoch_loss / valid_batch_count
            logger.info(f"ğŸ Epoch {epoch+1} | å¹³å‡æŸå¤±: {avg_loss:.6f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")
        else:
            logger.error(f"â›” Epoch {epoch+1} æ²¡æœ‰æœ‰æ•ˆè®­ç»ƒæ‰¹æ¬¡ï¼Œå°è¯•æ¢å¤...")
            recovered_epoch = safety.recover(epoch)
            logger.warning(f"æ¢å¤è‡³epoch {recovered_epoch}")
            epoch = recovered_epoch  # é‡ç½®epochè®¡æ•°å™¨
        
    
    # æµ‹è¯•å’Œè¯„ä¼°é˜¶æ®µ
    model.eval()
    all_predictions = []
    all_targets = []
    all_dates = []
    test_rewards = []
    test_files = get_data_files_from_dir(test_dir)
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªæµ‹è¯•æ–‡ä»¶ï¼‰
    test_datasets = []
    for test_file in test_files['test']:
        logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")
        test_dataset = CSVDataset(
            data_path=test_file,
            label_col=label_col,
            date_col=date_col,
            sequence_length=config['training'].get('sequence_length', 30)
        )
        test_datasets.append(test_dataset)
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨ConcatDataset
    if len(test_datasets) == 1:
        final_test_dataset = test_datasets[0]
    else:
        final_test_dataset = torch.utils.data.ConcatDataset(test_datasets)
    test_loader = DataLoader(
        final_test_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=False,
        num_workers=4
    )
    with torch.no_grad():
        for inputs, targets, dates in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs= model(inputs) 
            all_predictions.extend(outputs.squeeze().cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_dates.extend(dates)
            model2_preds = predict(model2, outputs)
            direction_match = (torch.sign(model2_preds) == torch.sign(targets)).float()
            accuracy_reward = direction_match * 0.8
            error_reward = torch.exp(-2 * torch.abs(model2_preds- targets)) * 0.2
            batch_reward = (accuracy_reward + error_reward).cpu().numpy()
            test_rewards.extend(batch_reward)
    # è½¬æ¢ä¸ºDataFrameä¾¿äºä¿å­˜ç»“æ„åŒ–æ•°æ®
    results_df = pd.DataFrame({
        'date': all_dates,  # ç›´æ¥ä»åŠ è½½å™¨ä¸­è·å–çš„æ—¥æœŸ
        'prediction': model2_preds,
        'target': all_targets,
        'error': np.abs(np.array(model2_preds) - np.array(all_targets))
    })
    
    # ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_output
    output_path = save_structured_data(results_df, config, f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    logger.info(f"ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {output_path}")
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))
    rmse = np.sqrt(np.mean((np.array(all_predictions) - np.array(all_targets))**2))
    sharpe_val = sharpe_ratio(np.array(all_predictions))
    drawdown = max_drawdown(np.array(all_predictions))
    if test_rewards:
        avg_test_reward = sum(test_rewards) / len(test_rewards)
        reward_std = np.std(test_rewards)  # éœ€è¦import numpy as np
    else:
        avg_test_reward = 0.0
        reward_std = 0.0
    eval_results = {
        "MAE": mae,
        "RMSE": rmse,
        "Sharpe_Ratio": sharpe_val,
        "Max_Drawdown": drawdown,
        "num_samples": len(all_targets),
        "input_features": train_dataset.feature_names,
        "config_path": config_path,
        "test_data_path": test_dir,
        "model_summary": str(model),
        "Avg_Reward": avg_test_reward,  # æ–°å­—æ®µ
        "Reward_STD": reward_std,       # æ–°å­—æ®µ
    }
    eval_results_serializable = convert_to_serializable(eval_results) #è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    output_path, _ = save_evaluation_results(eval_results, config, filename="evaluation_" + datetime.now().strftime('%Y%m%d_%H%M%S') + ".json")
    with open(output_path, 'w') as f:
        json.dump(eval_results_serializable, f, indent=4)
    # ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_eval
    save_evaluation_results(eval_results, config, f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    # é¢„æµ‹é˜¶æ®µï¼ˆå¦‚æœé…ç½®ï¼‰
    if 'predict' in config and config['predict'].get('steps', 0) > 0:
        logger.info(f"ğŸ”® å¼€å§‹é¢„æµ‹, æ­¥æ•°: {config['predict']['steps']}")

        # è®¡ç®—æœ€å¤§é•¿åº¦
        len_dates = len(test_dataset.dates)
        len_preds = len(all_predictions)
        max_len = max(len_dates, len_preds, config['predict']['steps'])

        # è¡¥é½dates
        dates_padded = np.pad(test_dataset.dates, (0, max_len - len_dates), constant_values=np.nan)
        # è¡¥é½é¢„æµ‹ç»“æœ
        preds_padded = np.pad(all_predictions, (0, max_len - len_preds), constant_values=np.nan)
        # ç”Ÿæˆ confidence æ•°ç»„ï¼Œé•¿åº¦ä¸º max_len
        confidence_array = np.random.uniform(0.7, 0.95, max_len)

        # åªå–å‰ max_len
        dates_for_df = dates_padded[:max_len]
        preds_for_df = preds_padded[:max_len]

        # ä¿å­˜é¢„æµ‹ç»“æœ
        predict_path = Path(config['predict']['output_path'])
        predict_path.parent.mkdir(parents=True, exist_ok=True)

        # æ„é€  DataFrame
        predict_df = pd.DataFrame({
            'date': dates_for_df,
            'prediction': preds_for_df,
            'confidence': confidence_array
        })

        predict_df.to_csv(predict_path, index=False)
        logger.info(f"âœ… ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {predict_path}")

    logger.info("ğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="RLäº¤æ˜“ç³»ç»Ÿè®­ç»ƒè„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument("--config", 
                        type=str, 
                        default="configs/model1.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: configs/model1.yaml")
    
    args = parser.parse_args()
    main(args.config)
