import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from pathlib import Path
import numpy as np
import pandas as pd
import yaml
import logging
import json
import argparse
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from torch.utils.data import DataLoader, Dataset, random_split
import random
import copy
import torch.nn.functional as F 
from contextlib import contextmanager
import inspect
import warnings
from ..model2.inference import load_model, predict, DeepSeekModel


config_path = '/home/liyakun/twitter-stock-prediction/configs/model1.yaml'
def load_model(config_path):
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if not config:
            print("é…ç½®ä¸ºç©ºæˆ–æ ¼å¼æœ‰è¯¯")
            return None
        return config
    except Exception as e:
        print("è¯»å–é…ç½®å¤±è´¥ï¼š", e)
        return None
config = load_model(config_path)


try:
    model_path = config['env']['prediction_model_path']
    print("æ¨¡å‹è·¯å¾„ï¼š", model_path)
except KeyError:
    print("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° prediction_model_path å­—æ®µã€‚")
    # ä½ å¯ä»¥è®¾ç½®é»˜è®¤è·¯å¾„æˆ–æŠ›å‡ºå¼‚å¸¸
if not config:
    raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥ï¼")

# æå–å‚æ•°å’Œæ¨¡å‹è·¯å¾„
try:
    hidden_size = config['env']['hidden_size']
    num_layers = config['env']['num_layers']
    num_heads = config['env']['num_heads']
    model_path = config['env']['prediction_model_path']
    input_dim=config['model']['params']['input_dim']
except KeyError as e:
    raise RuntimeError(f"é…ç½®ç¼ºå°‘å­—æ®µ: {e}")
# æ ¹æ®é…ç½®è·¯å¾„åŠ è½½æ¨¡å‹
if model_path:
    model2 = DeepSeekModel(input_dim,hidden_size, num_layers, num_heads)
    
    print("åŠ è½½çš„model2ç±»å‹ï¼š", type(model2))
    assert model2 is not None, "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel2ä¸ºNoneï¼"
else:
    print("æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚")

class ConfigLoader:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config



def setup_logger(log_file: str) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_dir = Path(log_file).parent
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logger = logging.getLogger("training")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ä¹‹å‰çš„handlersé¿å…é‡å¤
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_file)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    return logger

def create_output_directories(config: Dict[str, Any]) -> None:
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•"""
    paths = config['paths']
    
    # æ¨¡å‹è¾“å‡ºç›®å½• (model1_output/)
    output_dir = Path(paths.get('processed_output_dir', 'data/processed/model1_output'))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¯„ä¼°ç»“æœç›®å½• (model1_eval/)
    eval_dir = Path(paths.get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_dir = Path(config['paths']['output_model']).parent
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ—¥å¿—æ–‡ä»¶ç›®å½•
    log_dir = Path(config['paths']['log_file']).parent
    log_dir.mkdir(parents=True, exist_ok=True)

def convert_to_serializable(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(obj)
    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.detach().cpu().tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]
    else:
        return obj



def save_structured_data(data: pd.DataFrame, config: Dict[str, Any], filename: str = "processed_data.csv") -> Path:
    """
    ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_outputç›®å½•
    """
    output_dir = Path(config['paths'].get('processed_output_dir', 'data/processed/model1_output'))
    output_path = output_dir / filename
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ·»åŠ æ—¥æœŸåˆ—ä½œä¸ºç»“æ„åŒ–æ•°æ®çš„ä¸€éƒ¨åˆ†
    if 'date' not in data.columns:
        if 'timestamp' in data.columns:
            data['date'] = pd.to_datetime(data['timestamp']).dt.date
        else:
            data['date'] = pd.Timestamp.now().strftime("%Y-%m-%d")
    
    data.to_csv(output_path, index=False)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°: {output_path}")
    return output_path

def save_evaluation_results(results: Dict[str, Any], config: Dict[str, Any], filename: str = "evaluation_results.json") -> Tuple[Path, Path]:
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_evalç›®å½•
    """
    eval_dir = Path(config['paths'].get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = eval_dir / filename
    results_serializable = convert_to_serializable(results)
    
    with open(output_path, 'w') as f:
        json.dump(results_serializable, f, indent=4)
    
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
    
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ä¾¿äºåˆ†æ
    csv_path = output_path.with_suffix('.csv')
    eval_df = pd.DataFrame([results])
    eval_df.to_csv(csv_path, index=False)
    logger.info(f"âœ… åŒæ—¶ä¿å­˜CSVæ ¼å¼è¯„ä¼°ç»“æœ: {csv_path}")
    
    return output_path, csv_path

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    """è®¡ç®—å¤æ™®æ¯”ç‡"""
    excess_returns = returns - risk_free_rate
    return excess_returns.mean() / (excess_returns.std() + 1e-8)

def max_drawdown(values: np.ndarray) -> float:
    """è®¡ç®—æœ€å¤§å›æ’¤"""
    peak = np.maximum.accumulate(values)
    drawdown = (values - peak) / peak
    return np.min(drawdown)
class SafetyController:
    def __init__(self, model, optimizer, config, logger):
        self.model = model
        self.optimizer = optimizer
        self.config = config['training']['stability']
        self.logger = logger
        self.epoch_backup_count = 0
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        self.backup_dir = Path(f"{config['paths']['log_file']}_safety_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # å­¦ä¹ ç‡å¤‡ä»½
        self.original_lr = optimizer.param_groups[0]['lr']
    
    def check_inputs(self, inputs, targets):
        """è¾“å…¥æ•°æ®å¥åº·æ£€æŸ¥"""
        nan_count = torch.isnan(inputs).sum().item() + torch.isnan(targets).sum().item()
        inf_count = torch.isinf(inputs).sum().item() + torch.isinf(targets).sum().item()
        
        if nan_count > self.config.get('max_nan_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«NaNæ•°æ®çš„æ‰¹æ¬¡ (NaNæ•°é‡: {nan_count})")
            return False
        
        if inf_count > self.config.get('max_inf_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«Infæ•°æ®çš„æ‰¹æ¬¡ (Infæ•°é‡: {inf_count})")
            return False
        
        return True
    
    def protect_outputs(self, outputs):
        """æ¨¡å‹è¾“å‡ºé˜²æŠ¤"""
        if torch.isnan(outputs).any():
            self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œæ‰§è¡Œä¿®å¤...")
            outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1e4, neginf=-1e4)
        
            
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        outputs = torch.clamp(outputs, 
                              min=self.config['output_clip_range'][0], 
                              max=self.config['output_clip_range'][1])
        
        # æ·»åŠ å™ªå£°å¢å¼ºç¨³å®šæ€§
        # if self.config.get('add_output_noise'):
        #     noise_std = self.config.get('noise_std', 1e-3)
        #     outputs += torch.randn_like(outputs) * noise_std
        
        return outputs
    
    def check_gradients(self):
        """æ¢¯åº¦å¥åº·æ£€æŸ¥å’Œä¿®å¤"""
        max_grad_norm = self.config.get('max_grad_norm', 1.0)
        problematic_layers = []
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad
                
                # ä¿®å¤NaN/Infæ¢¯åº¦
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    problematic_layers.append(name)
                    param.grad = torch.where(
                        torch.isnan(grad) | torch.isinf(grad),
                        torch.zeros_like(grad),
                        grad
                    )
                
                # æ¢¯åº¦è£å‰ª
                torch.clamp_(param.grad, -self.config['gradient_clip_value'], 
                            self.config['gradient_clip_value'])
        
        # å…¨å±€æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_grad_norm)
        
        if problematic_layers:
            self.logger.warning(f"ä¿®å¤æ¢¯åº¦å¼‚å¸¸å±‚: {problematic_layers[:3]}{'...' if len(problematic_layers)>3 else ''}")
        
        return len(problematic_layers) == 0
    
    def create_backup(self, epoch):
        """åˆ›å»ºå®‰å…¨æ¢å¤ç‚¹"""
        if epoch % self.config.get('backup_interval', 5) == 0:
            backup_path = self.backup_dir / f"epoch_{epoch}_safety_backup.pt"
            torch.save({
                'model_state': self.model.state_dict(),
                'optimizer_state': self.optimizer.state_dict(),
                'epoch': epoch
            }, backup_path)
            
            # ä¿å­˜æœ€è¿‘çš„5ä¸ªå¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if len(backup_files) > 5:
                os.remove(backup_files[0])
    
    def recover(self, current_epoch):
        """ä»é”™è¯¯ä¸­æ¢å¤"""
        recovery_type = self.config.get('recovery_strategy', 'backoff')
        
        if recovery_type == 'backoff':
            # å­¦ä¹ ç‡é€€é¿
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= self.config.get('lr_backoff_factor', 0.5)
            
            self.logger.warning(f"é‡‡ç”¨å­¦ä¹ ç‡é€€é¿ç­–ç•¥ï¼Œæ–°å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # å°è¯•æ¢å¤æœ€è¿‘çš„å¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if backup_files:
                latest_backup = backup_files[-1]
                checkpoint = torch.load(latest_backup)
                self.model.load_state_dict(checkpoint['model_state'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                self.logger.warning(f"ä»å¤‡ä»½æ¢å¤: {latest_backup.name} (epoch {checkpoint['epoch']})")
                return checkpoint['epoch']  # è¿”å›æ¢å¤åˆ°çš„epoch
            
        elif recovery_type == 'reset':
            # å®Œå…¨é‡ç½®æ¨¡å‹
            self.logger.error("æ‰§è¡Œæ¨¡å‹å®Œå…¨é‡ç½®!")
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            for module in self.model.modules():
                if hasattr(module, 'reset_parameters'):
                    module.reset_parameters()
            
            # é‡ç½®ä¼˜åŒ–å™¨
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.original_lr
            
            return 0  # ä»å¤´å¼€å§‹è®­ç»ƒ
        
        return current_epoch
    
    @contextmanager
    def safe_forward_context(self):
        """å®‰å…¨å‰å‘ä¼ æ’­ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # å¯ç”¨PyTorchå¼‚å¸¸æ£€æµ‹
            torch.autograd.set_detect_anomaly(True)
            yield
        except RuntimeError as e:
            self.logger.error(f"å‰å‘ä¼ æ’­å¼‚å¸¸: {str(e)}")
            # æå–å¼‚å¸¸ä½ç½®
            stack = inspect.stack()
            caller = stack[1]  # è°ƒç”¨safe_forward_contextçš„å‡½æ•°
            self.logger.error(f"å¼‚å¸¸ä½ç½®: {caller.filename}:{caller.lineno}")
            raise
        finally:
            torch.autograd.set_detect_anomaly(False)

# === æ–°å¢: æ¢¯åº¦ç›‘æ§å™¨ ===
class GradientMonitor:
    def __init__(self, model, layers_to_watch=None):
        self.model = model
        self.layers = layers_to_watch or self._identify_critical_layers()
        self.handles = []
        self.logger = logging.getLogger("gradient_monitor")
        self.reset_stats()
    
    def _identify_critical_layers(self):
        """è¯†åˆ«å…³é”®å±‚ï¼ˆæœ€åä¸€å±‚å’Œå«LayerNormçš„å±‚ï¼‰"""
        critical_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
                critical_layers.append(name)
            if 'out' in name or 'final' in name or 'proj' in name:
                critical_layers.append(name)
        return list(set(critical_layers))
    
    def reset_stats(self):
        self.max_grad = 0.0
        self.max_grad_layer = None
        self.nan_count = 0
    
    def _hook_fn(self, module, grad_input, grad_output, layer_name):
        # æ£€æŸ¥NaNå’ŒInf
        for g in grad_output:
            if g is not None:
                if torch.isnan(g).any():
                    self.nan_count += torch.isnan(g).sum().item()
                if torch.isinf(g).any():
                    self.nan_count += torch.isinf(g).sum().item()
        
        # è®°å½•æœ€å¤§æ¢¯åº¦
        for g in grad_output:
            if g is not None:
                grad_norm = torch.norm(g)
                if grad_norm > self.max_grad:
                    self.max_grad = grad_norm.item()
                    self.max_grad_layer = layer_name
    
    def attach(self):
        """é™„åŠ æ¢¯åº¦é’©å­"""
        self.reset_stats()
        for name in self.layers:
            module = self._get_module(name)
            hook = lambda m, gi, go, n=name: self._hook_fn(m, gi, go, n)
            handle = module.register_full_backward_hook(hook)
            self.handles.append(handle)
    
    def detach(self):
        """ç§»é™¤æ¢¯åº¦é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles = []
    
    def _get_module(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å—"""
        names = name.split('.')
        module = self.model
        for n in names:
            module = getattr(module, n)
        return module
    
    def report(self, batch_idx, epoch):
        """ç”Ÿæˆæ¢¯åº¦æŠ¥å‘Š"""
        report = f"æ¢¯åº¦ç›‘æ§ - Epoch {epoch} Batch {batch_idx}:\n"
        report += f"  ğŸš© æœ€å¤§æ¢¯åº¦: {self.max_grad:.2e} ({self.max_grad_layer})\n"
        report += f"  âš ï¸ å¼‚å¸¸æ¢¯åº¦è®¡æ•°: {self.nan_count}"
        
        if self.nan_count > 0:
            self.logger.warning(report)
        elif batch_idx % 10 == 0:
            self.logger.info(report)


# === æ–°å¢: SmoothL1æŸå¤±å‡½æ•° ===

class RLoss(nn.Module):
    def __init__(self, supervised_criterion, base_loss_weight=0.5):
        super().__init__()
        self.base_loss_weight = base_loss_weight
        self.supervised_criterion = supervised_criterion  # SafeSmoothL1Loss
    
    def forward(self, policy_outputs, model2_outputs, targets, reward):
        # ç›‘ç£æŸå¤±éƒ¨åˆ† - ä¿æŒåŸºç¡€é¢„æµ‹èƒ½åŠ›
        supervised_loss = self.supervised_criterion(model2_outputs.squeeze(), targets)
         # 2. ä¸“ä¸šçš„ç­–ç•¥æ¢¯åº¦æŸå¤± - å°†å¥–åŠ±ä¸åŠ¨ä½œæ¦‚ç‡å…³è”
        # å°†ç­–ç•¥è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        action_probs = F.softmax(policy_outputs, dim=1)
        # é‡‘èç­–ç•¥æ¢¯åº¦æŸå¤± (æ–¹å‘æ€§åŠ æƒ)
        position_direction = torch.argmax(action_probs, dim=1)  # 0: åšå¤š, 1: ç©ºä»“, 2: åšç©º
        market_direction = (targets > 0).long()  # å¸‚åœºæ–¹å‘ 1: ä¸Šæ¶¨, 0: ä¸‹è·Œ
        valid_actions = market_direction.clone()
        valid_actions[market_direction == 0] = 2  # ä¸‹è·Œæ—¶å¸Œæœ›æ¨¡å‹é€‰åšç©º
        valid_actions[market_direction == 1] = 0  # ä¸Šæ¶¨æ—¶å¸Œæœ›æ¨¡å‹é€‰åšå¤š
        
        # æ–¹å‘åŒ¹é…åº¦ (ç”¨äºç­–ç•¥æ¢¯åº¦åŠ æƒ)
        directional_match = (position_direction == valid_actions).float()
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡ (ç”¨äºç­–ç•¥æ¢¯åº¦)
        chosen_action_probs = torch.gather(action_probs, 1, position_direction.unsqueeze(1)).squeeze()
        log_probs = torch.log(chosen_action_probs + 1e-8)
        
        # é‡‘èç‰¹åŒ–çš„ç­–ç•¥æ¢¯åº¦æŸå¤±
        rl_loss = -torch.mean(log_probs * reward * (1.0 + 0.5 * directional_match))
        
        # 3. åŠ¨æ€è°ƒæ•´æƒé‡ (åŸºäºå¸‚åœºæ³¢åŠ¨ç‡)
        volatility = torch.abs(targets).mean()
        current_weight = self.dynamic_weight_adjust(volatility)
        
        # 4. æ··åˆæŸå¤±
        total_loss = current_weight * supervised_loss + (1 - current_weight) * rl_loss
        
        # è¿”å›æŸå¤±å’Œç›¸å…³æŒ‡æ ‡
        return {
            "total_loss": total_loss,
            "supervised_loss": supervised_loss,
            "rl_loss": rl_loss,
            "weight": current_weight,
            "mean_reward": reward.mean(),
            "match_rate": directional_match.mean()
        }
    
    def dynamic_weight_adjust(self, volatility):
        """æ ¹æ®å¸‚åœºæ³¢åŠ¨ç‡è°ƒæ•´ç›‘ç£æŸå¤±æƒé‡"""
        # æ³¢åŠ¨ç‡è¾ƒé«˜æ—¶é™ä½ç›‘ç£æƒé‡ (0.2-0.8èŒƒå›´)
        return torch.clamp(0.7 - volatility * 20, min=0.2, max=0.8).item()

def get_rl_weight(epoch):
    """æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ç›‘ç£æŸå¤±æƒé‡"""
    # ç¬¬ä¸€é˜¶æ®µï¼šä¾§é‡ç›‘ç£å­¦ä¹ ï¼ˆepoch 0-9ï¼‰
    if epoch < 10:
        return 0.7
    # ç¬¬äºŒé˜¶æ®µï¼šå¹³è¡¡å­¦ä¹ ï¼ˆepoch 10-19ï¼‰
    elif epoch < 20:
        # çº¿æ€§è¿‡æ¸¡ï¼š0.7 â†’ 0.5
        return 0.7 - 0.2 * (epoch - 9) / 10
    # ç¬¬ä¸‰é˜¶æ®µï¼šä¾§é‡å¼ºåŒ–å­¦ä¹ ï¼ˆepoch â‰¥20ï¼‰
    else:
        return 0.3
    
class SafeSmoothL1Loss(nn.Module):
    """æ•°å€¼ç¨³å®šçš„SmoothL1æŸå¤±"""
    def __init__(self, beta=1.0):
        super().__init__()
        self.beta = beta
     
    def forward(self, input, target):
        if input.shape != target.shape:
            raise ValueError(f"è¾“å…¥å’Œç›®æ ‡çš„å½¢çŠ¶ä¸ä¸€è‡´ï¼š{input.shape} vs {target.shape}")
        safe_input = torch.clamp(input, -50, 50)
        safe_target = torch.clamp(target, -50, 50)
        diff = safe_input - safe_target
        diff = torch.where(torch.isinf(diff), torch.zeros_like(diff), diff)
        diff = torch.where(torch.isnan(diff), torch.zeros_like(diff), diff)
        
        abs_diff = torch.abs(diff)
        mask = abs_diff < self.beta
        
        # L2éƒ¨åˆ†: 0.5 * x^2 / beta
        l2_loss = 0.5 * torch.pow(diff, 2) / self.beta
        # L1éƒ¨åˆ†: |x| - 0.5 * beta
        l1_loss = abs_diff - 0.5 * self.beta
        
        loss = torch.where(mask, l2_loss, l1_loss)
        loss = loss.mean()
        if torch.isnan(loss) or torch.isinf(loss):
            warnings.warn("æ£€æµ‹åˆ°NaN/InfæŸå¤±å€¼ï¼Œæ‰§è¡Œä¿®å¤!")
            return torch.zeros_like(loss)
            
        return loss

class NPYDataset(Dataset):
    def __init__(self, x_file: str, y_file: str,dates_file: str,filter_nan=True,feature_names=None):
        print("Loading data files:")
        print("x_file:", x_file)
        print("y_file:", y_file)
        print("dates_file:", dates_file)
        self.x_data = np.load(x_file)
        self.y_data = np.load(y_file)
        self.dates = np.load(dates_file)  # [N]
        try:
            self.x_data = np.load(x_file)
            self.y_data = np.load(y_file)
            self.dates = np.load(dates_file) if dates_file else None
        except Exception as e:
            print("åŠ è½½npzæ–‡ä»¶æ—¶å‡ºé”™:", e)
            raise
        print("x_data.shape:", self.x_data.shape)
        print("y_data.shape:", self.y_data.shape)
        if self.dates is not None:
            print("dates.shape:", self.dates.shape)
        assert len(self.x_data) == len(self.y_data), "xå’Œyè¡Œæ•°ä¸ä¸€è‡´"
        if self.dates is not None:
            assert len(self.x_data) == len(self.dates), "xå’Œdatesè¡Œæ•°ä¸ä¸€è‡´"
        assert len(self.x_data) == len(self.y_data) == len(self.dates), "x, y, datesè¡Œæ•°ä¸ä¸€è‡´"
        if filter_nan:
            self._filter_nan_samples()
        self.feature_names = feature_names
    
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ï¼Œåˆ™å¯ä»¥å°è¯•è‡ªåŠ¨å®šä¹‰ï¼ˆæ¯”å¦‚ï¼šå‡è®¾æ˜¯æ‰€æœ‰ç‰¹å¾åï¼‰
        if self.feature_names is None:
            # è¿™é‡Œå¯ä»¥è‡ªå®šä¹‰é»˜è®¤ç‰¹å¾å
            self.feature_names = [f'Feature_{i}' for i in range(self.x_data.shape[1])]
    
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, idx):
        data = self.x_data[idx]
        label = self.y_data[idx]
        date = self.dates[idx]
        # é¢å¤–æ£€æŸ¥
        data = torch.tensor(data, dtype=torch.float32) 
        label = torch.tensor(label, dtype=torch.float32)
        if torch.isnan(data).any():
            # å¤„ç†NaNï¼Œæ¯”å¦‚ç”¨0æ›¿ä»£
            data = torch.nan_to_num(data, nan=0.0)
        if torch.isinf(data).any():
            data = torch.nan_to_num(data, posinf=1e6, neginf=-1e6)
            print("inputs:",data.shape)
        return data, label, date
    
    def _filter_nan_samples(self):
        """è¿‡æ»¤åŒ…å«NaNçš„æ ·æœ¬"""
        original_count = len(self.x_data)
        
        # æ‰¾åˆ°æœ‰æ•ˆç´¢å¼•
        valid_indices = [
            i for i in range(original_count)
            if not np.isnan(self.x_data[i]).any() and not np.isnan(self.y_data[i]).any()
        ]
        
        # åº”ç”¨è¿‡æ»¤
        self.x_data = self.x_data[valid_indices]
        self.y_data = self.y_data[valid_indices]
        self.dates = self.dates[valid_indices] if self.dates is not None else None
        
        filtered_count = original_count - len(valid_indices)
        print(f"è¿‡æ»¤æ‰åŒ…å«NaNçš„æ ·æœ¬: {filtered_count}/{original_count}")

class TransformerModel(nn.Module):
    """Transformeræ¨¡å‹å®šä¹‰ï¼ŒåŒ¹é…é¢„è®­ç»ƒæƒé‡ç»“æ„"""
    def __init__(self, 
                 vocab_size: int,
                 input_dim: int, 
                 hidden_size: int, 
                 num_layers: int,
                 output_size: int,
                 num_attention_heads: int = 8,
                 intermediate_size: int = 11008):
        super().__init__()
        
          # ç›´æ¥ç”¨Linearå±‚å¤„ç†è¾“å…¥ç‰¹å¾
        self.input_fc = nn.Linear(input_dim, hidden_size)
        
        # Transformerå±‚å®šä¹‰ - åŒ¹é…"layers.x..."æƒé‡
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=hidden_size,
                nhead=num_attention_heads,
                dim_feedforward=intermediate_size
            )
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.out_proj = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # è¾“å…¥ä¿æŠ¤
        x = torch.nan_to_num(x, nan=0.0, posinf=1e4, neginf=-1e4)
        
        # åŸå§‹å‰å‘ä¼ æ’­
        x = self.input_fc(x)
        for layer in self.layers:
            x = layer(x)
        
        outputs = self.out_proj(x)
        
        # è¾“å‡ºä¿æŠ¤
        outputs = torch.clamp(outputs, min=-10.0, max=10.0)
        outputs = torch.where(torch.isnan(outputs), torch.zeros_like(outputs), outputs)
        
        return outputs

    
    @classmethod
    def load_pretrained(cls, config: Dict[str, Any]) -> 'TransformerModel':
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        logger = logging.getLogger("training")
        prediction_model_path = config['paths']['output_model']
        model_params = config['model']['params']
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(
            input_dim=model_params['input_dim'],
            vocab_size=model_params['vocab_size'],
            hidden_size=model_params['hidden_size'],
            num_layers=model_params['num_layers'],
            output_size=model_params['action_space_size'],
            num_attention_heads=model_params.get('num_attention_heads', 8),
            intermediate_size=model_params.get('intermediate_size', 11008)
        )
        
        # å¦‚æœé¢„è®­ç»ƒæ¨¡å‹å­˜åœ¨ï¼ŒåŠ è½½æƒé‡
        if os.path.exists(prediction_model_path):
            logger.info(f"âš™ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {prediction_model_path}")
            pretrained_dict = torch.load(prediction_model_path)
            model.load_state_dict(pretrained_dict, strict=False)
            logger.info(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡")
        else:
            logger.warning(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æœªæ‰¾åˆ°: {prediction_model_path}")
        
        return model

def compute_reward(model2_result, targets,lookback=5):
    """æ¨¡æ‹Ÿå®é™…äº¤æ˜“æ•ˆæœçš„å¥–åŠ±"""
    # è·å–æ¨¡å‹é¢„æµ‹åºåˆ—
    predictions = model2_result[:, :lookback]
    
    # æ„å»ºæ¨¡æ‹ŸæŒä»“
    portfolio = torch.zeros_like(targets)
    total_return = torch.zeros_like(targets)
    
    # åŸºäºè¿ç»­é¢„æµ‹çš„ä»“ä½å˜åŒ–
    for t in range(1, lookback):
        # åŸºäºé¢„æµ‹ä¿¡å·è°ƒæ•´ä»“ä½
        position_change = predictions[:, t] - predictions[:, t-1]
        portfolio += position_change
        
        # è®¡ç®—æ¯æ—¥æ”¶ç›Š
        daily_return = portfolio * targets[:, t]
        total_return += daily_return
        
        # é£é™©ç®¡ç†: å¼ºåˆ¶å¹³ä»“è§„åˆ™
        portfolio = torch.where(torch.abs(portfolio) > 2, 
                                torch.clamp(portfolio, -1, 1), 
                                portfolio)
    
    # æœ€ç»ˆç»„åˆä»·å€¼ä½œä¸ºå¥–åŠ±
    final_value = 1 + total_return
    reward = final_value - 1  # ç»„åˆæ”¶ç›Šä½œä¸ºå¥–åŠ±
    return reward

def get_actual_price(stock_code, date):
    # æ–‡ä»¶è·¯å¾„ç¤ºä¾‹
    csv_file = os.path.join('data', 'stock_prices', f'{stock_code}.csv')
    df = pd.read_csv(csv_file)

    # å‡è®¾csvæœ‰åˆ—ï¼š'date', 'close'
    row = df[df['date'] == date]
    if row.empty:
        print(f"æœªæ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„è‚¡ä»·ï¼š{date}")
        return None
    return float(row['close'].values[0])

def get_data_files_from_dir(dir_path: str) -> Dict[str, str]:
    """å®‰å…¨åœ°è¯†åˆ«X/Y/datesæ–‡ä»¶ï¼Œæ·»åŠ dateså¤„ç†"""
    path_obj = Path(dir_path)
    files = {'x': None, 'y': None, 'dates': None}
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰.npyæ–‡ä»¶
    npy_files = list(path_obj.glob('*.npy'))
    if not npy_files:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°.npyæ–‡ä»¶: {dir_path}")
    
    # 2. å°è¯•ä¼˜å…ˆåŒ¹é…æ ‡å‡†å‘½åæ–‡ä»¶
    for f in npy_files:
        fname = f.name
        if fname in ['X.npy', 'x.npy']:
            files['x'] = str(f)
        elif fname in ['Y.npy', 'y.npy']:
            files['y'] = str(f)
        elif fname in ['dates.npy', 'date.npy', 'time.npy']:
            files['dates'] = str(f)
    
    # 3. å¦‚æœæœªæ‰¾åˆ°æ ‡å‡†æ–‡ä»¶ï¼Œå°è¯•æŒ‰æ¨¡å¼åŒ¹é…
    if files['x'] is None:
        for f in npy_files:
            fname_lower = f.name.lower()
            if 'x' in fname_lower or 'feature' in fname_lower or 'input' in fname_lower:
                files['x'] = str(f)
                break
    
    if files['y'] is None:
        for f in npy_files:
            fname_lower = f.name.lower()
            if 'y' in fname_lower or 'target' in fname_lower or 'label' in fname_lower:
                files['y'] = str(f)
                break
    
    if files['dates'] is None:
        for f in npy_files:
            fname_lower = f.name.lower()
            if 'date' in fname_lower or 'time' in fname_lower or 'timestamp' in fname_lower:
                files['dates'] = str(f)
                break
    
    # 4. å¿…éœ€çš„éªŒè¯
    if files['x'] is None:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶(X): {dir_path}")
    if files['y'] is None:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶(Y): {dir_path}")
    
    return files

def main(config_path: str):
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config_loader = ConfigLoader(config_path)
    config = config_loader.config
    config['training']['stability'] = config.get('stability', {
        'max_nan_allowed': 0,
        'output_clip_range': [-5.0, 5.0],
        'max_grad_norm': 1.0,
        'gradient_clip_value': 10000.0,
        'add_output_noise': True,
        'noise_std': 0.01,
        'backup_interval': 5,
        'recovery_strategy': 'backoff',
        'lr_backoff_factor': 0.5
    })
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = setup_logger(config['paths']['log_file'])
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œè®­ç»ƒè„šæœ¬")
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•
    create_output_directories(config)
    logger.info("ğŸ“ æ‰€æœ‰è¾“å‡ºç›®å½•å·²åˆ›å»º")
    # è·å–ç›®å½•ä¸‹çš„xå’Œyæ–‡ä»¶è·¯å¾„
    train_dir = config['env'].get('train_data_path')
    test_data_path = config['env'].get('test_data_path') 

    if not train_dir or not os.path.exists(train_dir):
        raise RuntimeError(f"è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–æœªå®šä¹‰ï¼š{train_dir}")
    data_files = get_data_files_from_dir(train_dir)
    print("æ‰¾åˆ°çš„æ–‡ä»¶ï¼š", data_files)
    if 'x' not in data_files or 'y' not in data_files:
        raise RuntimeError(f"æœªåœ¨ç›®å½•ä¸­æ‰¾åˆ°xæˆ–y.npyæ–‡ä»¶ï¼Œæ–‡ä»¶åˆ—è¡¨ï¼š{data_files}")
    

    # è¯»å–xå’Œy
    x_path = data_files.get('x')
    print("x_path:", x_path)
    
    y_path = data_files.get('y')
    print("y_path:", y_path)
    dates_path = data_files.get('dates') 
    print("dates_path:",dates_path)



    if not x_path:
        raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°x.npyçš„è·¯å¾„")
    if not y_path:
        raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°y.npyçš„è·¯å¾„")
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    train_dataset =NPYDataset(x_path,y_path ,dates_path)
    input_dim = train_dataset.x_data.shape[1]
    model = TransformerModel.load_pretrained(config)
    logger.info(f"ğŸ”„ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ, ç»“æ„: {model}")
    
    # æ£€æŸ¥æ¨¡å‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    logger.info(f"âš™ï¸ æ¨¡å‹è¿è¡Œåœ¨: {device}")
    
    # åŠ è½½æ•°æ®é›†
    train_dir = config['env'].get('train_data_path')

    # è·å–ç›®å½•ä¸‹çš„xå’Œyæ–‡ä»¶è·¯å¾„
    data_files = get_data_files_from_dir(train_dir)

    # è¯»å–xå’Œy
    x_path = data_files.get('x')
    y_path = data_files.get('y')
    dates_path = data_files.get('dates')  

    if not x_path or not y_path:
        raise RuntimeError("æœªæ‰¾åˆ°xæˆ–yæ–‡ä»¶ï¼Œè¯·ç¡®è®¤ç›®å½•ä¸­å­˜åœ¨å¯¹åº”çš„csvæ–‡ä»¶ã€‚")

    # ä½¿ç”¨xå’Œyä½œä¸ºæ•°æ®è·¯å¾„
    train_dataset =NPYDataset(x_path,y_path,dates_path)
    sequence_length = config['model']['params'].get('sequence_length', 30)
    
    # è®­ç»ƒæ•°æ®é›†
    logger.info(f"ğŸ“¦ åŠ è½½è®­ç»ƒæ•°æ®: {train_dir}")
    train_dataset = NPYDataset(x_path,y_path,dates_path)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=True,
        num_workers=4
    )
    
    # æµ‹è¯•æ•°æ®é›†
    test_dir = config['env'].get('test_data_path')
    test_data_files = get_data_files_from_dir(test_dir)

    x_test_path = test_data_files.get('x')
    y_test_path = test_data_files.get('y')
    dates_test_path = test_data_files.get('dates')

    if not x_test_path or not y_test_path:
        raise RuntimeError("æœªæ‰¾åˆ°æµ‹è¯•xæˆ–yæ–‡ä»¶ï¼Œè¯·ç¡®è®¤ç›®å½•ä¸­å­˜åœ¨å¯¹åº”çš„.npyæ–‡ä»¶ã€‚")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = NPYDataset(x_test_path,y_test_path ,dates_test_path)

    test_loader = DataLoader ( 
        test_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=False,
        num_workers=4
    )
    
    # å‡†å¤‡ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(
        model.parameters(), 
        lr=float(config['training']['learning_rate']),
        eps=1e-6  # é˜²æ­¢é™¤é›¶
    )

    max_grad_norm = config['training'].get('max_grad_norm', 1.0) 
    logger.info("â„¹ï¸ ä½¿ç”¨SmoothL1Lossä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’")
    # åˆ›å»ºå®‰å…¨æ§åˆ¶å™¨
    safety = SafetyController(model, optimizer, config, logger)
    
    # åˆ›å»ºæ¢¯åº¦ç›‘æ§å™¨
    grad_monitor = GradientMonitor(model)
    grad_monitor.attach()
    torch.autograd.set_detect_anomaly(True)
    supervised_criterion = SafeSmoothL1Loss(beta=1.0).to(device)
    rl_criterion = RLoss(supervised_criterion, base_loss_weight=0.5).to(device)  
    for epoch in range(config['training']['epochs']):
        new_weight = get_rl_weight(epoch)
        rl_criterion.update_weight(new_weight)
        safety.create_backup(epoch)
        model.train()
        epoch_loss = 0.0
        nan_batch_count = 0
        valid_batch_count = 0
            # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶è¾“å‡º
        print(f"\nã€è°ƒè¯•ã€‘å¼€å§‹ç¬¬ {epoch+1} è½®è®­ç»ƒ")
        for batch_idx,batch_data in enumerate(train_loader):
            print(f"\nã€è°ƒè¯•ã€‘ç¬¬ {batch_idx+1} æ‰¹æ¬¡")
            # print("batch_data å†…å®¹ï¼š", batch_data)

            inputs, targets, dates = batch_data
            print("åŸå§‹inputs.shape:", inputs.shape)
            print("åŸå§‹targets.shape:", targets.shape)
            print("inputsè®¾å¤‡:", inputs.device)
            print("targetsè®¾å¤‡:", targets.device)
            # è½¬ç§»åˆ°è®¾å¤‡
            inputs = inputs.to(device)
            targets = targets.to(device)
                        
           
            # 1. è¾“å…¥æ•°æ®æ£€æŸ¥
            if not safety.check_inputs(inputs, targets):
                nan_batch_count += 1
                continue
                
            
            
            # 2. åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œå‰å‘ä¼ æ’­
            loss = None
            with safety.safe_forward_context():
                outputs = model(inputs)
                print("outputså½¢çŠ¶")
                print(outputs)
                print("outputsçš„å½¢çŠ¶ï¼š", outputs.shape)
                print(torch.min(outputs).item(), torch.max(outputs).item())
                
                if outputs is None:
                    print("æ¨¡å‹è¾“å‡ºä¸ºNoneï¼Œè·³è¿‡ä¿æŠ¤å’Œlossè®¡ç®—")
                    # å¯ä»¥å®šä¹‰è·³å‡ºå½“å‰batchæˆ–ç»ˆæ­¢
                    continue
                if model2 is None:
                    raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œä¸èƒ½è¿›è¡Œé¢„æµ‹ï¼")
                model2_result = predict(model2, outputs)
                print("æ¨¡å‹äºŒæ¨ç†ç»“æœï¼š", model2_result)
                # è®¡ç®—å¥–åŠ±ï¼ˆç¤ºä¾‹ï¼‰
                direction_match = (torch.sign(model2_result[:,0]) == torch.sign(targets)).float()
                accuracy_reward = direction_match * 0.8
                error_reward = torch.exp(-2 * torch.abs(model2_result[:,0] - targets)) * 0.2
                reward = (accuracy_reward + error_reward).detach()
                print("æ¨¡å‹è¾“å‡ºï¼ˆè°ƒè¯•ï¼‰ï¼š", outputs)
                print(f"å¹³å‡å¥–åŠ±: {reward.mean().item():.4f}")
                outputs_protected = safety.protect_outputs(outputs)

                if outputs_protected is None:
                    print("ä¿æŠ¤åè¾“å‡ºä¸ºNoneï¼Œè·³è¿‡lossè®¡ç®—")
                    continue

                # ä»¥ä¿æŠ¤åè¾“å‡ºè¿›è¡Œloss
                loss = rl_criterion(outputs_protected, model2_result, targets, reward)
                batch_reward = reward.mean().item()
                print("å½“å‰loss:", loss.item())
                
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_max = param.grad.abs().max().item()
                    print(f"å±‚ {name} æ¢¯åº¦æœ€å¤§å€¼: {grad_max}")
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
            safety.check_gradients()
            optimizer.step()
            batch_reward = reward.mean().item()
            epoch_loss += loss.item()
            valid_batch_count += 1
            # æ¯10æ‰¹æ¬¡æŠ¥å‘Šæ¢¯åº¦çŠ¶æ€
            if batch_idx % 10 == 0:
                grad_monitor.report(batch_idx, epoch)
        # ç§»é™¤æ¢¯åº¦ç›‘æ§é’©å­
        grad_monitor.detach()
        
        # æŠ¥å‘Šå½“å‰epochçŠ¶æ€
        if valid_batch_count > 0:
            avg_loss = epoch_loss / valid_batch_count
            logger.info(f"ğŸ Epoch {epoch+1} | å¹³å‡æŸå¤±: {avg_loss:.6f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")
        else:
            logger.error(f"â›” Epoch {epoch+1} æ²¡æœ‰æœ‰æ•ˆè®­ç»ƒæ‰¹æ¬¡ï¼Œå°è¯•æ¢å¤...")
            recovered_epoch = safety.recover(epoch)
            logger.warning(f"æ¢å¤è‡³epoch {recovered_epoch}")
            epoch = recovered_epoch  # é‡ç½®epochè®¡æ•°å™¨
        
    
    # æµ‹è¯•å’Œè¯„ä¼°é˜¶æ®µ
    model.eval()
    all_predictions = []
    all_targets = []
    all_dates = []
    test_rewards = []
    with torch.no_grad():
        for inputs, targets, dates in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            outputs= model(inputs)
            
            all_predictions.extend(outputs.squeeze().cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_dates.extend(dates)
            model2_preds = predict(model2, outputs)
            direction_match = (torch.sign(model2_preds[:,0]) == torch.sign(targets)).float()
            accuracy_reward = direction_match * 0.8
            error_reward = torch.exp(-2 * torch.abs(model2_preds[:,0] - targets)) * 0.2
            batch_reward = (accuracy_reward + error_reward).cpu().numpy()
            test_rewards.extend(batch_reward)
    # è½¬æ¢ä¸ºDataFrameä¾¿äºä¿å­˜ç»“æ„åŒ–æ•°æ®
    results_df = pd.DataFrame({
        'date': test_dataset.dates[:len(all_predictions)],
        'prediction': all_predictions,
        'target': all_targets,
        'error': np.abs(np.array(all_predictions) - np.array(all_targets))
    })
    
    # ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_output
    output_path = save_structured_data(results_df, config, f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))
    rmse = np.sqrt(np.mean((np.array(all_predictions) - np.array(all_targets))**2))
    sharpe_val = sharpe_ratio(np.array(all_predictions))
    drawdown = max_drawdown(np.array(all_predictions))
    if test_rewards:
        avg_test_reward = sum(test_rewards) / len(test_rewards)
        reward_std = np.std(test_rewards)  # éœ€è¦import numpy as np
    else:
        avg_test_reward = 0.0
        reward_std = 0.0
    eval_results = {
        "MAE": mae,
        "RMSE": rmse,
        "Sharpe_Ratio": sharpe_val,
        "Max_Drawdown": drawdown,
        "num_samples": len(all_targets),
        "input_features": train_dataset.feature_names,
        "config_path": config_path,
        "test_data_path": test_data_path,
        "model_summary": str(model),
        "Avg_Reward": avg_test_reward,  # æ–°å­—æ®µ
        "Reward_STD": reward_std,       # æ–°å­—æ®µ
    }
    eval_results_serializable = convert_to_serializable(eval_results) #è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))
    rmse = np.sqrt(np.mean((np.array(all_predictions) - np.array(all_targets))**2))
    sharpe_val = sharpe_ratio(np.array(all_predictions))
    drawdown = max_drawdown(np.array(all_predictions))
    
    eval_results = {
        "MAE": mae,
        "RMSE": rmse,
        "Sharpe_Ratio": sharpe_val,
        "Max_Drawdown": drawdown,
        "num_samples": len(all_targets),
        "input_features": train_dataset.feature_names,
        "config_path": config_path,
        "test_data_path": test_data_path,
        "model_summary": str(model)
    }
    eval_results_serializable = convert_to_serializable(eval_results)
    
    output_path, _ = save_evaluation_results(eval_results, config, filename="evaluation_" + datetime.now().strftime('%Y%m%d_%H%M%S') + ".json")
    with open(output_path, 'w') as f:
        json.dump(eval_results_serializable, f, indent=4)
    # ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_eval
    save_evaluation_results(eval_results, config, f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    # é¢„æµ‹é˜¶æ®µï¼ˆå¦‚æœé…ç½®ï¼‰
    if 'predict' in config and config['predict'].get('steps', 0) > 0:
        logger.info(f"ğŸ”® å¼€å§‹é¢„æµ‹, æ­¥æ•°: {config['predict']['steps']}")

        # è®¡ç®—æœ€å¤§é•¿åº¦
        len_dates = len(test_dataset.dates)
        len_preds = len(all_predictions)
        max_len = max(len_dates, len_preds, config['predict']['steps'])

        # è¡¥é½dates
        dates_padded = np.pad(test_dataset.dates, (0, max_len - len_dates), constant_values=np.nan)
        # è¡¥é½é¢„æµ‹ç»“æœ
        preds_padded = np.pad(all_predictions, (0, max_len - len_preds), constant_values=np.nan)
        # ç”Ÿæˆ confidence æ•°ç»„ï¼Œé•¿åº¦ä¸º max_len
        confidence_array = np.random.uniform(0.7, 0.95, max_len)

        # åªå–å‰ max_len
        dates_for_df = dates_padded[:max_len]
        preds_for_df = preds_padded[:max_len]

        # ä¿å­˜é¢„æµ‹ç»“æœ
        predict_path = Path(config['predict']['output_path'])
        predict_path.parent.mkdir(parents=True, exist_ok=True)

        # æ„é€  DataFrame
        predict_df = pd.DataFrame({
            'date': dates_for_df,
            'prediction': preds_for_df,
            'confidence': confidence_array
        })

        predict_df.to_csv(predict_path, index=False)
        logger.info(f"âœ… ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {predict_path}")

    logger.info("ğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="RLäº¤æ˜“ç³»ç»Ÿè®­ç»ƒè„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument("--config", 
                        type=str, 
                        default="configs/model1.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: configs/model1.yaml")
    
    args = parser.parse_args()
    main(args.config)