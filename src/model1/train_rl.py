import math
from transformers import AutoConfig, AutoTokenizer
import os
import re
from collections import OrderedDict
from venv import logger
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import ConcatDataset, DataLoader, Dataset
from pathlib import Path
import numpy as np
import pandas as pd
import yaml                                                                    
import logging
import json
import argparse
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from torch.utils.data import DataLoader, Dataset, random_split
import torch.nn.functional as F 
from contextlib import contextmanager
import inspect
import warnings
from ..model2.inference import DeepSeekPredictor
from transformers import AutoTokenizer, AutoModel  
from sklearn.metrics import mean_squared_error, mean_absolute_error


if torch.cuda.is_available():
    device = torch.device("cuda:0")  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPU")
class SafetyModule(nn.Module):  # ğŸ”´ å…³é”®ï¼šç»§æ‰¿nn.Module
    def __init__(self, input_size=773):
        super().__init__()
        self.protector = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.Linear(256, 256)
        )
    
    def forward(self, x):
        return self.protector(x)

config_path = '/home/liyakun/twitter-stock-prediction/configs/model1.yaml'
def load_model(config_path):
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if not config:
            print("é…ç½®ä¸ºç©ºæˆ–æ ¼å¼æœ‰è¯¯")
            return None
        return config
    except Exception as e:
        print("è¯»å–é…ç½®å¤±è´¥ï¼š", e)
        return None
config = load_model(config_path)

try:
    model_path = config['env']['prediction_model_path']
    print("æ¨¡å‹è·¯å¾„ï¼š", model_path)
except KeyError:
    print("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° prediction_model_path å­—æ®µã€‚")
    # ä½ å¯ä»¥è®¾ç½®é»˜è®¤è·¯å¾„æˆ–æŠ›å‡ºå¼‚å¸¸
if not config:
    raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥ï¼")

# æå–å‚æ•°å’Œæ¨¡å‹è·¯å¾„



class ConfigLoader:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config

def setup_logger(log_file: str) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_dir = Path(log_file).parent
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logger = logging.getLogger("training")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ä¹‹å‰çš„handlersé¿å…é‡å¤
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_file)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    return logger

def create_output_directories(config: Dict[str, Any]) -> None:
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•"""
    paths = config['paths']
    # æ¨¡å‹è¾“å‡ºç›®å½• (model1_output/)
    output_dir = Path(paths.get('processed_output_dir', 'data/processed/model1_output'))
    output_dir.mkdir(parents=True, exist_ok=True)
    # è¯„ä¼°ç»“æœç›®å½• (model1_eval/)
    eval_dir = Path(paths.get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_dir = Path(config['paths']['output_model']).parent
    model_save_dir.mkdir(parents=True, exist_ok=True)
    # æ—¥å¿—æ–‡ä»¶ç›®å½•
    log_dir = Path(config['paths']['log_file']).parent
    log_dir.mkdir(parents=True, exist_ok=True)

def convert_to_serializable(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(obj)
    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.detach().cpu().tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]
    else:
        return obj

def save_structured_data(data: pd.DataFrame, config: Dict[str, Any], filename: str = "processed_data.csv") -> Path:
    """
    ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_outputç›®å½•
    """
    output_dir = Path(config['paths'].get('processed_output_dir', 'data/processed/model1_output'))
    output_path = output_dir / filename 
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    # æ·»åŠ æ—¥æœŸåˆ—ä½œä¸ºç»“æ„åŒ–æ•°æ®çš„ä¸€éƒ¨åˆ†
    if 'date' not in data.columns:
        if 'timestamp' in data.columns:
            data['date'] = pd.to_datetime(data['timestamp']).dt.date
        else:
            data['date'] = pd.Timestamp.now().strftime("%Y-%m-%d")
    data.to_csv(output_path, index=False)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°: {output_path}")
    return output_path

def save_evaluation_results(results: Dict[str, Any], config: Dict[str, Any], filename: str = "evaluation_results.json") -> Tuple[Path, Path]:
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_evalç›®å½•
    """
    eval_dir = Path(config['paths'].get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    output_path = eval_dir / filename
    results_serializable = convert_to_serializable(results)
    with open(output_path, 'w') as f:
        json.dump(results_serializable, f, indent=4)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ä¾¿äºåˆ†æ
    csv_path = output_path.with_suffix('.csv')
    eval_df = pd.DataFrame([results])
    eval_df.to_csv(csv_path, index=False)
    logger.info(f"âœ… åŒæ—¶ä¿å­˜CSVæ ¼å¼è¯„ä¼°ç»“æœ: {csv_path}")
    
    return output_path, csv_path

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    """è®¡ç®—å¤æ™®æ¯”ç‡"""
    excess_returns = returns - risk_free_rate
    return excess_returns.mean() / (excess_returns.std() + 1e-8)

def max_drawdown(values: np.ndarray) -> float:
    """è®¡ç®—æœ€å¤§å›æ’¤"""
    peak = np.maximum.accumulate(values)
    drawdown = (values - peak) / peak
    return np.min(drawdown)
class SafetyController:
    def __init__(self, model, optimizer, config, logger):
        self.model = model
        self.optimizer = optimizer
        self.config = config['training']['stability']
        self.logger = logger
        self.epoch_backup_count = 0
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        self.backup_dir = Path(f"{config['paths']['log_file']}_safety_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        # å­¦ä¹ ç‡å¤‡ä»½
        self.original_lr = optimizer.param_groups[0]['lr']
    
    def check_inputs(self, inputs, targets):
        """è¾“å…¥æ•°æ®å¥åº·æ£€æŸ¥"""
        nan_count = torch.isnan(inputs).sum().item() + torch.isnan(targets).sum().item()
        inf_count = torch.isinf(inputs).sum().item() + torch.isinf(targets).sum().item()
        
        if nan_count > self.config.get('max_nan_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«NaNæ•°æ®çš„æ‰¹æ¬¡ (NaNæ•°é‡: {nan_count})")
            return False
        
        if inf_count > self.config.get('max_inf_allowed', 0):
            self.logger.warning(f"è·³è¿‡å«Infæ•°æ®çš„æ‰¹æ¬¡ (Infæ•°é‡: {inf_count})")
            return False 
        return True
    
    def protect_outputs(self, outputs):
        """æ¨¡å‹è¾“å‡ºé˜²æŠ¤"""
        if torch.isnan(outputs).any():
            self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œæ‰§è¡Œä¿®å¤...")
            outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1e4, neginf=-1e4)
          # ä¿®å¤NaN/Inféœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆä½¿ç”¨torch.whereå¯å¯¼ï¼‰
        outputs = torch.where(
       torch.isnan(outputs) | torch.isinf(outputs),
       torch.zeros_like(outputs),
       outputs
   )    
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        outputs = torch.clamp(outputs, 
                              min=self.config['output_clip_range'][0], 
                              max=self.config['output_clip_range'][1]) 
        return outputs
    
    def check_gradients(self):
        """æ¢¯åº¦å¥åº·æ£€æŸ¥å’Œä¿®å¤"""
        max_grad_norm = self.config.get('max_grad_norm', 1.0)
        problematic_layers = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad              
                # ä¿®å¤NaN/Infæ¢¯åº¦
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    problematic_layers.append(name)
                    param.grad = torch.where(
                        torch.isnan(grad) | torch.isinf(grad),
                        torch.zeros_like(grad),
                        grad
                    )            
                # æ¢¯åº¦è£å‰ª
                torch.clamp_(param.grad, -self.config['gradient_clip_value'], 
                            self.config['gradient_clip_value'])
        
        # å…¨å±€æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_grad_norm) 
        if problematic_layers:
            self.logger.warning(f"ä¿®å¤æ¢¯åº¦å¼‚å¸¸å±‚: {problematic_layers[:3]}{'...' if len(problematic_layers)>3 else ''}")
        
        return len(problematic_layers) == 0
    
    def create_backup(self, epoch):
        """åˆ›å»ºå®‰å…¨æ¢å¤ç‚¹"""
        if epoch % self.config.get('backup_interval', 5) == 0:
            backup_path = self.backup_dir / f"epoch_{epoch}_safety_backup.pt"
            torch.save({
                'model_state': self.model.state_dict(),
                'optimizer_state': self.optimizer.state_dict(),
                'epoch': epoch
            }, backup_path)
            
            # ä¿å­˜æœ€è¿‘çš„5ä¸ªå¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if len(backup_files) > 5:
                os.remove(backup_files[0])
    
    def recover(self, current_epoch):
        """ä»é”™è¯¯ä¸­æ¢å¤"""
        recovery_type = self.config.get('recovery_strategy', 'backoff')    
        if recovery_type == 'backoff':
            # å­¦ä¹ ç‡é€€é¿
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= self.config.get('lr_backoff_factor', 0.5)            
            self.logger.warning(f"é‡‡ç”¨å­¦ä¹ ç‡é€€é¿ç­–ç•¥ï¼Œæ–°å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")            
            # å°è¯•æ¢å¤æœ€è¿‘çš„å¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if backup_files:
                latest_backup = backup_files[-1]
                checkpoint = torch.load(latest_backup)
                self.model.load_state_dict(checkpoint['model_state'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                self.logger.warning(f"ä»å¤‡ä»½æ¢å¤: {latest_backup.name} (epoch {checkpoint['epoch']})")
                return checkpoint['epoch']  # è¿”å›æ¢å¤åˆ°çš„epoch
            
        elif recovery_type == 'reset':
            # å®Œå…¨é‡ç½®æ¨¡å‹
            self.logger.error("æ‰§è¡Œæ¨¡å‹å®Œå…¨é‡ç½®!")            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            for module in self.model.modules():
                if hasattr(module, 'reset_parameters'):
                    module.reset_parameters()            
            # é‡ç½®ä¼˜åŒ–å™¨
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.original_lr            
            return 0  # ä»å¤´å¼€å§‹è®­ç»ƒ
        
        return current_epoch
    
    @contextmanager
    def safe_forward_context(self):
        """å®‰å…¨å‰å‘ä¼ æ’­ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # å¯ç”¨PyTorchå¼‚å¸¸æ£€æµ‹
            torch.autograd.set_detect_anomaly(True)
            yield
        except RuntimeError as e:
            self.logger.error(f"å‰å‘ä¼ æ’­å¼‚å¸¸: {str(e)}")
            # æå–å¼‚å¸¸ä½ç½®
            stack = inspect.stack()
            caller = stack[1]  # è°ƒç”¨safe_forward_contextçš„å‡½æ•°
            self.logger.error(f"å¼‚å¸¸ä½ç½®: {caller.filename}:{caller.lineno}")
            raise
        finally:
            torch.autograd.set_detect_anomaly(False)

# === æ–°å¢: æ¢¯åº¦ç›‘æ§å™¨ ===
class GradientMonitor:
    def __init__(self, model, layers_to_watch=None):
        self.model = model
        self.layers = layers_to_watch or self._identify_critical_layers()
        self.handles = []
        self.logger = logging.getLogger("gradient_monitor")
        self.reset_stats()
    
    def _identify_critical_layers(self):
        """è¯†åˆ«å…³é”®å±‚ï¼ˆæœ€åä¸€å±‚å’Œå«LayerNormçš„å±‚ï¼‰"""
        critical_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
                critical_layers.append(name)
            if 'out' in name or 'final' in name or 'proj' in name:
                critical_layers.append(name)
        return list(set(critical_layers))
    
    def reset_stats(self):
        self.max_grad = 0.0
        self.max_grad_layer = None
        self.nan_count = 0
    
    def _hook_fn(self, module, grad_input, grad_output, layer_name):
        # æ£€æŸ¥NaNå’ŒInf
        for g in grad_output:
            if g is not None:
                if torch.isnan(g).any():
                    self.nan_count += torch.isnan(g).sum().item()
                if torch.isinf(g).any():
                    self.nan_count += torch.isinf(g).sum().item()        
        # è®°å½•æœ€å¤§æ¢¯åº¦
        for g in grad_output:
            if g is not None:
                grad_norm = torch.norm(g)
                if grad_norm > self.max_grad:
                    self.max_grad = grad_norm.item()
                    self.max_grad_layer = layer_name
    
    def attach(self):
        """é™„åŠ æ¢¯åº¦é’©å­"""
        self.reset_stats()
        for name in self.layers:
            module = self._get_module(name)
            hook = lambda m, gi, go, n=name: self._hook_fn(m, gi, go, n)
            handle = module.register_full_backward_hook(hook)
            self.handles.append(handle)
    
    def detach(self):
        """ç§»é™¤æ¢¯åº¦é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles = []
    
    def _get_module(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å—"""
        names = name.split('.')
        module = self.model
        for n in names:
            module = getattr(module, n)
        return module
    
    def report(self, batch_idx, epoch):
        """ç”Ÿæˆæ¢¯åº¦æŠ¥å‘Š"""
        report = f"æ¢¯åº¦ç›‘æ§ - Epoch {epoch} Batch {batch_idx}:\n"
        report += f"  ğŸš© æœ€å¤§æ¢¯åº¦: {self.max_grad:.2e} ({self.max_grad_layer})\n"
        report += f"  âš ï¸ å¼‚å¸¸æ¢¯åº¦è®¡æ•°: {self.nan_count}"
        
        if self.nan_count > 0:
            self.logger.warning(report)
        elif batch_idx % 10 == 0:
            self.logger.info(report)


# === æ–°å¢: SmoothL1æŸå¤±å‡½æ•° ===
class RLoss(nn.Module):
    def __init__(self, supervised_criterion, base_loss_weight=0.5):
        super().__init__()
        self.base_loss_weight = base_loss_weight
        self.supervised_criterion = supervised_criterion
    
    def forward(self, model_outputs, targets, reward):
        supervised_loss = self.supervised_criterion(model_outputs, targets)
        # 3. åˆ†æƒ…å†µå¤„ç†çš„ç­–ç•¥æŸå¤±
        policy_loss, match_rate = self.calculate_policy_loss(
            model_outputs, 
            targets,
            reward,
        )
        print("ç›‘ç£æŸå¤± grad_fn:",supervised_loss.grad_fn)
        print("ç­–ç•¥æŸå¤± grad_fn:", policy_loss.grad_fn) 
        # 4. åŠ¨æ€è°ƒæ•´æƒé‡
        volatility = torch.abs(targets).mean()
        current_weight = self.dynamic_weight_adjust(volatility)

        # 5. æ··åˆæŸå¤± - ç»¼åˆæ‰€æœ‰æˆåˆ†
        total_loss = (
            current_weight * supervised_loss +
            (1 - current_weight) * 0.7 * policy_loss
        )
        # è¿”å›æŸå¤±å’Œç›¸å…³æŒ‡æ ‡
        return {
            "total_loss": total_loss,
            "supervised_loss": supervised_loss,
            "policy_loss": policy_loss,
            "weight": current_weight,
            "mean_reward": reward.mean(),
            "match_rate": match_rate
        }

    def calculate_policy_loss(self, model_outputs, targets, reward):
        logits = model_outputs["logits"]        
        # ç¡®ä¿å¤„ç†æ‰¹é‡æ•°æ®
        if logits.dim() == 1:
            logits = logits.unsqueeze(0)  
        # å¯¹ç‰¹å¾ç»´åº¦åº”ç”¨softmax (dim=1)
        action_probs = F.softmax(logits, dim=1)        
        # æ¯ä¸ªæ ·æœ¬é€‰æ‹©æœ€å¤§æ¦‚ç‡åŠ¨ä½œ
        position_direction = torch.argmax(action_probs, dim=1)  # [batch_size]        
        # é£é™©åˆ¤æ–­ - ç¡®ä¿targetsé€‚å½“å½¢çŠ¶
        if targets.dim() > 1:
            targets = targets.squeeze()
        risk_mask = targets < -0.05  # [batch_size]        
        # æœ‰æ•ˆåŠ¨ä½œè®¾ç½®
        valid_actions = torch.zeros_like(position_direction)
        valid_actions[risk_mask] = 3        
        # è®¡ç®—åŒ¹é…ç‡
        directional_match = (position_direction == valid_actions).float()        
        # å®‰å…¨é€‰æ‹©æ¦‚ç‡ï¼ˆé¿å…in-placeæ“ä½œï¼‰
        batch_indices = torch.arange(logits.size(0))
        chosen_probs = action_probs[batch_indices, position_direction]
        log_probs = torch.log(chosen_probs + 1e-8)        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±
        with torch.no_grad():
            advantage = reward * (1.0 + 0.5 * directional_match)
        
        rl_loss = -torch.mean(log_probs * advantage)
        
        return rl_loss, directional_match.mean()
    
    def dynamic_weight_adjust(self, volatility):
        """æ ¹æ®å¸‚åœºæ³¢åŠ¨ç‡è°ƒæ•´ç›‘ç£æŸå¤±æƒé‡ (å®Œå…¨ä¿ç•™)"""
        return torch.clamp(0.65 - volatility * 25, min=0.2, max=0.7).item()

def get_rl_weight(epoch):
    """æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ç›‘ç£æŸå¤±æƒé‡"""
    # ç¬¬ä¸€é˜¶æ®µï¼šä¾§é‡ç›‘ç£å­¦ä¹ ï¼ˆepoch 0-9ï¼‰
    if epoch < 10:
        return 0.7
    # ç¬¬äºŒé˜¶æ®µï¼šå¹³è¡¡å­¦ä¹ ï¼ˆepoch 10-19ï¼‰
    elif epoch < 20:
        # çº¿æ€§è¿‡æ¸¡ï¼š0.7 â†’ 0.5
        return 0.7 - 0.2 * (epoch - 9) / 10
    # ç¬¬ä¸‰é˜¶æ®µï¼šä¾§é‡å¼ºåŒ–å­¦ä¹ ï¼ˆepoch â‰¥20ï¼‰
    else:
        return 0.3
    
class SafeSmoothL1Loss(nn.Module):
    """æ•°å€¼ç¨³å®šçš„SmoothL1æŸå¤±"""
    def __init__(self, beta=1.0):
        super().__init__()
        self.beta = beta
     
    def forward(self, model_outputs, target):
        if isinstance(model_outputs, dict):
        # å°è¯•å¸¸è§é”®åï¼ˆæ ¹æ®å®é™…æ¨¡å‹è¾“å‡ºé”®åè°ƒæ•´ï¼‰
            if "logits" in model_outputs:
                input_tensor = model_outputs["logits"]
            elif "value" in model_outputs:
                input_tensor = model_outputs["value"]
            else:
                # å¦‚æœæ²¡æœ‰æ ‡å‡†é”®åï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
                input_tensor = next(iter(model_outputs.values()))
        else:
            input_tensor = model_outputs
        print("model_outputs",input_tensor)
        safe_input = torch.clamp(input_tensor, -50, 50)
        safe_target = torch.clamp(target, -50, 50)
        diff = safe_input - safe_target
        diff = torch.where(torch.isinf(diff), torch.zeros_like(diff), diff)
        diff = torch.where(torch.isnan(diff), torch.zeros_like(diff), diff)
        
        abs_diff = torch.abs(diff)
        mask = abs_diff < self.beta        
        # L2éƒ¨åˆ†: 0.5 * x^2 / beta
        l2_loss = 0.5 * torch.pow(diff, 2) / self.beta
        # L1éƒ¨åˆ†: |x| - 0.5 * beta
        l1_loss = abs_diff - 0.5 * self.beta        
        loss = torch.where(mask, l2_loss, l1_loss)
        loss = loss.mean()
        if torch.isnan(loss) or torch.isinf(loss):
            warnings.warn("æ£€æµ‹åˆ°NaN/InfæŸå¤±å€¼ï¼Œæ‰§è¡Œä¿®å¤!")
            return torch.zeros_like(loss)
            
        return loss
class CSVDataset(Dataset):
    def __init__(
        self,
        data_path: Optional[str] = None,
        filter_nan: bool = True,
        sequence_length: int = 30,
        feature_names: Optional[List[str]] = None,
        exclude_text_columns: bool = True,
        min_valid_sequences: int = 100,
        logger: Optional[logging.Logger] = None
    ):
        super().__init__()
        self.logger = logger or setup_logger(self.__class__.__name__)
        self.sequence_length = sequence_length
        self.filter_nan = filter_nan
        self.feature_names = feature_names
        self.exclude_text_columns = exclude_text_columns
        self.min_valid_sequences = min_valid_sequences
        
        self.logger.info(f"ğŸ” åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
        self.logger.info(f"åºåˆ—é•¿åº¦: {sequence_length}, è¿‡æ»¤NaN: {filter_nan}, æœ€å°åºåˆ—æ•°: {min_valid_sequences}")
        self.logger.info("=" * 50)
        
        # é”™è¯¯æ—¥å¿—æ–‡ä»¶è·¯å¾„
        error_log_dir = "logs"
        os.makedirs(error_log_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.error_log_path = os.path.join(error_log_dir, f"data_parse_errors_{timestamp}.log")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®æ–‡æœ¬
        self.logger.info("ğŸ“¥ è¯»å–åŸå§‹æ•°æ®...")
        try:
            with open(data_path, 'r', encoding='utf-8', errors='replace') as f:
                raw_lines = f.readlines()
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
            raise
        
        self.logger.info(f"ğŸ“„ è¯»å– {len(raw_lines)} è¡Œæ•°æ®")
        
        # 2. æå–ç»“æ„åŒ–æ•°æ®
        self.logger.info("\nâš™ï¸ è§£æç»“æ„åŒ–æ•°æ®...")
        data_records = []
        valid_count = 0
        error_count = 0
        
        # æ‰“å°å‰5è¡Œç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ” å‰5è¡ŒåŸå§‹æ•°æ®å†…å®¹:")
        for i, line in enumerate(raw_lines[:5], 1):
            self.logger.info(f"è¡Œ {i} åŸå§‹å†…å®¹: {line.strip()[:200]}")
        
        with open(self.error_log_path, "w", encoding="utf-8") as error_log:
            self.logger.info(f"å¼€å§‹è§£æ... (é”™è¯¯æ—¥å¿—å°†ä¿å­˜åˆ°: {self.error_log_path})")
            
            for i, line in enumerate(raw_lines, 1):
                if i % 1000 == 0 or i <= 5 or i == len(raw_lines):
                    self.logger.info(f"å¤„ç†ä¸­... å·²å¤„ç†: {i}/{len(raw_lines)} è¡Œ")
                
                try:
                    if not line.strip():
                        continue
                        
                    # å°è¯•è§£æè¡Œ
                    record = self._parse_hydra_format(line)
                    
                    # DEBUG: æ‰“å°å‰5è¡Œçš„è§£æç»“æœ
                    if i <= 5:
                        self.logger.info(f"è¡Œ {i} è§£æç»“æœ: {str(record)[:300]}")
                    
                    # æ£€æŸ¥è§£æç»“æœ
                    if not isinstance(record, dict) or not record:
                        raise ValueError("è§£æç»“æœä¸ºç©ºæˆ–ä¸æ˜¯å­—å…¸")
                        
                    # ğŸ”‘ é”®åè§„èŒƒåŒ–å¤„ç† (æ ¸å¿ƒä¿®æ”¹)
                    normalized_record = {}
                    for key, value in record.items():
                        # æ ‡å‡†åŒ–é”®å: å°å†™ï¼Œå»æ‰å‰åç©ºæ ¼ï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
                        clean_key = str(key).strip().lower().replace(" ", "_")
                        normalized_record[clean_key] = value
                    
                    record = normalized_record
                    
                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    missing_fields = []
                    if 'timestamp' not in record:
                        missing_fields.append('timestamp')
                        record['timestamp'] = pd.Timestamp.now()
                    
                    if 'symbol' not in record:
                        missing_fields.append('symbol')
                        record['symbol'] = f"UNKNOWN_{i}"
                    
                    if 'sentiment' not in record:
                        missing_fields.append('sentiment')
                        record['sentiment'] = 0.0
                    
                    # å¦‚æœæœ‰ç¼ºå¤±å­—æ®µï¼Œè®°å½•è­¦å‘Š
                    if missing_fields:
                        self.logger.debug(f"è¡Œ {i}: ç¼ºå¤±å¿…è¦å­—æ®µ {', '.join(missing_fields)}ï¼Œå·²æ·»åŠ é»˜è®¤å€¼")
                    
                    # ç±»å‹è½¬æ¢ç¡®ä¿æ­£ç¡®
                    try:
                        if isinstance(record['timestamp'], str):
                            record['timestamp'] = pd.to_datetime(record['timestamp'], errors='coerce')
                        if not isinstance(record['timestamp'], pd.Timestamp):
                            record['timestamp'] = pd.Timestamp.now()
                    except Exception:
                        record['timestamp'] = pd.Timestamp.now()
                    
                    try:
                        record['sentiment'] = float(record['sentiment'])
                    except:
                        record['sentiment'] = 0.0
                    
                    # æ·»åŠ è®°å½•
                    data_records.append(record)
                    valid_count += 1
                    
                except Exception as e:
                    error_count += 1
                    # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    error_details = {
                        "error": str(e),
                        "line_number": i,
                        "raw_content": line.strip()[:500],
                        "parsed_result": str(record)[:300] if "record" in locals() else "æœªè§£æ"
                    }
                    error_log.write(json.dumps(error_details, indent=2) + "\n\n")
                    
                    if error_count % 100 == 0:
                        self.logger.warning(f"è§£æé”™è¯¯æ•°å·²è¾¾ {error_count} (æœ€åé”™è¯¯: {str(e)})")
        
        self.logger.info(f"âœ… æˆåŠŸè§£æ {valid_count} æ¡è®°å½•")
        self.logger.info(f"âš ï¸ è§£æé”™è¯¯: {error_count} æ¡ (è¯¦ç»†æ—¥å¿—: {self.error_log_path})")
        
        # å¦‚æœæœ‰æ•ˆè®°å½•å°‘äºé˜ˆå€¼ï¼Œå‘å‡ºä¸¥é‡è­¦å‘Š
        if valid_count < min_valid_sequences:
            self.logger.warning(f"âš ï¸ æœ‰æ•ˆè®°å½•æ•° ({valid_count}) ä½äºé˜ˆå€¼ ({min_valid_sequences})")
        
        # 3. åˆ›å»ºDataFrame
        self.logger.info("\nğŸ“Š åˆ›å»ºDataFrame...")
        self.df = pd.DataFrame(data_records) if data_records else pd.DataFrame()
        
        if self.df.empty:
            self.logger.warning("âš ï¸ è­¦å‘Š: DataFrameä¸ºç©ºï¼Œæ·»åŠ å ä½ç¬¦")
            self.df = pd.DataFrame({
                'timestamp': [pd.Timestamp.now() for _ in range(sequence_length)],
                'symbol': ['PLACEHOLDER' for _ in range(sequence_length)],
                'sentiment': [0.0 for _ in range(sequence_length)]
            })
        
        self.logger.info(f"ğŸ“‹ æ•°æ®é›†åŒ…å« {self.df['symbol'].nunique()} åªè‚¡ç¥¨/ç»„")
        
        # 4. å¤„ç†æ–‡æœ¬åˆ—
        if exclude_text_columns:
            text_columns = [col for col in self.df.columns if col in ['text', 'content', 'message']]
            self.logger.info(f"ğŸ“ æ’é™¤ {len(text_columns)} ä¸ªæ–‡æœ¬åˆ—: {text_columns}")
            self.df = self.df.drop(columns=text_columns, errors='ignore')
        
        # 5. ç¡®å®šç‰¹å¾åˆ—
        self.feature_names = self._detect_features() if feature_names is None else feature_names
        if not self.feature_names:
            self.logger.warning("âš ï¸ è‡ªåŠ¨é‡æ–°ç¡®å®šç‰¹å¾åç§°: 0 ä¸ªç‰¹å¾")
        
        # 6. æ¸…ç†å’Œè½¬æ¢
        self.logger.info("\nğŸ› ï¸ æ•°æ®æ¸…ç†å’Œè½¬æ¢...")
        self._clean_and_transform()
        
        # 7. åˆ›å»ºåºåˆ—
        self.logger.info("\nâ±ï¸ åˆ›å»ºåºåˆ—...")
        self._create_sequences()
        
        # 8. è¿‡æ»¤NaNåºåˆ—
        self.logger.info("\nğŸ§¹ è¿‡æ»¤NaNåºåˆ—...")
        self._filter_nan_sequences()
        
        self.logger.info("\nğŸ‰ æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\n{self.get_data_report()}")

    def _parse_hydra_format(self, line: str) -> dict:
        """è§£æå¤šç§æ ¼å¼çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œç‰¹åˆ«é’ˆå¯¹Pythonå­—å…¸å­—ç¬¦ä¸²æ ¼å¼"""
        try:
            # æ¸…ç†è¾“å…¥
            clean_line = line.strip()
            
            # æ–°å¢ï¼šå¤„ç†æ—¥å¿—ä¸­é”™è¯¯ç±»å‹2çš„æƒ…å†µ - å¤–éƒ¨åŒå¼•å·åŒ…è£¹é—®é¢˜
            # ç¤ºä¾‹: "\"{'symbol': 'GOOG', ...}\""
            if clean_line.startswith('"') and clean_line.endswith('"'):
                inner = clean_line[1:-1]
                # å¦‚æœå†…å®¹æœ¬èº«æ˜¯æœ‰æ•ˆçš„å­—å…¸æ ¼å¼ï¼Œå°è¯•è§£æ
                if inner.startswith("{") or inner.startswith("'{"):
                    clean_line = inner
            
            # 1. å°è¯•ä½¿ç”¨ast.literal_evalï¼ˆæœ€å¯èƒ½å¤„ç†Pythonå­—å…¸æ ¼å¼ï¼‰
            try:
                import ast
                parsed = ast.literal_eval(clean_line)
                if isinstance(parsed, dict):
                    return parsed
            except (SyntaxError, ValueError, TypeError):
                pass
            
            # æ–°å¢ï¼šæ›´å…¨é¢çš„å•å¼•å·è½¬åŒå¼•å·å¤„ç†
            # å¤„ç†ç±»ä¼¼: '{...}' æˆ– '{"key": "value"}' æ ¼å¼
            if clean_line.startswith("{") or clean_line.startswith("{"):
                # åˆ†æ­¥éª¤è½¬æ¢ä»¥ç¡®ä¿å®‰å…¨
                normalized = clean_line
                # æ›¿æ¢å•å¼•å·é”®å’Œå€¼
                normalized = re.sub(r"'\s*:\s*'", '": "', normalized)  # é”®å€¼å¯¹
                normalized = re.sub(r"'\s*,\s*'", '", "', normalized)  # é”®å€¼å¯¹ä¹‹é—´
                normalized = normalized.replace("{'", '{"').replace("'}", '"}')  # å¤§æ‹¬å·
                normalized = normalized.replace(": '", ': "').replace("',", '",')  # é€šç”¨æ›¿æ¢
                
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦
                normalized = normalized.replace("None", "null")
                normalized = normalized.replace("True", "true").replace("False", "false")
                
                # å°è¯•è§£æè½¬æ¢åçš„å­—ç¬¦ä¸²
                try:
                    return json.loads(normalized)
                except json.JSONDecodeError:
                    pass
            
            # 2. å°è¯•ç›´æ¥è§£æä¸ºJSON
            try:
                return json.loads(clean_line)
            except json.JSONDecodeError:
                pass
            
            # 3. å°è¯•è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆä¿ç•™åŸå§‹é€»è¾‘ï¼‰
            try:
                json_str = (
                    clean_line
                    .replace("'", '"')  # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                    .replace("None", "null")  # Python None â†’ JSON null
                    .replace("True", "true")  # Python True â†’ JSON true
                    .replace("False", "false")  # Python False â†’ JSON false
                    .replace("\\\"", "'")  # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
                )
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
            
            # 4. å°è¯•å¤„ç†åµŒå¥—å¼•å·ï¼ˆä¿ç•™åŸå§‹é€»è¾‘ï¼‰
            try:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†å¤æ‚å¼•å·ç»“æ„
                pattern = r'("[^"]*"|\'[^\']*\')'
                fixed_line = re.sub(pattern, lambda m: m.group(0).replace('"', "!QUOTE!"), clean_line)
                fixed_line = fixed_line.replace("'", '"').replace('!QUOTE!', "'")
                return json.loads(fixed_line)
            except:
                pass
            if clean_line.startswith('{') and not clean_line.endswith('}'):
            # å°è¯•è¡¥å…¨ç¼ºå°‘çš„ç»“æŸèŠ±æ‹¬å·
                fixed_line = clean_line.rstrip()  # ç§»é™¤å¯èƒ½çš„ç©ºç™½å­—ç¬¦
                if not fixed_line.endswith('}') and not fixed_line.endswith('"'):
                    # æ·»åŠ ç¼ºå¤±çš„ç»“æŸç»“æ„
                    if fixed_line.rfind(',') > fixed_line.rfind(':'):
                        # ç±»ä¼¼ 'key' è¿™ç§ä¸å®Œæ•´é”®çš„æƒ…å†µ
                        fixed_line = fixed_line.rsplit(',', 1)[0] + '}'
                    else:
                        fixed_line += '}'
                        
                    # å°è¯•è§£æä¿®å¤åçš„æ•°æ®
                    try:
                        return self._parse_hydra_format(fixed_line)
                    except:
                        pass
            
            return {}
        
        except Exception as e:
            self.logger.debug(f"è§£æå‡½æ•°å†…éƒ¨é”™è¯¯: {str(e)}")
            return {}

    def _detect_features(self) -> List[str]:
        """è‡ªåŠ¨æ£€æµ‹æ•°å€¼ç‰¹å¾åˆ—"""
        if self.df.empty:
            return []
        
        # æ’é™¤éæ•°å€¼åˆ—å’Œå…³é”®åˆ—
        non_features = ['symbol', 'timestamp', 'sentiment', 'text', 'content', 'message', 'id']
        numeric_cols = self.df.select_dtypes(include=['number', 'float', 'int']).columns.tolist()
        
        # è¿‡æ»¤éç‰¹å¾åˆ—
        features = [col for col in numeric_cols if col not in non_features]
        self.logger.info(f"ğŸ“Š è‡ªåŠ¨è¯†åˆ«å‡º {len(features)} ä¸ªç‰¹å¾åˆ—: {features[:10]}{'...' if len(features) > 10 else ''}")
        return features

    def _clean_and_transform(self):
        """æ•°æ®æ¸…ç†å’Œè½¬æ¢"""
        # 1. å¡«å……ç¼ºå¤±å€¼
        if self.df.empty:
            self.logger.warning("ğŸ§¼ å¡«å……ç¼ºå¤±å€¼ (è·³è¿‡ï¼Œæ•°æ®é›†ä¸ºç©º)")
            return
            
        missing_values = self.df.isnull().sum().sum()
        if missing_values > 0:
            self.logger.info(f"ğŸ§¼ å¡«å……ç¼ºå¤±å€¼ (å…± {missing_values} ä¸ª)")
            self.df = self.df.ffill().bfill().fillna(0)
        else:
            self.logger.info("ğŸ§¼ æ— ç¼ºå¤±å€¼")

    def _create_sequences(self):
        """ä¸ºæ¯ä¸ªè‚¡ç¥¨/åˆ†ç»„åˆ›å»ºæ—¶é—´åºåˆ—"""
        if self.df.empty:
            self.logger.warning("âš ï¸ è­¦å‘Š: æœªåˆ›å»ºä»»ä½•åºåˆ—ï¼Œä½¿ç”¨å ä½ç¬¦")
            self.sequences = [np.zeros((self.sequence_length, 1))]  # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦åºåˆ—
            self.targets = [0.0]
            return
        
        self.sequences = []
        self.targets = []
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†
        for symbol, group in self.df.groupby('symbol'):
            group = group.sort_values('timestamp')
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
            if len(group) < self.sequence_length:
                self.logger.debug(f"è·³è¿‡{symbol}: æ•°æ®ç‚¹ä¸è¶³ ({len(group)} < {self.sequence_length})")
                continue
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            features = group[self.feature_names].values if self.feature_names else np.zeros((len(group), 1))
            sentiments = group['sentiment'].values
            
            # åˆ›å»ºåºåˆ—
            for i in range(len(group) - self.sequence_length):
                seq = features[i:i+self.sequence_length]
                target = sentiments[i+self.sequence_length-1]
                self.sequences.append(seq)
                self.targets.append(target)
        
        if not self.sequences:
            self.logger.warning("âš ï¸ è­¦å‘Š: æœªåˆ›å»ºä»»ä½•æœ‰æ•ˆåºåˆ—ï¼Œä½¿ç”¨å ä½ç¬¦")
            self.sequences = [np.zeros((self.sequence_length, 1))]
            self.targets = [0.0]
            
        self.logger.info(f"âœ… åˆ›å»ºäº† {len(self.sequences)} ä¸ªæ—¶é—´åºåˆ— (æ¥è‡ª {self.df['symbol'].nunique()} åªè‚¡ç¥¨)")
        self.logger.info(f"ğŸ“ åºåˆ—å½¢çŠ¶: {self.sequences[0].shape}")

    def _filter_nan_sequences(self):
        """è¿‡æ»¤åŒ…å«NaNçš„åºåˆ—"""
        if not self.filter_nan:
            self.logger.info("è·³è¿‡è¿‡æ»¤NaNåºåˆ—")
            return
            
        original_count = len(self.sequences)
        valid_indices = [i for i, seq in enumerate(self.sequences) if not np.isnan(seq).any()]
        
        self.sequences = [self.sequences[i] for i in valid_indices]
        self.targets = [self.targets[i] for i in valid_indices]
        
        filtered_count = original_count - len(self.sequences)
        self.logger.info(f"âœ… æ‰€æœ‰ {len(self.sequences)} ä¸ªåºåˆ—å‡æœ‰æ•ˆ (è¿‡æ»¤äº† {filtered_count} ä¸ªæ— æ•ˆåºåˆ—)")
        
        # å¦‚æœåºåˆ—æ•°é‡ä¸è¶³ï¼Œæ·»åŠ å ä½ç¬¦
        if len(self.sequences) < self.min_valid_sequences:
            self.logger.warning(f"âš ï¸ åºåˆ—æ•°é‡ä¸è¶³ ({len(self.sequences)} < {self.min_valid_sequences})ï¼Œæ·»åŠ å ä½ç¬¦")
            placeholder = np.zeros((self.sequence_length, len(self.feature_names) if self.feature_names else 1))
            self.sequences.append(placeholder)
            self.targets.append(0.0)

    def get_data_report(self) -> str:
        """ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š"""
        report = f"  åºåˆ—æ•°é‡: {len(self.sequences)}\n"
        if self.sequences:
            report += f"  åºåˆ—é•¿åº¦: {self.sequence_length}\n"
            report += f"  ç‰¹å¾ç»´åº¦: {self.sequences[0].shape[1]}\n"
            report += f"  åºåˆ—å½¢çŠ¶: {self.sequences[0].shape}\n"
        
        if not self.df.empty:
            report += f"  æ•°æ®èµ·å§‹æ—¶é—´: {self.df['timestamp'].min()}\n"
            report += f"  æ•°æ®ç»“æŸæ—¶é—´: {self.df['timestamp'].max()}\n"
            report += f"  è‚¡ç¥¨æ•°é‡: {self.df['symbol'].nunique()}\n"
            report += "  æƒ…æ„Ÿåˆ†å¸ƒ:\n"
            report += f"    ä¸­æ€§: {np.sum(self.df['sentiment'] == 0)}\n"
            report += f"    ç§¯æ: {np.sum(self.df['sentiment'] > 0)}\n"
            report += f"    æ¶ˆæ: {np.sum(self.df['sentiment'] < 0)}"
        
        return report
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†ä¸­åºåˆ—çš„æ•°é‡"""
        return len(self.sequences)
    
    def __getitem__(self, index: int) -> tuple:
        """
        è·å–ç´¢å¼•å¯¹åº”çš„æ•°æ®æ ·æœ¬
        
        å‚æ•°:
            index (int): æ ·æœ¬ç´¢å¼•
            
        è¿”å›:
            tuple: (sequence_tensor, sentiment_target, metadata)
                   åŒ…å«ç‰¹å¾åºåˆ—å¼ é‡ã€æƒ…æ„Ÿç›®æ ‡å€¼å’Œå…¶ä»–å…ƒæ•°æ®
        """
        # 1. ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
        if index < 0 or index >= len(self.sequences):
            raise IndexError(f"ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ [0, {len(self.sequences)-1}]")
        
        # 2. è·å–åºåˆ—ç‰¹å¾å’Œç›®æ ‡
        sequence = self.sequences[index]
        sentiment = self.targets[index]
        
        # 3. è½¬æ¢ä¸ºå¼ é‡ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
        try:
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡® - ä½¿ç”¨ float32 ç²¾åº¦
            sequence_tensor = torch.tensor(sequence, dtype=torch.float32)
            sentiment_target = torch.tensor([sentiment], dtype=torch.float32)  # ä½¿å…¶å˜ä¸ºå¼ é‡å½¢å¼
        except Exception as e:
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶å°è¯•å¤„ç†
            self.logger.error(f"å¼ é‡è½¬æ¢é”™è¯¯ï¼ˆç´¢å¼• {index}ï¼‰: {e}")
            # å›é€€æ–¹æ¡ˆï¼šåˆ›å»ºå ä½ç¬¦åºåˆ—
            sequence_tensor = torch.zeros(
                (self.sequence_length, len(self.feature_names) if self.feature_names else 1),
                dtype=torch.float32
            )
            sentiment_target = torch.tensor([0.0], dtype=torch.float32)
        
        # 4. è·å–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œä½†æœ‰ä»·å€¼ï¼‰
        metadata = {
            'sequence_id': index,
            'features': self.feature_names,
            'sequence_length': self.sequence_length
        }
        
        return sequence_tensor, sentiment_target, metadata
# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
class TransformerModel(nn.Module):
    """Deepseek 1.5Bå…¼å®¹æ¨¡å‹ï¼ˆå®Œå…¨åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½åï¼‰"""
    def __init__(
        self,
        tokenizer_path: str = "",
        model_weights_path: Optional[str] = None,
        vocab_size: int = 51200,
        hidden_size: int = 1536,
        num_heads: int = 24,
        num_kv_heads: int = 4,
        hidden_size_mlp: int = 8960,
        num_layers: int = 30,
        output_size: int = 1,
        max_seq_len: int = 4096,
        device: Optional[torch.device] = None
    ):
        super().__init__()
        # ä¿å­˜æ¨¡å‹é…ç½®
        self.config = {
            "tokenizer_path": tokenizer_path,
            "model_weights_path": model_weights_path,
            "vocab_size": vocab_size,
            "hidden_size": hidden_size,
            "num_heads": num_heads,
            "num_kv_heads": num_kv_heads,
            "hidden_size_mlp": hidden_size_mlp,
            "num_layers": num_layers,
            "output_size": output_size,
            "max_seq_len": max_seq_len
        }
        
        # 1. TokenåµŒå…¥ï¼ˆç”¨äºæ–‡æœ¬è¾“å…¥ï¼‰
        self.embed_tokens = nn.Embedding(vocab_size, hidden_size)
        
        # æ–°å¢ï¼šè¾“å…¥æŠ•å½±å±‚ï¼ˆç”¨äºæµ®ç‚¹æ•°è¾“å…¥ï¼‰
        self.input_projection = nn.Sequential(
            nn.Linear(1, hidden_size),
            nn.GELU(),
            nn.LayerNorm(hidden_size)
        )
        
        # 2. æ—‹è½¬ä½ç½®ç¼–ç 
        self.rotary_emb = RotaryEmbedding(
            dim=hidden_size // num_heads, 
            max_seq_len=max_seq_len
        )
        
        # 3. Transformerå±‚å †å ï¼ˆä½¿ç”¨ä¿®æ­£åçš„å—ï¼‰
        self.layers = nn.ModuleList([
            DeepseekBlock(
                hidden_size=hidden_size,
                num_heads=num_heads,
                num_kv_heads=num_kv_heads,
                mlp_size=hidden_size_mlp,
            )
            for _ in range(num_layers)
        ])
        
        # 4. æœ€ç»ˆå½’ä¸€åŒ–å±‚ - åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½å: norm
        self.norm = DeepseekRMSNorm(hidden_size)
        
        # 5. è¾“å‡ºå±‚
        self.output = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, output_size)
        )
        
        # è®¾å¤‡ç®¡ç†
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)
        
        # è®°å½•æ¨¡å‹ä¿¡æ¯
        self.log_model_info()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if model_weights_path:
            self._smart_load_weights(model_weights_path)
    
    def log_model_info(self):
        """è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ”„ æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
        print(f"ğŸ”„ å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        print(f"ğŸ”„ é…ç½®ä¿¡æ¯: {self.config}")
    
    def _smart_load_weights(self, weights_path: str):
        """æ™ºèƒ½åŠ è½½æ¨¡å‹æƒé‡ï¼Œå¤„ç†å±‚çº§å‘½åå·®å¼‚"""
        try:
            # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
            state_dict = torch.load(weights_path, map_location=self.device)
            
            print(f"ğŸ”„ åŸå§‹æƒé‡åŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
            
            # åˆ›å»ºä¿®æ­£åçš„çŠ¶æ€å­—å…¸
            new_state_dict = {}
            for key, value in state_dict.items():
                # åº”ç”¨é‡å‘½åè§„åˆ™
                if key.startswith("layers."):
                    # 1. å¤„ç†å½’ä¸€åŒ–å±‚
                    if "input_layernorm" in key:
                        new_key = key.replace("input_layernorm", "layers.{}.input_layernorm")
                    elif "post_attention_layernorm" in key:
                        new_key = key.replace("post_attention_layernorm", "layers.{}.post_attention_layernorm")
                    
                    # 2. å¤„ç†æ³¨æ„åŠ›å±‚
                    elif "self_attn_q_proj" in key:
                        new_key = key.replace("self_attn_q_proj", "layers.{}.self_attn_q_proj")
                    elif "self_attn_k_proj" in key:
                        new_key = key.replace("self_attn_k_proj", "layers.{}.self_attn_k_proj")
                    elif "self_attn_v_proj" in key:
                        new_key = key.replace("self_attn_v_proj", "layers.{}.self_attn_v_proj")
                    elif "self_attn_o_proj" in key:
                        new_key = key.replace("self_attn_o_proj", "layers.{}.self_attn_o_proj")
                    
                    # 3. å¤„ç†MLPå±‚
                    elif "mlp_gate_proj" in key:
                        new_key = key.replace("mlp_gate_proj", "layers.{}.mlp_gate_proj")
                    elif "mlp_up_proj" in key:
                        new_key = key.replace("mlp_up_proj", "layers.{}.mlp_up_proj")
                    elif "mlp_down_proj" in key:
                        new_key = key.replace("mlp_down_proj", "layers.{}.mlp_down_proj")
                    
                    # åº”ç”¨å±‚çº§ç´¢å¼•
                    parts = key.split(".")
                    layer_idx = parts[1]  # ä» "layers.0..." è·å–ç´¢å¼•
                    new_key = new_key.format(layer_idx)
                else:
                    # ç‰¹æ®Šå¤„ç†éå±‚çº§çš„é”®
                    new_key = key
                
                new_state_dict[new_key] = value
            
            # åŠ è½½ä¿®æ­£åçš„æƒé‡
            result = self.load_state_dict(new_state_dict, strict=False)
            missing_keys = result.missing_keys
            unexpected_keys = result.unexpected_keys
            
            # æ£€æŸ¥æƒé‡åŠ è½½æƒ…å†µ
            print(f"âœ… æƒé‡åŠ è½½å®Œæˆ ({len(state_dict) - len(missing_keys)}/{len(state_dict)} åŒ¹é…)")
            
            # æ‰“å°è¯¦ç»†æŠ¥å‘Š
            self.print_weight_report(state_dict, missing_keys, unexpected_keys)
            
        except Exception as e:
            print(f"âŒ åŠ è½½æƒé‡å¤±è´¥: {str(e)}")
            raise
    
    def print_weight_report(self, state_dict, missing_keys, unexpected_keys):
        """æ‰“å°è¯¦ç»†çš„æƒé‡åŠ è½½æŠ¥å‘Š"""
        total = len(state_dict)
        loaded = total - len(missing_keys)
        missing_count = len(missing_keys)
        unused_count = len(unexpected_keys)
        
        print(f"\n{'='*60}")
        print("æƒé‡åŠ è½½è¯Šæ–­æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"âœ“ æƒé‡æ–‡ä»¶åŒ…å« {total} ä¸ªå‚æ•°")
        print(f"âœ“ æˆåŠŸåŠ è½½ {loaded} ä¸ªå‚æ•° ({loaded/total*100:.1f}%)")
        print(f"âš ï¸ ç¼ºå¤± {missing_count} ä¸ªå‚æ•°")
        print(f"âš ï¸ æœªä½¿ç”¨ {unused_count} ä¸ªå‚æ•°")
        
        if missing_keys:
            print("\nå…³é”®ç¼ºå¤±å‚æ•°:")
            for i, key in enumerate(missing_keys[:10]):
                print(f"  {i+1}. {key}")
            if len(missing_keys) > 10:
                print(f"  ... åŠ {len(missing_keys)-10} ä¸ªæ›´å¤š")
        
        if unexpected_keys:
            print("\næœªä½¿ç”¨çš„æƒé‡å‚æ•°:")
            for i, key in enumerate(unexpected_keys[:10]):
                print(f"  {i+1}. {key}")
            if len(unexpected_keys) > 10:
                print(f"  ... åŠ {len(unexpected_keys)-10} ä¸ªæ›´å¤š")
        
        print(f"{'='*60}\n")
    
    def forward(
        self, 
        input_data: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å®Œæ•´çš„å‰å‘ä¼ æ’­å®ç°
        
        å‚æ•°:
            input_data: è¾“å…¥å¼ é‡, å½¢çŠ¶ [batch_size, seq_len]
                        å¯ä»¥æ˜¯æ•´æ•°Token(ä½¿ç”¨åµŒå…¥å±‚)æˆ–æµ®ç‚¹æ•°(ä½¿ç”¨æŠ•å½±å±‚)
            position_ids: ä½ç½®IDå¼ é‡, å½¢çŠ¶ [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç , å½¢çŠ¶ [batch_size, seq_len]
            
        è¿”å›:
            torch.Tensor: è¾“å‡ºé¢„æµ‹ç»“æœ, å½¢çŠ¶ [batch_size, output_size]
        """
        # === 1. è¾“å…¥å½¢çŠ¶æ£€æŸ¥å’Œå¤„ç† ===
        # ç¡®ä¿è¾“å…¥æ˜¯2Då¼ é‡ [batch_size, seq_len]
        if input_data.dim() != 2:
            raise ValueError(f"è¾“å…¥å¼ é‡å¿…é¡»æ˜¯2D (batch_size, seq_len), ä½†æ˜¯å¾—åˆ° {input_data.dim()}D")
        
        batch_size, seq_len = input_data.size()
        
        # === 2. è¾“å…¥ç±»å‹å¤„ç† ===
        # ç¡®å®šè¾“å…¥ç±»å‹ï¼ˆæ•´æ•°Tokenæˆ–æµ®ç‚¹æ•°ï¼‰
        if input_data.dtype in [torch.int, torch.int8, torch.int16, torch.int32, torch.int64]:
            # æ•´æ•°è¾“å…¥ï¼šé€šè¿‡TokenåµŒå…¥å±‚
            inputs_embeds = self.embed_tokens(input_data)
        else:
            # æµ®ç‚¹æ•°è¾“å…¥ï¼šé€šè¿‡æŠ•å½±å±‚
            # æ·»åŠ ä¸€ä¸ªç‰¹å¾ç»´åº¦ [batch_size, seq_len, 1] ç„¶åæŠ•å½±åˆ°éšè—ç»´åº¦
            inputs_embeds = self.input_projection(input_data.unsqueeze(-1))
        
        # === 3. ä½ç½®IDå¤„ç† ===
        if position_ids is None:
            # åˆ›å»ºé»˜è®¤ä½ç½®ID: [0, 1, 2, ..., seq_len-1]
            position_ids = torch.arange(
                seq_len, 
                dtype=torch.long, 
                device=self.device
            ).unsqueeze(0).expand(batch_size, seq_len)
        
        # === 4. æ³¨æ„åŠ›æ©ç å¤„ç† ===
        if attention_mask is None:
            # åˆ›å»ºé»˜è®¤å…¨1æ©ç 
            attention_mask = torch.ones(
                (batch_size, seq_len),
                device=self.device,
                dtype=torch.long
            )
        
        # è½¬æ¢æ³¨æ„åŠ›æ©ç ä¸ºæµ®ç‚¹å‹å¹¶æ‰©å±•ç»´åº¦
        # åŸå§‹å½¢çŠ¶: [batch_size, seq_len]
        # æ‰©å±•å: [batch_size, 1, 1, seq_len] (ç”¨äºæ‰¹å¤„ç†å’Œå¤´ç»´åº¦)
        attn_mask = attention_mask.to(dtype=inputs_embeds.dtype)
        attn_mask = attn_mask.unsqueeze(1).unsqueeze(2)  # -> [batch_size, 1, 1, seq_len]
        
        # åº”ç”¨è´Ÿæ— ç©·å€¼å±è”½éœ€è¦å…³æ³¨çš„ä½ç½®
        # åˆ›å»ºå› æœæ©ç ï¼ˆé˜²æ­¢ä½ç½®æŸ¥çœ‹æœªæ¥ä½ç½®ï¼‰
        causal_mask = torch.tril(
            torch.ones((1, 1, seq_len, seq_len), device=self.device)
        ).to(inputs_embeds.dtype)
        
        # åˆå¹¶æ³¨æ„åŠ›æ©ç å’Œå› æœæ©ç 
        combined_mask = attn_mask * causal_mask
        
        # ä¸ºå±è”½ä½ç½®è®¾ç½®éå¸¸å¤§çš„è´Ÿå€¼ï¼ˆç”¨äºsoftmaxä¸­çš„å±è”½ï¼‰
        # å…¶ä¸­ combined_mask == 0 çš„ä½ç½®éœ€è¦è¢«å±è”½
        neg_inf = -1e10
        attn_mask = combined_mask * (1.0 - neg_inf) + (1.0 - combined_mask) * neg_inf
        
        # === 5. é€šè¿‡Transformerå±‚ ===
        hidden_states = inputs_embeds
        for layer in self.layers:
            hidden_states = layer(
                x=hidden_states,
                rotary_emb=self.rotary_emb,  # ä¼ é€’RotaryEmbeddingå®ä¾‹
                position_ids=position_ids,
                attention_mask=attn_mask
            )
        
        # === 6. æœ€ç»ˆå½’ä¸€åŒ– ===
        hidden_states = self.norm(hidden_states)
        
        # === 7. è·å–åºåˆ—æœ€åä½ç½®çš„è¡¨ç¤º ===
        # æ–¹æ³•1ï¼šä½¿ç”¨æ³¨æ„åŠ›æ©ç ç¡®å®šå®é™…æœ‰æ•ˆä½ç½®
        sequence_lengths = attention_mask.sum(dim=1) - 1  # æœ€åæœ‰æ•ˆä½ç½®ç´¢å¼•
        sequence_lengths = sequence_lengths.clamp(min=0)  # ç¡®ä¿éè´Ÿ
        
        # åˆ›å»ºæ‰¹å¤„ç†ç´¢å¼•
        batch_indices = torch.arange(batch_size, device=self.device)
        
        # æå–æ¯ä¸ªåºåˆ—çš„æœ€åæœ‰æ•ˆéšè—çŠ¶æ€
        last_hidden_states = hidden_states[batch_indices, sequence_lengths, :]
        
        # === 8. è¾“å‡ºé¢„æµ‹ ===
        logits = self.output(last_hidden_states)
        
        return logits
    
class RotaryEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç å®ç°ï¼ˆä¿æŒä¸å˜ï¼‰"""
    def __init__(self, dim: int, max_seq_len: int = 2048):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        t = torch.arange(max_seq_len).type_as(inv_freq)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :])
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :])
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return apply_rotary_pos_emb(x, self.cos_cached, self.sin_cached)
def apply_rotary_pos_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:
    """åº”ç”¨æ—‹è½¬ä½ç½®åµŒå…¥åˆ°è¾“å…¥å¼ é‡"""
    seq_len = x.size(-2)
    cos = cos[:, :, :seq_len, :].to(x.device)
    sin = sin[:, :, :seq_len, :].to(x.device)
    x_rot = x[..., : x.shape[-1] // 2]
    x_pass = x[..., x.shape[-1] // 2 :]
    x_rot = (x_rot * cos) + (rotate_half(x_rot) * sin)
    return torch.cat((x_rot, x_pass), dim=-1)

def rotate_half(x: torch.Tensor) -> torch.Tensor:
    """å°†è¾“å…¥å¼ é‡çš„åä¸€åŠæ—‹è½¬"""
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)
class DeepSeekCompatibleTransformerBlock(nn.Module):
    def __init__(self, d_model, n_head, num_kv_heads, head_dim, dim_feedforward, dropout):
        super().__init__()
        # å…³é”®ï¼šä½¿ç”¨ç»Ÿä¸€ç»´åº¦å®šä¹‰
        self.self_attn = DeepseekAttention(
            embed_dim=d_model,
            num_heads=n_head,
            num_kv_heads=num_kv_heads,
            head_dim=head_dim,
            rope_theta=10000.0
        )
        
        # MLPä½¿ç”¨dim_feedforward=8960
        self.mlp = SwishGLU(
            input_size=d_model,
            hidden_size=dim_feedforward  # è®¾ä¸º8960
        )
        
        self.input_norm = DeepseekRMSNorm(d_model, eps=1e-5)
        self.output_norm = DeepseekRMSNorm(d_model, eps=1e-5)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Pre-LNç»“æ„
        # ç¬¬ä¸€ä¸ªå½’ä¸€åŒ–
        x_normalized = self.input_norm(x)
        
        # è‡ªæ³¨æ„åŠ›
        attn_output = self.self_attn(x_normalized)
        attn_output = self.dropout(attn_output)
        x = x + attn_output  # æ®‹å·®è¿æ¥
        
        # MLPå‰çš„ç¬¬äºŒä¸ªå½’ä¸€åŒ–
        x_normalized = self.output_norm(x)
        
        # å‰é¦ˆç½‘ç»œ
        mlp_output = self.mlp(x_normalized)
        mlp_output = self.dropout(mlp_output)
        return x + mlp_output  # æ®‹å·®è¿æ¥
class DeepseekRMSNorm(nn.Module):
    """Deepseekä½¿ç”¨çš„RMSNormï¼ˆä¿æŒä¸å˜ï¼‰"""
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)
        return self.weight * hidden_states.to(input_dtype)
class SwishGLU(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        # å…³é”®ï¼šä¿®æ­£ç»´åº¦ä¸º8960
        self.gate_proj = nn.Linear(input_size, hidden_size, bias=False)
        self.up_proj = nn.Linear(input_size, hidden_size, bias=False)
        self.down_proj = nn.Linear(hidden_size, input_size, bias=False)
        self.act = nn.SiLU()
        
    def forward(self, x):
        # SwishGLUæ¿€æ´»
        gate = self.act(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)
class DeepseekAttention(nn.Module):
    """Deepseek 1.5B çš„è‡ªæ³¨æ„åŠ›æ¨¡å— (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)"""
    def __init__(self, hidden_size, num_heads, num_kv_heads=None):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        
        # å¤„ç†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads
        self.num_kv_repeats = num_heads // self.num_kv_heads
        
        # ç¡®ä¿éšè—å¤§å°å¯è¢«å¤´æ•°æ•´é™¤
        if hidden_size % num_heads != 0:
            raise ValueError(f"hidden_size ({hidden_size}) å¿…é¡»èƒ½è¢« num_heads ({num_heads}) æ•´é™¤")
        
        self.head_dim = hidden_size // num_heads
        
        # çº¿æ€§æŠ•å½±å±‚
        self.q_proj = nn.Linear(hidden_size, num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # ç”¨äºæ³¨æ„åŠ›æ©ç çš„å¸¸é‡
        self.register_buffer("mask_bias", torch.tril(torch.ones(1024, 1024)))
    
    def group_kv_repeat(self, kv):
        """ä¸ºåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›é‡å¤é”®å€¼å¯¹"""
        return (
            kv[0].repeat_interleave(self.num_kv_repeats, dim=2),
            kv[1].repeat_interleave(self.num_kv_repeats, dim=2)
        )
    
    def forward(
        self, 
        x: torch.Tensor,
        rotary_emb: RotaryEmbedding,
        position_ids: torch.Tensor,
        attention_mask: torch.Tensor = None
    ) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        # æŠ•å½±æŸ¥è¯¢ã€é”®å’Œå€¼
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # åˆ†å‰²å¤šå¤´
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q = rotary_emb(q, position_ids)
        k = rotary_emb(k, position_ids)
        
        # å¤„ç†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (é‡å¤é”®å€¼å¯¹)
        if self.num_kv_repeats > 1:
            k, v = self.group_kv_repeat((k, v))
        
        # è½¬ç½®ä»¥ä¾¿çŸ©é˜µä¹˜æ³• (æ‰¹é‡å¤§å°, å¤´æ•°, åºåˆ—é•¿åº¦, å¤´ç»´åº¦)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç  (å› æœæ©ç æˆ–è‡ªå®šä¹‰æ©ç )
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask.unsqueeze(1)
        else:
            # åº”ç”¨å› æœæ©ç 
            causal_mask = self.mask_bias[:seq_len, :seq_len].view(1, 1, seq_len, seq_len)
            attn_scores = torch.where(
                causal_mask > 0, 
                attn_scores, 
                torch.tensor(-1e9, dtype=attn_scores.dtype, device=attn_scores.device)
            )
        
        # Softmax å½’ä¸€åŒ–
        attn_probs = F.softmax(attn_scores, dim=-1)
        
        # æ³¨æ„åŠ›è¾“å‡º
        attn_output = torch.matmul(attn_probs, v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        
        # æœ€ç»ˆæŠ•å½±
        return self.o_proj(attn_output)

class DeepseekMLP(nn.Module):
    """Deepseek 1.5B çš„MLPæ¨¡å— (SwiGLUæ¿€æ´»)"""
    def __init__(self, hidden_size, mlp_size):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, mlp_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, mlp_size, bias=False)
        self.down_proj = nn.Linear(mlp_size, hidden_size, bias=False)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­ä½¿ç”¨SwiGLUæ¿€æ´»"""
        gate = F.silu(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)
class DeepseekBlock(nn.Module):
    """å®Œå…¨åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½åçš„Transformerå—"""
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        mlp_size: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = hidden_size // num_heads
        
        # æ£€æŸ¥å¤´æ•°å’Œå¤´ç»´åº¦æ˜¯å¦æœ‰æ•ˆ
        assert hidden_size % num_heads == 0, "hidden_sizeå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        assert self.head_dim * num_heads == hidden_size, "å¤´ç»´åº¦è®¡ç®—é”™è¯¯"
        assert num_kv_heads > 0 and num_kv_heads <= num_heads, "num_kv_headså¿…é¡»åœ¨1å’Œnum_headsä¹‹é—´"
        
        # 1. è¾“å…¥å½’ä¸€åŒ– - åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½å: input_layernorm
        self.input_layernorm = DeepseekRMSNorm(hidden_size)
        
        # 2. æ³¨æ„åŠ›æœºåˆ¶ - å®Œå…¨åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½å
        self.self_attn_q_proj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.self_attn_k_proj = nn.Linear(
            hidden_size, 
            num_kv_heads * self.head_dim,
            bias=True
        )
        self.self_attn_v_proj = nn.Linear(
            hidden_size, 
            num_kv_heads * self.head_dim,
            bias=True
        )
        self.self_attn_o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # 3. MLPè¾“å…¥å½’ä¸€åŒ– - åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½å: post_attention_layernorm
        self.post_attention_layernorm = DeepseekRMSNorm(hidden_size)
        
        # 4. MLPç½‘ç»œ - åŒ¹é…é¢„è®­ç»ƒæƒé‡å‘½å
        self.mlp_gate_proj = nn.Linear(hidden_size, mlp_size, bias=False)
        self.mlp_up_proj = nn.Linear(hidden_size, mlp_size, bias=False)
        self.mlp_down_proj = nn.Linear(mlp_size, hidden_size, bias=False)
        
        # 5. æ³¨æ„åŠ›ç¼©æ”¾å› å­
        self.scale_factor = 1.0 / math.sqrt(self.head_dim)
        
        # 6. æ³¨æ„åŠ›ä¸¢å¼ƒ
        self.attn_dropout = nn.Dropout(dropout)
    
    def forward(
        self, 
        x: torch.Tensor, 
        rotary_emb: RotaryEmbedding,
        position_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        # ä¿å­˜æ®‹å·®è¿æ¥
        residual = x
        
        # === 1. æ³¨æ„åŠ›å½’ä¸€åŒ– ===
        x = self.input_layernorm(x)
        
        # === 2. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ ===
        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±
        q = self.self_attn_q_proj(x)  # [batch, seq_len, hidden_size]
        k = self.self_attn_k_proj(x)  # [batch, seq_len, num_kv_heads * head_dim]
        v = self.self_attn_v_proj(x)  # [batch, seq_len, num_kv_heads * head_dim]
        
        # é‡å¡‘æŸ¥è¯¢å¼ é‡ [batch, seq_len, num_heads, head_dim]
        q = q.view(q.size(0), q.size(1), self.num_heads, self.head_dim)
        
        # é‡å¡‘é”®å¼ é‡ [batch, seq_len, num_kv_heads, head_dim]
        k = k.view(k.size(0), k.size(1), self.num_kv_heads, self.head_dim)
        
        # é‡å¡‘å€¼å¼ é‡ [batch, seq_len, num_kv_heads, head_dim]
        v = v.view(v.size(0), v.size(1), self.num_kv_heads, self.head_dim)
        
        # === 3. åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç  ===
        q = rotary_emb(q)
        k = rotary_emb(k)
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†æŸ¥è¯¢å¤´å’Œé”®å€¼å¤´ä¸åŒ¹é…é—®é¢˜
        if self.num_kv_heads != self.num_heads:
            # è®¡ç®—é‡å¤å› å­
            repeat_factor = self.num_heads // self.num_kv_heads
            
            # é‡å¤é”®å€¼å¤´ä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°
            k = k.repeat_interleave(repeat_factor, dim=2)  # [batch, seq_len, num_heads, head_dim]
            v = v.repeat_interleave(repeat_factor, dim=2)  # [batch, seq_len, num_heads, head_dim]
        
        # === 4. é‡å¡‘å¼ é‡ç”¨äºæ³¨æ„åŠ›è®¡ç®— ===
        # è½¬ç½®å¼ é‡ç»´åº¦ [batch, heads, seq_len, head_dim]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # === 5. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° ===
        # ç‚¹ç§¯æ³¨æ„åŠ›: Q @ K^T
        attn_scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale_factor
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            if attention_mask.dim() == 2:
                attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            elif attention_mask.dim() == 3:
                attention_mask = attention_mask.unsqueeze(1)
            
            # åº”ç”¨æ©ç ï¼šå°†å±è”½ä½ç½®è®¾ä¸ºå¤§è´Ÿæ•°
            attn_scores = attn_scores.masked_fill(attention_mask == 0, -1e10)
        
        # è®¡ç®—æ³¨æ„åŠ›æ¦‚ç‡
        attn_probs = torch.softmax(attn_scores, dim=-1)
        attn_probs = self.attn_dropout(attn_probs)
        
        # === 6. åº”ç”¨æ³¨æ„åŠ›æƒé‡ ===
        attn_output = torch.matmul(attn_probs, v)
        
        # === 7. é‡å¡‘æ³¨æ„åŠ›è¾“å‡º ===
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.contiguous().view(
            attn_output.size(0), 
            attn_output.size(1), 
            self.hidden_size
        )
        
        # === 8. æ³¨æ„åŠ›è¾“å‡ºæŠ•å½± ===
        attn_output = self.self_attn_o_proj(attn_output)
        
        # === 9. æ·»åŠ æ®‹å·®è¿æ¥ ===
        x = residual + attn_output
        
        # === 10. MLPéƒ¨åˆ† ===
        residual = x
        x = self.post_attention_layernorm(x)
        
        # MLPå±‚è®¡ç®— (SwiGLUæ¿€æ´»)
        gate = torch.sigmoid(self.mlp_gate_proj(x))
        up = self.mlp_up_proj(x)
        mlp_output = self.mlp_down_proj(gate * up)
        
        # æ®‹å·®è¿æ¥
        x = residual + mlp_output
        
        return x
class DeepseekLayer(nn.Module):
    """Deepseek Transformeræ¨¡å—çš„å®Œæ•´å®ç°"""
    def __init__(self, hidden_size, num_heads, num_kv_heads, hidden_size_mlp):
        super().__init__()
        
        # æ³¨æ„åŠ›å±‚
        self.input_norm = DeepseekRMSNorm(hidden_size)
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # æ—‹è½¬ä½ç½®ç¼–ç 
        self.rotary_emb = RotaryEmbedding(hidden_size // num_heads)
        
        # MLPå±‚
        self.output_norm = DeepseekRMSNorm(hidden_size)
        self.gate_proj = nn.Linear(hidden_size, hidden_size_mlp, bias=False)
        self.up_proj = nn.Linear(hidden_size, hidden_size_mlp, bias=False)
        self.down_proj = nn.Linear(hidden_size_mlp, hidden_size, bias=False)
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.num_kv_heads = num_kv_heads
        self.kv_dim = self.head_dim * num_kv_heads
        
        # ç¡®ä¿å‚æ•°è®¾ç½®æœ‰æ•ˆ
        if hidden_size % num_heads != 0:
            raise ValueError(
                f"hidden_size ({hidden_size}) å¿…é¡»èƒ½è¢« num_heads ({num_heads}) æ•´é™¤"
            )

    def forward(self, x, attention_mask=None, position_ids=None):
        # è¾“å…¥å½’ä¸€åŒ–
        residual = x
        x = self.input_norm(x)
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        batch_size, seq_len, _ = x.size()
        
        # æŠ•å½±æŸ¥è¯¢ã€é”®å’Œå€¼
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # åˆ†å‰²å¤šå¤´
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q = self.rotary_emb(q, position_ids)
        k = self.rotary_emb(k, position_ids)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = torch.einsum("bqhd,bkhd->bhqk", q, k) / (self.head_dim ** 0.5)
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask.unsqueeze(1)
        
        # Softmax å½’ä¸€åŒ–
        attn_probs = torch.softmax(attn_scores, dim=-1)
        
        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
        attn_output = torch.einsum("bhqk,bkhd->bqhd", attn_probs, v)
        attn_output = attn_output.contiguous().view(batch_size, seq_len, -1)
        
        # æœ€ç»ˆæŠ•å½±
        attn_output = self.o_proj(attn_output)
        
        # æ®‹å·®è¿æ¥
        x = residual + attn_output
        
        # MLPéƒ¨åˆ†
        residual = x
        x = self.output_norm(x)
        
        # MLPå±‚è®¡ç®—
        gate = torch.sigmoid(self.gate_proj(x))
        up = self.up_proj(x)
        mlp_output = self.down_proj(gate * up)
        
        # æ®‹å·®è¿æ¥
        x = residual + mlp_output
        
        return x
def compute_reward(model2_result, targets,lookback=5):
    """æ¨¡æ‹Ÿå®é™…äº¤æ˜“æ•ˆæœçš„å¥–åŠ±"""
    # è·å–æ¨¡å‹é¢„æµ‹åºåˆ—
    predictions = model2_result[:, :lookback]
    
    # æ„å»ºæ¨¡æ‹ŸæŒä»“
    portfolio = torch.zeros_like(targets)
    total_return = torch.zeros_like(targets)
    
    # åŸºäºè¿ç»­é¢„æµ‹çš„ä»“ä½å˜åŒ–
    for t in range(1, lookback):
        # åŸºäºé¢„æµ‹ä¿¡å·è°ƒæ•´ä»“ä½
        position_change = predictions[:, t] - predictions[:, t-1]
        portfolio += position_change
        
        # è®¡ç®—æ¯æ—¥æ”¶ç›Š
        daily_return = portfolio * targets[:, t]
        total_return += daily_return
        
        # é£é™©ç®¡ç†: å¼ºåˆ¶å¹³ä»“è§„åˆ™
        portfolio = torch.where(torch.abs(portfolio) > 2, 
                                torch.clamp(portfolio, -1, 1), 
                                portfolio)
    
    # æœ€ç»ˆç»„åˆä»·å€¼ä½œä¸ºå¥–åŠ±
    final_value = 1 + total_return
    reward = final_value - 1  # ç»„åˆæ”¶ç›Šä½œä¸ºå¥–åŠ±
    return reward

def get_actual_price(stock_code, date):
    # æ–‡ä»¶è·¯å¾„ç¤ºä¾‹
    csv_file = os.path.join('data', 'stock_prices', f'{stock_code}.csv')
    df = pd.read_csv(csv_file)

    # å‡è®¾csvæœ‰åˆ—ï¼š'date', 'close'
    row = df[df['date'] == date]
    if row.empty:
        print(f"æœªæ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„è‚¡ä»·ï¼š{date}")
        return None
    return float(row['close'].values[0])

def main(config_path: str):
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config_loader = ConfigLoader(config_path)
    config = config_loader.config
    config['training']['stability'] = config.get('stability', {
        'max_nan_allowed': 0,
        'output_clip_range': [-5.0, 5.0],
        'max_grad_norm': 1.0,
        'gradient_clip_value': 10000.0,
        'add_output_noise': True,
        'noise_std': 0.01,
        'backup_interval': 5,
        'recovery_strategy': 'backoff',
        'lr_backoff_factor': 0.5
    })
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = setup_logger(config['paths']['log_file'])
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œè®­ç»ƒè„šæœ¬")
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•
    create_output_directories(config)
    logger.info("ğŸ“ æ‰€æœ‰è¾“å‡ºç›®å½•å·²åˆ›å»º")
    
    # è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è·¯å¾„
    train_dir = config['paths'].get('train_data_path')
    val_dir = config['paths'].get('val_data_path', None)  
    test_dir = config['paths'].get('test_data_path')
    
    if not train_dir or not os.path.exists(train_dir):
        raise RuntimeError(f"è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–æœªå®šä¹‰ï¼š{train_dir}")
    if not test_dir or not os.path.exists(test_dir):
        raise RuntimeError(f"æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–æœªå®šä¹‰ï¼š{test_dir}")
    
    seq_length = config['training'].get('sequence_length', 30)

    logger.info(f"ğŸ“Š æ•°æ®é…ç½®: åºåˆ—é•¿åº¦={seq_length}")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    logger.info(f"ğŸ” åŠ è½½è®­ç»ƒæ•°æ®é›†: {train_dir}")
    train_dataset = CSVDataset(
        data_path=train_dir,
        sequence_length=seq_length
    )
    device = torch.device("cuda:0")
    # è·å–ç‰¹å¾åç§°
    feature_names = train_dataset.feature_names
    logger.info(f"ğŸ”¤ ç‰¹å¾åç§°: {feature_names}")

    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    train_dataset.get_data_report()
    
    # å¤„ç†éªŒè¯æ•°æ®é›†
    val_dataset = None
    if val_dir and os.path.exists(val_dir):
        logger.info(f"ğŸ” åŠ è½½éªŒè¯æ•°æ®é›†: {val_dir}")
        val_dataset = CSVDataset(
            data_path=val_dir,
            sequence_length=seq_length
        )
        val_dataset.get_data_report()
    try:
        hidden_size = config['env']['hidden_size']
        num_layers = config['env']['num_layers']
        num_heads = config['env']['num_heads']
        model_path = config['env']['prediction_model_path']
        sample, _, _ = train_dataset[0]
        input_dim=sample.shape[-1]
    except KeyError as e:
        raise RuntimeError(f"é…ç½®ç¼ºå°‘å­—æ®µ: {e}")
    # æ ¹æ®é…ç½®è·¯å¾„åŠ è½½æ¨¡å‹
    safety = SafetyModule()
    if model_path:
        model2 = DeepSeekPredictor(device=device)
        safety = safety.to(device)
        print("åŠ è½½çš„model2ç±»å‹ï¼š", type(model2))
        assert model2 is not None, "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel2ä¸ºNoneï¼"
    else:
        print("æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚")    
    # åˆ›å»ºæ¨¡å‹
    try:
        # ç›´æ¥ä»è®­ç»ƒæ•°æ®é›†è·å–è¾“å…¥ç»´åº¦
        sample, _, _ = train_dataset[0]
        input_dim = sample.shape[-1]  # ç‰¹å¾ç»´åº¦åœ¨æœ€åä¸€ä½
        logger.info(f"ğŸ§  æ¨¡å‹è¾“å…¥ç»´åº¦: {input_dim}")
    except Exception as e:
        logger.error(f"æ— æ³•ç¡®å®šè¾“å…¥ç»´åº¦: {e}")
        raise
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    pretrained_path = config['paths'].get('output_model')
    if pretrained_path and os.path.exists(pretrained_path):
        logger.info(f"â¬‡ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path}")
        try:
            model = TransformerModel(
                tokenizer_path= "/home/liyakun/LLaMA-Factory-main/deepseek1.5B",       # âœ”ï¸ æ–°çš„åˆ†è¯å™¨å‚æ•°
                model_weights_path=pretrained_path,   # âœ”ï¸ æ–°çš„æƒé‡è·¯å¾„å‚æ•°
                vocab_size=151936,  # ä¿®æ­£ï¼šDeepseek 1.5Bçš„å®é™…è¯æ±‡è¡¨å¤§å°æ˜¯51200ï¼Œä¸æ˜¯32000
                hidden_size=1536,  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åï¼ˆä¸æ˜¯d_modelæˆ–2048ï¼‰
                num_heads=24,      # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åï¼ˆä¸æ˜¯n_headæˆ–16ï¼‰
                num_kv_heads=4,    # å…³é”®ï¼šå›ºå®šä¸º4
                hidden_size_mlp=8960, # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åï¼ˆä¸æ˜¯dim_feedforwardæˆ–8192ï¼‰
                num_layers=30,      # 1.5Bæ¨¡å‹çš„å®é™…å±‚æ•°æ˜¯30
                output_size=1
            ).to(device)
            logger.info(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"ğŸ”„ æ¨¡å‹æ¶æ„:\n{model}")
        except Exception as e:
                logger.error(f"âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {str(e)}")
                exit(1)
    else:
        logger.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„æˆ–è·¯å¾„æ— æ•ˆ")

    # 2. åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    weights_path = pretrained_path 
    if os.path.exists(weights_path):
        logger.info(f"â¬‡ï¸ åŠ è½½é¢„è®­ç»ƒæƒé‡: {weights_path}")
        try:
            # åŠ è½½ä¿å­˜çš„çŠ¶æ€å­—å…¸
            checkpoint = torch.load(weights_path, map_location=device)
            
            # æ ¹æ®ä¿å­˜çš„æ–¹å¼å¤„ç†
            state_dict = {}
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
                
            # å¤„ç†ä¸åŒ¹é…çš„é”®åï¼ˆå¸¸è§è§£å†³æ–¹æ¡ˆï¼‰
            new_state_dict = {}
            for k, v in state_dict.items():
                # å»æ‰å¯èƒ½çš„"module."å‰ç¼€ï¼ˆå¤šGPUè®­ç»ƒä¿å­˜çš„æƒé‡ï¼‰
                name = k.replace("module.", "")
                new_state_dict[name] = v
                
            # åŠ è½½æƒé‡
            model.load_state_dict(new_state_dict, strict=False)
            logger.info("âœ… æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {str(e)}")
            exit(1)
    else:
        logger.error(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
        exit(1)
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config['training'].get('batch_size', 32)
    logger.info(f"ğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨ (batch_size={batch_size})")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=min(4, os.cpu_count())  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=min(2, os.cpu_count())  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
        )
        logger.info(f"âœ… è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    else:
        logger.warning("âš ï¸ æœªåˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨")
    
    # å‡†å¤‡ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(
        model.parameters(), 
        lr=float(config['training']['learning_rate']),
        eps=1e-6  # é˜²æ­¢é™¤é›¶
    )

    max_grad_norm = config['training'].get('max_grad_norm', 1.0) 
    logger.info("â„¹ï¸ ä½¿ç”¨SmoothL1Lossä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’")
    # åˆ›å»ºå®‰å…¨æ§åˆ¶å™¨
    safety = SafetyController(model, optimizer, config, logger)

# ä½¿ç”¨æ–°æŸå¤±å‡½æ•°
    loss_fn = RLoss(
    supervised_criterion=SafeSmoothL1Loss(beta=1.0).to(device),
    base_loss_weight=config['training']['base_loss_weight'],
).to(device)
    grad_monitor = GradientMonitor(model)
    grad_monitor.attach()
    torch.autograd.set_detect_anomaly(True)
    for epoch in range(config['training']['epochs']):
        new_weight = get_rl_weight(epoch)
        loss_fn.base_loss_weight = new_weight  # åŠ¨æ€è°ƒæ•´æƒé‡
        safety.create_backup(epoch)
        model.train()
        epoch_loss = 0.0
        nan_batch_count = 0
        valid_batch_count = 0
            # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶è¾“å‡º
        print(f"\nã€è°ƒè¯•ã€‘å¼€å§‹ç¬¬ {epoch+1} è½®è®­ç»ƒ")
        for batch_idx, (inputs, targets, dates) in enumerate(train_loader):
            print(f"\nã€è°ƒè¯•ã€‘ç¬¬ {batch_idx+1} æ‰¹æ¬¡")
            # print("batch_data å†…å®¹ï¼š", batch_data)123456
            print("åŸå§‹inputs.shape:", inputs.shape)
            print("åŸå§‹targets.shape:", targets.shape)
            # è½¬ç§»åˆ°è®¾å¤‡
            inputs = inputs.to(device)
            targets = targets.to(device)  
            print("inputsè®¾å¤‡:", inputs.device)
            print("targetsè®¾å¤‡:", targets.device)   
           
            # 1. è¾“å…¥æ•°æ®æ£€æŸ¥
            if not safety.check_inputs(inputs, targets):
                nan_batch_count += 1
                continue
                           
            # 2. åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œå‰å‘ä¼ æ’­
            loss = None
            with safety.safe_forward_context():
                outputs = model(inputs)
                print("outputså½¢çŠ¶")
                print(outputs)
                print("outputsçš„å½¢çŠ¶ï¼š", outputs.shape)
                print(torch.min(outputs).item(), torch.max(outputs).item())
                
                if outputs is None:
                    print("æ¨¡å‹è¾“å‡ºä¸ºNoneï¼Œè·³è¿‡ä¿æŠ¤å’Œlossè®¡ç®—")
                    # å¯ä»¥å®šä¹‰è·³å‡ºå½“å‰batchæˆ–ç»ˆæ­¢
                    continue
                if model2 is None:
                    raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œä¸èƒ½è¿›è¡Œé¢„æµ‹ï¼")
                model2_result = DeepSeekPredictor(model2, outputs)
                print("æ¨¡å‹äºŒæ¨ç†ç»“æœï¼š", model2_result)
                # è®¡ç®—å¥–åŠ±ï¼ˆç¤ºä¾‹ï¼‰
                direction_match = (torch.sign(model2_result) == torch.sign(targets)).float()
                magnitude_error = torch.abs(model2_result - targets)
                accuracy_reward = direction_match * 0.8
                
                # å¹…åº¦è¯¯å·®å¥–åŠ±ï¼ˆè¯¯å·®è¶Šå°å¥–åŠ±è¶Šé«˜ï¼‰
                error_reward = torch.exp(-2 * magnitude_error) * 0.2
                reward = (accuracy_reward + error_reward)
                print(f"å¹³å‡æ–¹å‘åŒ¹é…ç‡: {direction_match.mean().item():.4f}")
                print(f"å¹³å‡è¯¯å·®å¥–åŠ±: {error_reward.mean().item():.4f}")
                print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {reward.mean().item():.4f}")
                protected_logits = safety.protect_outputs(outputs)
                print(f"logitsä¿æŠ¤å: min={protected_logits.min().item():.4f}, max={protected_logits.max().item():.4f}")
                protected_outputs = {'logits': protected_logits}
            # 6. ä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆä¿æŒåŸæ¥å£ä¸å˜ï¼‰
                loss_dict = loss_fn(
                    model_outputs=protected_outputs,
                    targets=targets,
                    reward=reward,
                )
                
                loss = loss_dict["total_loss"]
                print(f"æ€»æŸå¤±: {loss.item():.4f} | "
                    f"ç›‘ç£æŸå¤±: {loss_dict['supervised_loss'].item():.4f} | "
                    f"ç­–ç•¥æŸå¤±: {loss_dict['policy_loss'].item():.4f} | "
                    f"åŒ¹é…ç‡: {loss_dict['match_rate'].item():.4f}")
                
            
            # åå‘ä¼ æ’­
            print("logits.requires_grad:", protected_logits.requires_grad)
            print("loss.requires_grad:", loss.requires_grad)
            optimizer.zero_grad()
            loss.backward()
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_max = param.grad.abs().max().item()
                    print(f"å±‚ {name} æ¢¯åº¦æœ€å¤§å€¼: {grad_max}")
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
            safety.check_gradients()
            optimizer.step()
            batch_reward = reward.mean().item()
            epoch_loss += loss.item()
            valid_batch_count += 1
            # æ¯10æ‰¹æ¬¡æŠ¥å‘Šæ¢¯åº¦çŠ¶æ€
            if batch_idx % 10 == 0:
                grad_monitor.report(batch_idx, epoch)
        if val_loader:
            val_loss = 0.0
            model.eval()  # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
                for val_inputs, val_targets, val_dates in val_loader:
                    # å°†æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
                    val_inputs = val_inputs.to(device)
                    val_targets = val_targets.to(device)
                    
                    # éªŒè¯å‰å‘ä¼ æ’­
                    val_outputs = model(val_inputs)
                    
                    # è®¡ç®—éªŒè¯æŸå¤±ï¼ˆä»…åŸºç¡€ç›‘ç£æŸå¤±ï¼‰
                    val_loss_dict = loss_fn(
                        model_outputs={'logits': val_outputs},
                        targets=val_targets,
                        reward=None  # éªŒè¯æ—¶ä¸ä½¿ç”¨å¥–åŠ±æ¨¡å‹
                    )
                    
                    # ç´¯è®¡éªŒè¯æŸå¤±
                    val_loss += val_loss_dict["supervised_loss"].item()
            
            # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
            avg_val_loss = val_loss / len(val_loader)
            print(f"Epoch {epoch+1} éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            # ===== 3. æ¨¡å‹ä¿å­˜å†³ç­– =====
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                model_save_path = Path(config['paths']['output_model'])
                torch.save(model.state_dict(), model_save_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {model_save_path} (éªŒè¯æŸå¤±: {avg_val_loss:.4f})")
            else:
                print(f"å½“å‰éªŒè¯æŸå¤± {avg_val_loss:.4f} æ¯”æœ€ä½³ {best_val_loss:.4f} å·®ï¼Œä¸ä¿å­˜æ¨¡å‹")
        # ç§»é™¤æ¢¯åº¦ç›‘æ§é’©å­
        grad_monitor.detach()
        # æŠ¥å‘Šå½“å‰epochçŠ¶æ€
        if valid_batch_count > 0:
            avg_loss = epoch_loss / valid_batch_count
            logger.info(f"ğŸ Epoch {epoch+1} | å¹³å‡æŸå¤±: {avg_loss:.6f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")
        else:
            logger.error(f"â›” Epoch {epoch+1} æ²¡æœ‰æœ‰æ•ˆè®­ç»ƒæ‰¹æ¬¡ï¼Œå°è¯•æ¢å¤...")
            recovered_epoch = safety.recover(epoch)
            logger.warning(f"æ¢å¤è‡³epoch {recovered_epoch}")
            epoch = recovered_epoch  # é‡ç½®epochè®¡æ•°å™¨

    model_save_path = Path(config['paths']['output_model']) 
    torch.save(model.state_dict(), model_save_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
    # æµ‹è¯•å’Œè¯„ä¼°é˜¶æ®µ
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ...")

    # 1. æ•°æ®åŠ è½½å’Œå‡†å¤‡
    if not test_dir or not test_dir.get('test'):
        logger.info("âš ï¸ æ²¡æœ‰é…ç½®æµ‹è¯•æ•°æ®é›†ï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
    else:
        # å®‰å…¨åŠ è½½æµ‹è¯•æ•°æ®é›†
        test_datasets = []
        for test_file in test_dir['test']:
            logger.info(f"ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_file}")
            if not os.path.exists(test_file):
                logger.warning(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {test_file}")
                continue
                
            try:
                test_dataset = CSVDataset(
                    data_path=test_file,
                    sequence_length=config['training'].get('sequence_length', 30),
                )
                test_datasets.append(test_dataset)
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {test_file}, é”™è¯¯: {str(e)}")
        
        # å¤„ç†æ²¡æœ‰æœ‰æ•ˆæµ‹è¯•æ•°æ®çš„æƒ…å†µ
        if not test_datasets:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
        else:
            # ç»„åˆæµ‹è¯•æ•°æ®é›†
            if len(test_datasets) > 1:
                final_test_dataset = torch.utils.data.ConcatDataset(test_datasets)
                logger.info(f"âœ… åˆå¹¶äº† {len(test_datasets)} ä¸ªæµ‹è¯•æ•°æ®é›†ï¼Œæ€»æ ·æœ¬æ•°: {len(final_test_dataset)}")
            else:
                final_test_dataset = test_datasets[0]
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            test_loader = DataLoader(
                final_test_dataset, 
                batch_size=config['training']['batch_size'], 
                shuffle=False,
                num_workers=4
            )
            
            # 2. æµ‹è¯•è¿‡ç¨‹
            model.eval()
            all_predictions = []
            all_targets = []
            all_dates = []
            all_rewards = []
            all_model2_preds = []  # å­˜å‚¨model2çš„é¢„æµ‹ç»“æœ
            all_feature_sequences = []  # å­˜å‚¨è¾“å…¥ç‰¹å¾åºåˆ—
            all_processed_features = []  # å­˜å‚¨å¤„ç†åçš„ç‰¹å¾
            
            with torch.no_grad():
                for batch_idx, batch_data in enumerate(test_loader):
                    # è§£åŒ…æ•°æ® (æ ¹æ®CSVDatasetçš„__getitem__æ–¹æ³•)
                    feature_sequences = batch_data[0]
                    processed_features = batch_data[1]
                    targets = batch_data[2]
                    dates = batch_data[3]
                    
                    # è½¬ç§»åˆ°è®¾å¤‡
                    processed_features = processed_features.to(device)
                    targets = targets.to(device)
                    
                    # æ¨¡å‹é¢„æµ‹ (ä½¿ç”¨å¤„ç†åçš„ç‰¹å¾)
                    outputs = model(processed_features)
                    model2_preds = DeepSeekPredictor(model2, outputs)  # ä½¿ç”¨model2è¿›è¡Œé¢„æµ‹
                    
                    # æ”¶é›†æ•°æ®
                    all_predictions.extend(outputs.squeeze().cpu().numpy())
                    all_targets.extend(targets.cpu().numpy())
                    all_dates.extend(dates)
                    all_model2_preds.extend(model2_preds.cpu().numpy())
                    
                    # æ”¶é›†ç‰¹å¾æ•°æ®ç”¨äºåˆ†æ
                    all_feature_sequences.extend(feature_sequences.cpu().numpy())
                    all_processed_features.extend(processed_features.cpu().numpy())
                    
                    # è®¡ç®—å¥–åŠ±
                    direction_match = (torch.sign(model2_preds) == torch.sign(targets)).float()
                    accuracy_reward = direction_match * 0.8
                    error_reward = torch.exp(-2 * torch.abs(model2_preds - targets)) * 0.2
                    batch_reward = (accuracy_reward + error_reward).cpu().numpy()
                    all_rewards.extend(batch_reward)
                    
                    # æ¯100æ‰¹æ¬¡æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                    if batch_idx % 100 == 0:
                        logger.info(f"ğŸ”„ å¤„ç†æµ‹è¯•æ‰¹æ¬¡ {batch_idx+1}/{len(test_loader)}")
            
            # 3. ç»“æœä¿å­˜ï¼ˆç»“æ„åŒ–æ•°æ®ï¼‰
            results_df = pd.DataFrame({
                'date': all_dates,
                'prediction': all_predictions,  # ä¸»æ¨¡å‹é¢„æµ‹
                'model2_prediction': all_model2_preds,  # model2çš„é¢„æµ‹
                'target': all_targets,
                'error': np.abs(np.array(all_model2_preds) - np.array(all_targets)),
                'reward': all_rewards
            })

            # æ·»åŠ ç‰¹å¾æ•°æ®åˆ°DataFrame
            if all_feature_sequences:
                # ç‰¹å¾åºåˆ—é€šå¸¸æ˜¯ä¸‰ç»´çš„ [batch, seq_len, features]
                # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿é«˜æ•ˆå¤„ç†
                feature_array = np.array(all_feature_sequences)
                
                # è·å–ç‰¹å¾æ•°é‡
                num_features = feature_array.shape[-1]
                
                # ç›´æ¥ä½¿ç”¨ç‰¹å¾ç´¢å¼•å‘½ååˆ—
                for i in range(num_features):
                    # å–åºåˆ—ä¸­æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å€¼ä½œä¸ºå½“å‰ç‰¹å¾å€¼
                    results_df[f'feature_{i}'] = feature_array[:, -1, i]

            # ä¿å­˜é¢„æµ‹ç»“æœ
            output_path = save_structured_data(
                results_df, 
                config, 
                f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            )
            logger.info(f"ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {output_path}")

            # 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            try:
                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
                num_samples = len(all_targets)
                if num_samples < 10:
                    raise ValueError(f"æ ·æœ¬æ•°é‡ä¸è¶³: {num_samples}, æ— æ³•è¿›è¡Œå¯é çš„è¯„ä¼°")
                
                # åŸºæœ¬ç»Ÿè®¡æŒ‡æ ‡
                mae = np.mean(np.abs(np.array(all_model2_preds) - np.array(all_targets)))
                rmse = np.sqrt(mean_squared_error(all_targets, all_model2_preds))
                
                # é‡‘èç›¸å…³æŒ‡æ ‡
                sharpe_val = sharpe_ratio(np.array(all_model2_preds))
                drawdown = max_drawdown(np.array(all_model2_preds))
                
                # å¥–åŠ±ç»Ÿè®¡
                avg_test_reward = np.mean(all_rewards) if all_rewards else 0.0
                reward_std = np.std(all_rewards) if all_rewards else 0.0
                
                # æ–¹å‘å‡†ç¡®ç‡
                predicted_signs = np.sign(np.array(all_model2_preds))
                target_signs = np.sign(np.array(all_targets))
                valid_idx = (predicted_signs != 0) & (target_signs != 0)  # å¿½ç•¥é›¶å€¼
                direction_accuracy = np.mean(predicted_signs[valid_idx] == target_signs[valid_idx]) if any(valid_idx) else np.nan
                
                # ç‰¹å¾ç›¸å…³æ€§åˆ†æ (ä½¿ç”¨ç‰¹å¾ç´¢å¼•)
                feature_correlations = {}
                for i in range(num_features):
                    col_name = f'feature_{i}'
                    # è¿‡æ»¤æ— ç©·å¤§å’ŒNaNå€¼
                    valid_idx = np.isfinite(results_df[col_name]) & np.isfinite(results_df['model2_prediction'])
                    
                    if np.sum(valid_idx) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
                        correlation = np.corrcoef(
                            results_df.loc[valid_idx, col_name],
                            results_df.loc[valid_idx, 'model2_prediction']
                        )[0, 1]
                        feature_correlations[col_name] = correlation
                    else:
                        feature_correlations[col_name] = np.nan
                        logger.warning(f"æ— æ³•è®¡ç®—ç‰¹å¾'{col_name}'çš„ç›¸å…³æ€§ - æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³")
                
                # ç»„åˆè¯„ä¼°ç»“æœ
                eval_results = {
                    "MAE": mae,
                    "RMSE": rmse,
                    "Direction_Accuracy": direction_accuracy,
                    "Sharpe_Ratio": sharpe_val,
                    "Max_Drawdown": drawdown,
                    "Avg_Reward": avg_test_reward,
                    "Reward_STD": reward_std,
                    "num_samples": num_samples,
                    "config_path": config_path,
                    "test_data_path": test_dir,
                    "feature_correlations": feature_correlations,
                    "test_start_date": min(all_dates) if all_dates else "N/A",
                    "test_end_date": max(all_dates) if all_dates else "N/A"
                }
                
                # 5. ä¿å­˜è¯„ä¼°ç»“æœ
                eval_results_serializable = convert_to_serializable(eval_results)
                eval_dir = config['paths']['model1_eval'] if 'model1_eval' in config['paths'] else './eval_results'
                output_path = os.path.join(
                    eval_dir,
                    f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                os.makedirs(eval_dir, exist_ok=True)
                with open(output_path, 'w') as f:
                    json.dump(eval_results_serializable, f, indent=4)
                logger.info(f"ğŸ“ ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
                
                # 6. æ‰“å°å…³é”®æŒ‡æ ‡
                logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
                logger.info(f"æ ·æœ¬æ•°é‡: {num_samples}")
                if all_dates:
                    logger.info(f"æ—¶é—´èŒƒå›´: {min(all_dates)} è‡³ {max(all_dates)}")
                logger.info(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
                logger.info(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.6f}")
                logger.info(f"æ–¹å‘å‡†ç¡®ç‡: {direction_accuracy*100 if not np.isnan(direction_accuracy) else 'N/A':.2f}%")
                logger.info(f"å¹³å‡å¥–åŠ±: {avg_test_reward:.4f} Â± {reward_std:.4f}")
                logger.info(f"å¤æ™®æ¯”ç‡: {sharpe_val:.4f}")
                logger.info(f"æœ€å¤§å›æ’¤: {drawdown*100:.2f}%")
                
                # æ‰“å°ç‰¹å¾ç›¸å…³æ€§æ‘˜è¦
                if feature_correlations:
                    logger.info("\nğŸ” ç‰¹å¾é¢„æµ‹å€¼ç›¸å…³æ€§:")
                    for feature, corr in feature_correlations.items():
                        logger.info(f"{feature}: {corr:.4f}")
                
                # ä¿å­˜ç‰¹å¾æ˜ å°„ä¿¡æ¯
                if test_datasets:
                    try:
                        # è·å–ç‰¹å¾æ˜ å°„
                        feature_mapping = test_datasets[0].get_feature_mapping() if isinstance(test_datasets, list) else test_datasets.get_feature_mapping()
                        
                        # ä¿å­˜ç‰¹å¾æ˜ å°„æ–‡ä»¶
                        mapping_path = output_path.replace('.json', '_feature_mapping.json')
                        with open(mapping_path, 'w') as f:
                            json.dump(feature_mapping, f, indent=4)
                        logger.info(f"ğŸ“‹ ä¿å­˜ç‰¹å¾æ˜ å°„åˆ°: {mapping_path}")
                        
                        # åŒæ—¶è®°å½•åœ¨è¯„ä¼°ç»“æœä¸­
                        eval_results['feature_mapping'] = feature_mapping
                        with open(output_path, 'w') as f:  # é‡æ–°å†™å…¥åŒ…å«æ˜ å°„ä¿¡æ¯çš„ç»“æœ
                            json.dump(convert_to_serializable(eval_results), f, indent=4)
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ— æ³•ä¿å­˜ç‰¹å¾æ˜ å°„: {str(e)}")
                
            except ValueError as e:
                logger.warning(f"âš ï¸ {str(e)} - è·³è¿‡éƒ¨åˆ†è¯„ä¼°æŒ‡æ ‡")
                # åˆ›å»ºæœ€å°é”™è¯¯æŠ¥å‘Š
                error_report = {
                    "warning": str(e),
                    "num_samples": len(all_targets),
                    "test_files": test_dir['test'],
                    "partial_results": eval_results if 'eval_results' in locals() else None
                }
                error_path = os.path.join(
                    config['paths']['model1_eval'],
                    f"partial_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                with open(error_path, 'w') as f:
                    json.dump(convert_to_serializable(error_report), f, indent=4)
                logger.warning(f"âš ï¸ ä¿å­˜éƒ¨åˆ†è¯„ä¼°ç»“æœåˆ°: {error_path}")

            except Exception as e:
                logger.error(f"âŒ è¯„ä¼°æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}", exc_info=True)
                # åˆ›å»ºæœ€å°åŒ–é”™è¯¯æŠ¥å‘Š
                error_report = {
                    "error": str(e),
                    "num_samples": len(all_targets) if 'all_targets' in locals() else 0,
                    "test_files": test_dir['test']
                }
                error_path = os.path.join(
                    config['paths']['model1_eval'],
                    f"error_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                with open(error_path, 'w') as f:
                    json.dump(convert_to_serializable(error_report), f, indent=4)
                logger.error(f"ğŸ’¾ ä¿å­˜é”™è¯¯æŠ¥å‘Šåˆ°: {error_path}")

            logger.info("âœ… æµ‹è¯•é˜¶æ®µå®Œæˆ!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="RLäº¤æ˜“ç³»ç»Ÿè®­ç»ƒè„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument("--config", 
                        type=str, 
                        default="configs/model1.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: configs/model1.yaml")
    
    args = parser.parse_args()
    main(args.config)
