{
    "MAE": 13851.4111328125,
    "RMSE": 53220.421875,
    "Sharpe_Ratio": -26.324539184570312,
    "Max_Drawdown": -0.0,
    "num_samples": 15,
    "input_features": [
        "Feature_0",
        "Feature_1",
        "Feature_2",
        "Feature_3",
        "Feature_4",
        "Feature_5",
        "Feature_6",
        "Feature_7",
        "Feature_8",
        "Feature_9",
        "Feature_10",
        "Feature_11",
        "Feature_12",
        "Feature_13",
        "Feature_14",
        "Feature_15",
        "Feature_16",
        "Feature_17",
        "Feature_18",
        "Feature_19",
        "Feature_20",
        "Feature_21",
        "Feature_22",
        "Feature_23",
        "Feature_24",
        "Feature_25",
        "Feature_26",
        "Feature_27",
        "Feature_28",
        "Feature_29"
    ],
    "config_path": "configs/model1.yaml",
    "test_data_path": "/home/liyakun/twitter-stock-prediction/data/splits/test/",
    "model_summary": "TransformerModel(\n  (input_fc): Linear(in_features=773, out_features=1024, bias=True)\n  (layers): ModuleList(\n    (0-15): 16 x TransformerEncoderLayer(\n      (self_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n      )\n      (linear1): Linear(in_features=1024, out_features=5012, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (linear2): Linear(in_features=5012, out_features=1024, bias=True)\n      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n)"
}