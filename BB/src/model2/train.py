from torch.utils.data import DataLoader
import os
import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from ..model1.predict import predict_texts
import yaml
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import AutoTokenizer, AutoModel
# è§£å†³ import è·¯å¾„
THIS_FILE = Path(__file__).resolve()
PROJECT_ROOT = THIS_FILE.parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# ------------------ é»˜è®¤é…ç½®è·¯å¾„ï¼ˆå†™æ­»ï¼Œä¾¿äºç›´æ¥è¿è¡Œï¼‰ ------------------
DEFAULT_CONFIG_PATH = Path("/home/liyakun/twitter-stock-prediction/configs/model2.yaml")


# ------------------ æ—¥å¿— ------------------
def setup_logger(name: str,
                 log_file: Path,
                 console_level=logging.INFO,
                 file_level=logging.DEBUG,
                 also_timestamp_file: bool = True) -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(logging.DEBUG)

    # æ§åˆ¶å°
    ch = logging.StreamHandler()
    ch.setLevel(console_level)
    ch.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(ch)

    # å›ºå®šæ—¥å¿—æ–‡ä»¶
    log_file = Path(log_file)
    log_file.parent.mkdir(parents=True, exist_ok=True)
    fh = logging.FileHandler(str(log_file), encoding="utf-8")
    fh.setLevel(file_level)
    fh.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
    logger.addHandler(fh)

    # æ—¶é—´æˆ³æ—¥å¿—æ–‡ä»¶ï¼ˆé¢å¤–ï¼‰
    if also_timestamp_file:
        ts_name = f"{log_file.stem}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{log_file.suffix}"
        ts_path = log_file.parent / ts_name
        fh2 = logging.FileHandler(str(ts_path), encoding="utf-8")
        fh2.setLevel(file_level)
        fh2.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
        logger.addHandler(fh2)

    logger.debug(f"æ—¥å¿—å†™å…¥: {log_file}")
    return logger


# ------------------ é…ç½® ------------------
def load_config(path: Path):
    if not Path(path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# ------------------ æ•°æ®é›† ------------------
class TextTargetDataset(Dataset):
    """
    è¯»å– CSVï¼ˆå¿…é¡»åŒ…å« text, targetï¼›å¯é€‰ dateï¼‰
    """
    def __init__(self, csv_path: str):
        df = pd.read_csv(csv_path)
        cols = [c.lower() for c in df.columns]
        df.columns = cols

        if "text" not in df.columns:
            raise ValueError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ° 'text' åˆ—ï¼Œç°æœ‰åˆ—ï¼š{list(df.columns)}")
        if "target" not in df.columns:
            raise ValueError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ° 'target' åˆ—ï¼Œç°æœ‰åˆ—ï¼š{list(df.columns)}")

        self.texts = df["text"].astype(str).tolist()
        self.targets = torch.tensor(df["target"].astype(float).values, dtype=torch.float32)
        self.dates = df["date"].astype(str).tolist() if "date" in df.columns else [""] * len(df)

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, i):
        return self.texts[i], self.targets[i], self.dates[i]


# ------------------ ç‰¹å¾æŠ½å– ------------------
class EmbeddingExtractor:
    """
    ä¼˜å…ˆè°ƒç”¨ä½ è‡ªå·±çš„ src.model1.predict.infer_embeddings(texts, tokenizer_dir, device) -> torch.FloatTensor [N,D]
    å›é€€ï¼šç”¨ HuggingFace çš„ tokenizer+AutoModelï¼Œä»æœ€åéšå±‚å– CLS å‘é‡ï¼ˆæˆ–å¹³å‡æ± åŒ–ï¼‰
    """
    def __init__(self, tokenizer_dir: str, device: torch.device, logger: logging.Logger):
        self.tokenizer_dir = tokenizer_dir
        self.device = device
        self.logger = logger

        # ä¼˜å…ˆå°è¯•ä½ è‡ªå·±çš„å‡½æ•°
        self._user_infer = None
        self.logger.info(f"ğŸ§© æŠ½ç‰¹å¾ï¼šå›é€€ HuggingFace @ {tokenizer_dir}")
        self._build_hf_stack()

    def _build_hf_stack(self):
        
        # åªä»æœ¬åœ°åŠ è½½ï¼Œé¿å…è¢«å½“æˆ Hub å
        self.hf_tokenizer = AutoTokenizer.from_pretrained(
            self.tokenizer_dir, use_fast=True, local_files_only=True, trust_remote_code=True
        )
        self.hf_model = AutoModel.from_pretrained(
            self.tokenizer_dir,
            local_files_only=True,
            trust_remote_code=True,
            torch_dtype=(torch.float16 if self.device.type == "cuda" else None),
            attn_implementation="eager",   # â† å…³é”®
        ).to(self.device)
        self.hf_model.eval()


    @torch.no_grad()
    def encode(self, texts: list, max_length: int = 128, batch_size: int = 32) -> torch.Tensor:
        # 1) ä½ çš„è‡ªå®šä¹‰å‡½æ•°
        if self._user_infer is not None:
            try:
                embs = self._user_infer(texts=texts,
                                        tokenizer_dir=self.tokenizer_dir,
                                        device=str(self.device),
                                        max_length=max_length,
                                        batch_size=batch_size)
                if isinstance(embs, np.ndarray):
                    embs = torch.tensor(embs, dtype=torch.float32)
                return embs.to(self.device)
            except Exception as e:
                self.logger.warning(f"âš ï¸ è‡ªå®šä¹‰ infer_embeddings å¤±è´¥ï¼Œæ”¹ç”¨ HFï¼š{e}")
                self._user_infer = None  # åç»­ç›´æ¥èµ° HF

        # 2) HuggingFace å›é€€ï¼šCLS
        from transformers import BatchEncoding
        embs = []
        for i in range(0, len(texts), batch_size):
            chunk = texts[i:i + batch_size]
            enc = self.hf_tokenizer(
                chunk,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(self.device)

            outputs = self.hf_model(**enc, output_hidden_states=True)
            # å–æœ€åä¸€å±‚çš„ç¬¬ 1 ä¸ª token (CLS) å‘é‡
            last_hidden = outputs.last_hidden_state  # [B, L, H]
            cls_vec = last_hidden[:, 0, :]           # [B, H]
            embs.append(cls_vec.float().detach().cpu())

        embs = torch.cat(embs, dim=0)  # [N, H]
        return embs.to(self.device)


# ------------------ æ¨¡å‹æ„å»ºï¼ˆä»ç›®å½•è¯»å–ç»“æ„ï¼‰------------------
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_size: int = 512, output_dim: int = 1, num_layers: int = 2):
        super().__init__()
        layers = []
        d = input_dim
        for _ in range(max(1, num_layers - 1)):
            layers += [nn.Linear(d, hidden_size), nn.ReLU()]
            d = hidden_size
        layers += [nn.Linear(d, output_dim)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


def build_model_from_dir(model_dir: str, logger: logging.Logger, device: torch.device):
    """
    ä¼˜å…ˆå°è¯• HuggingFace AutoConfig/AutoModelï¼›
    å¦åˆ™æŒ‰ç®€å• config.json é‡Œ input_dim/hidden_size/output_dim é€ ä¸€ä¸ª MLPã€‚
    è¿”å›ï¼š(model, model_kind, output_dim)
    """
    model_dir = Path(model_dir)
    cfg_path = model_dir / "config.json"
    if not cfg_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json: {model_dir}")

    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg_json = json.load(f)

    # 1) HuggingFace é£æ ¼
    try:
        from transformers import AutoConfig, AutoModel
        hf_config = AutoConfig.from_pretrained(str(model_dir))
        model = AutoModel.from_pretrained(str(model_dir), config=hf_config).to(device)
        logger.info(f"ğŸ§  ä½¿ç”¨ HF æ¨¡å‹: {getattr(hf_config, 'architectures', ['AutoModel'])[0]}")
        # æˆ‘ä»¬åšçš„æ˜¯å›å½’å¤´è®­ç»ƒï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦ä¸€ä¸ªçº¿æ€§å¤´å°†éšè—å‘é‡ -> æ ‡é‡
        head = nn.Linear(hf_config.hidden_size, 1).to(device)
        return (model, "hf_backbone+linear", 1, head)
    except Exception:
        logger.info("â„¹ï¸ é HF æ¶æ„ï¼ŒæŒ‰ç®€å• MLP é…ç½®æ„å»º")

    # 2) ç®€å• MLP
    input_dim = int(cfg_json.get("input_dim", 768))
    hidden_size = int(cfg_json.get("hidden_size", 512))
    output_dim = int(cfg_json.get("output_dim", 1))
    num_layers = int(cfg_json.get("num_layers", 2))

    model = MLP(input_dim=input_dim,
                hidden_size=hidden_size,
                output_dim=output_dim,
                num_layers=num_layers).to(device)
    logger.info(f"ğŸ§  ä½¿ç”¨ MLP: in={input_dim}, hidden={hidden_size}, out={output_dim}, layers={num_layers}")
    return (model, "mlp", output_dim, None)


# ------------------ æŸå¤±ï¼ˆå•ç‹¬å†™ä¸€ä¸ªç±»ï¼‰------------------
class SafeSmoothL1Loss(nn.Module):
    def __init__(self):
        super().__init__()
        self._crit = nn.SmoothL1Loss()

    def forward(self, pred, target):
        if not torch.isfinite(pred).all():
            pred = torch.nan_to_num(pred, nan=0.0, posinf=1e6, neginf=-1e6)
        if not torch.isfinite(target).all():
            target = torch.nan_to_num(target, nan=0.0, posinf=1e6, neginf=-1e6)
        return self._crit(pred, target)


# ------------------ è®­ç»ƒä¸»æµç¨‹ ------------------
def main(config_path: Path = DEFAULT_CONFIG_PATH):
    # è·¯å¾„é…ç½®ï¼ˆå…¨éƒ¨ä»é…ç½®æ–‡ä»¶é‡Œå–ï¼Œä¸å†å†™æ­»é»˜è®¤å€¼ï¼‰
    cfg = load_config(config_path)
    paths = cfg["paths"]
    env   = cfg.get("env", {})

    train_csv = paths["train_data_path"]
    val_csv   = paths["val_data_path"]
    out_model = Path(paths["output_model"])
    log_file  = Path(paths["log_file"])
    model2_dir = paths["model2_dir"]

    # HF tokenizer ç”¨
    tokenizer_dir = env["tokenizer_dir"]

    # æ¨¡å‹ä¸€ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰çš„é…ç½®æ–‡ä»¶è·¯å¾„
    model1_config_path = paths["model1_config"]

    # è®­ç»ƒè¶…å‚ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    bs      = int(cfg["training"].get("batch_size", 16))
    lr      = float(cfg["training"].get("learning_rate", 1e-4))
    epochs  = int(cfg["training"].get("epochs", 10))
    max_len = int(cfg["training"].get("max_length", 128))

    # Debugï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    DEBUG_SMALL_TRAIN   = cfg["training"].get("debug_small_train", True)
    DEBUG_TRAIN_SAMPLES = int(cfg["training"].get("debug_train_samples", 100))
    DEBUG_MAX_BATCHES   = cfg["training"].get("debug_max_batches", None)  # None æˆ– æ­£æ•´æ•°

    # æ—¥å¿—ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    logger = setup_logger("model2_train", log_file,
                          console_level=logging.INFO,
                          file_level=logging.DEBUG,
                          also_timestamp_file=True)
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®: {config_path}")
    logger.info(f"ğŸ“ è®­ç»ƒé›†: {train_csv}")
    logger.info(f"ğŸ“ éªŒè¯é›†: {val_csv}")
    logger.info(f"ğŸ§­ æ¨¡å‹ç›®å½•ï¼ˆè¯»å–ç»“æ„ï¼‰: {model2_dir}")
    logger.info(f"ğŸ§© ç‰¹å¾æŠ½å–: ä¼˜å…ˆ src.model1.predict.infer_embeddingsï¼›å›é€€ HF @ {tokenizer_dir}")
    logger.info(f"ğŸ“ æ¨¡å‹ä¸€é…ç½®(æ–‡æœ¬ç”Ÿæˆ)è·¯å¾„: {model1_config_path}")

    # è®¾å¤‡ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"âš™ï¸ è®¾å¤‡: {device}")

    # æ•°æ®é›†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    train_ds = TextTargetDataset(train_csv)
    val_ds   = TextTargetDataset(val_csv)

    # === Debug å­é›†ï¼šè®­ç»ƒ & éªŒè¯éƒ½å—åŒä¸€å¼€å…³æ§åˆ¶ ===
    if DEBUG_SMALL_TRAIN:
        n_train = min(DEBUG_TRAIN_SAMPLES, len(train_ds))
        n_val   = min(DEBUG_TRAIN_SAMPLES, len(val_ds))
        train_ds_loader = Subset(train_ds, range(n_train))
        val_ds_loader   = Subset(val_ds,   range(n_val))
        logger.warning(f"âš ï¸ Debug æ¨¡å¼ï¼šè®­ç»ƒä»…ä½¿ç”¨å‰ {n_train} æ¡ï¼ŒéªŒè¯ä»…ä½¿ç”¨å‰ {n_val} æ¡æ ·æœ¬")
    else:
        train_ds_loader = train_ds
        val_ds_loader   = val_ds

    # ç‰¹å¾æŠ½å–å™¨ï¼ˆä¿æŒåŸé€»è¾‘ï¼Œç”¨äºæŠŠâ€œæ–‡æœ¬â€ç¼–ç æˆå‘é‡ï¼‰
    extractor = EmbeddingExtractor(tokenizer_dir=tokenizer_dir, device=device, logger=logger)

    # å…ˆæŠ½ä¸€å°æ‰¹ï¼Œå¾—åˆ°ç‰¹å¾ç»´åº¦ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    probe_texts = [train_ds[0][0]]
    try:
        probe_gen = predict_texts(
            probe_texts,
            config_path=str(model1_config_path),
            max_new_tokens=32,  # è°ƒå°ä¸€ç‚¹ï¼Œdebug æ›´å¿«
            temperature=1.0, top_p=1.0, do_sample=False
        )
    except Exception as e:
        logger.warning(f"âš ï¸ è°ƒç”¨æ¨¡å‹ä¸€ç”Ÿæˆæ¢é’ˆæ–‡æœ¬å¤±è´¥ï¼Œå°†å›é€€ä½¿ç”¨åŸå§‹æ–‡æœ¬ã€‚åŸå› : {e}")
        probe_gen = probe_texts

    probe_emb = extractor.encode(probe_gen, max_length=max_len, batch_size=1)  # [1, D]
    feat_dim = probe_emb.shape[-1]
    logger.info(f"ğŸ§· è®­ç»ƒç‰¹å¾ç»´åº¦: {feat_dim}")

    # æ„å»ºæ¨¡å‹äºŒï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    model2, model_kind, out_dim, hf_head = build_model_from_dir(model2_dir, logger, device)

    # å¦‚æœæ˜¯ HF backboneï¼Œå°±åŠ çº¿æ€§å¤´ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    use_linear_head_only = (model_kind == "hf_backbone+linear") and (hf_head is not None)
    if use_linear_head_only:
        for p in model2.parameters():
            p.requires_grad = False
        reg_head = hf_head  # nn.Linear(hidden_size, 1)
        params = list(reg_head.parameters())
        logger.info("ğŸ§± è®­ç»ƒç­–ç•¥ï¼šå†»ç»“ HF ä¸»ä½“ï¼Œä»…è®­ç»ƒçº¿æ€§å›å½’å¤´")
    else:
        reg_head = None
        params = list(model2.parameters())
        if model_kind == "mlp":
            try:
                _ = model2(torch.zeros(1, feat_dim, device=device))
            except Exception as e:
                logger.error(f"âŒ MLP å‰å‘æ£€æŸ¥å¤±è´¥ï¼Œå¯èƒ½æ˜¯ input_dim ä¸åŒ¹é…ï¼ˆæœŸæœ›ä¸ç‰¹å¾ç»´åº¦ {feat_dim} ä¸€è‡´ï¼‰: {e}")
                raise

    # ä¼˜åŒ–å™¨ & æŸå¤±ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    opt = torch.optim.AdamW(params, lr=lr)
    crit = SafeSmoothL1Loss()

    # DataLoaderï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    def collate(batch):
        texts, targets, dates = zip(*batch)
        return list(texts), torch.stack(targets), list(dates)

    train_loader = DataLoader(
        train_ds_loader, batch_size=bs, shuffle=True,
        num_workers=0, pin_memory=True, collate_fn=collate
    )
    val_loader = DataLoader(
        val_ds_loader, batch_size=bs, shuffle=False,
        num_workers=0, pin_memory=True, collate_fn=collate
    )

    # y ç»Ÿè®¡ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    target_only_loader = DataLoader(
        train_ds_loader, batch_size=4096, shuffle=False,
        num_workers=0, collate_fn=lambda batch: torch.stack([b[1] for b in batch])
    )
    with torch.no_grad():
        y_chunks = [yb.float().view(-1).cpu() for yb in target_only_loader]
        y_all = torch.cat(y_chunks, dim=0)
    logger.info(
        f"[y ç»Ÿè®¡] mean={y_all.mean().item():.4f}, "
        f"std={y_all.std().item():.4f}, "
        f"min={y_all.min().item():.4f}, "
        f"max={y_all.max().item():.4f}, "
        f"n={y_all.numel()}"
    )

    # è®­ç»ƒé…ç½®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    out_model.parent.mkdir(parents=True, exist_ok=True)
    SAVE_ONLY_LAST = True
    best_val = float("inf")
    best_state = None

    # ========= è®­ç»ƒå¾ªç¯ï¼ˆä»…åœ¨â€œæ–‡æœ¬â†’æ¨¡å‹ä¸€â†’ç”Ÿæˆæ–‡æœ¬â†’ç¼–ç â†’æ¨¡å‹äºŒâ€è¿™ä¸€ç¯åšæ”¹åŠ¨ï¼‰=========
    for epoch in range(1, epochs + 1):
        t0 = time.time()
        model2.train()
        if reg_head is not None:
            reg_head.train()

        run_loss = 0.0
        seen = 0

        logger.info(f"\nã€è®­ç»ƒã€‘å¼€å§‹ç¬¬ {epoch} è½®")
        for b, (orig_texts, y, _) in enumerate(train_loader, start=1):
            if DEBUG_MAX_BATCHES is not None and b > int(DEBUG_MAX_BATCHES):
                logger.info(f"ğŸ”§ Debug: ä»…è·‘å‰ {DEBUG_MAX_BATCHES} ä¸ª batch")
                break

            # â˜…â˜…â˜… 1) ç”¨â€œæ¨¡å‹ä¸€â€æŠŠåŸå§‹æ–‡æœ¬è½¬æˆâ€œç”Ÿæˆæ–‡æœ¬â€ï¼ˆä¸å†™æ–‡ä»¶ï¼Œç›´æ¥è¿”å›ï¼‰
            try:
                gen_texts = predict_texts(
                    input_texts=orig_texts,
                    config_path=str(model1_config_path),
                    max_new_tokens=64,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True
                )
            except Exception as e:
                # å¤±è´¥å›é€€åˆ°åŸå§‹æ–‡æœ¬ï¼Œä¿è¯è®­ç»ƒä¸ä¸­æ–­
                logger.warning(f"âš ï¸ æ¨¡å‹ä¸€ç”Ÿæˆå¤±è´¥ï¼Œå›é€€ä½¿ç”¨åŸå§‹æ–‡æœ¬ã€‚åŸå› : {e}")
                gen_texts = orig_texts

            # â˜…â˜…â˜… 2) æŠŠâ€œç”Ÿæˆæ–‡æœ¬â€ç¼–ç æˆç‰¹å¾å‘é‡ Xï¼Œå†é€è¿›æ¨¡å‹äºŒ
            with torch.no_grad():
                X = extractor.encode(gen_texts, max_length=max_len, batch_size=bs)  # [B, D]

            y = y.to(device)
            opt.zero_grad(set_to_none=True)
            if use_linear_head_only:
                pred = reg_head(X).squeeze(-1)    # [B]
            else:
                pred = model2(X).squeeze(-1)      # [B]

            loss = crit(pred, y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(params, 1.0)
            opt.step()

            run_loss += loss.item() * y.size(0)
            seen += y.size(0)

            if b % 20 == 0:
                logger.info(f"[Epoch {epoch}] batch {b} | loss={loss.item():.6f}")

        # â€”â€” æœ¬è½®è®­ç»ƒå‡å€¼ & æ—¶é—´ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        avg_train = run_loss / max(1, seen)
        dt = time.time() - t0
        logger.info(f"âœ… Epoch {epoch} å®Œæˆ | è®­ç»ƒæŸå¤±: {avg_train:.6f} | ç”¨æ—¶ {dt:.1f}s")

         # ========= éªŒè¯ï¼ˆåŒæ ·å…ˆç»è¿‡â€œæ¨¡å‹ä¸€ç”Ÿæˆæ–‡æœ¬â€ï¼‰=========
        model2.eval()
        if reg_head is not None:
            reg_head.eval()

        val_loss = 0.0
        vseen = 0

        # --- å¯è°ƒå‚æ•°ï¼šéªŒè¯æ—¶æ¯å¤šå°‘ä¸ª batch æ‰“ä¸€è¡Œè¿›åº¦ ---
        VAL_LOG_EVERY = 5
        # --- å¯é€‰ï¼šéªŒè¯æœ€å¤šè·‘å¤šå°‘ä¸ª batchï¼ˆNone è¡¨ç¤ºå…¨é‡ï¼‰---
        VAL_MAX_BATCHES = None  # e.g., 20 å…ˆè·‘å‰20ä¸ªbatchçœ‹çœ‹

        with torch.no_grad():
            for vb, (orig_texts, y, _) in enumerate(val_loader, start=1):
                # ï¼ˆå¯é€‰ï¼‰é™é‡è·‘ä¸€éƒ¨åˆ† batchï¼ŒåŠ é€Ÿè¯Šæ–­
                if VAL_MAX_BATCHES is not None and vb > int(VAL_MAX_BATCHES):
                    logger.info(f"ğŸ” ä»…éªŒè¯å‰ {VAL_MAX_BATCHES} ä¸ª batchï¼ˆè°ƒè¯•ç”¨ï¼‰")
                    break

                # éªŒè¯é˜¶æ®µï¼šç”¨æ›´ç¨³/æ›´å¿«çš„è®¾ç½®
                try:
                    gen_texts = predict_texts(
                        input_texts=orig_texts,
                        config_path=str(model1_config_path),
                        max_new_tokens=16,   # â† éªŒè¯ç”¨æ›´çŸ­ç”Ÿæˆ
                        temperature=1.0,
                        top_p=1.0,
                        do_sample=False      # â† è´ªå¿ƒï¼Œé¿å…é‡‡æ ·æ…¢å’Œæ¦‚ç‡æ•°å€¼é—®é¢˜
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ éªŒè¯ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åŸå§‹æ–‡æœ¬ã€‚åŸå› : {e}")
                    gen_texts = orig_texts

                X = extractor.encode(gen_texts, max_length=max_len, batch_size=bs)
                y = y.to(device)
                if use_linear_head_only:
                    pred = reg_head(X).squeeze(-1)
                else:
                    pred = model2(X).squeeze(-1)
                l = crit(pred, y)
                val_loss += l.item() * y.size(0)
                vseen += y.size(0)

                if vb % VAL_LOG_EVERY == 0:
                    logger.info(f"[Val] batch {vb} | partial_loss={l.item():.6f} | seen={vseen}")

        avg_val = val_loss / max(1, vseen)
        logger.info(f"ğŸ§ª éªŒè¯æŸå¤±: {avg_val:.6f}")

        # ========= æœ€ä½³ & ä¿å­˜ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰=========
        new_best = avg_val < best_val
        logger.info(
            f"Epoch {epoch}/{epochs} | è®­ç»ƒæŸå¤±={avg_train:.6f} | éªŒè¯æŸå¤±={avg_val:.6f} "
            f"| æœ€ä½³={ (best_val if best_val < float('inf') else float('nan')) :.6f}"
            f"{' âœ… æ–°æœ€ä½³' if new_best else ''}"
        )

        if new_best:
            best_val = avg_val
            if SAVE_ONLY_LAST:
                best_state = {"head": reg_head.state_dict()} if use_linear_head_only \
                            else {"model": model2.state_dict()}
            else:
                if use_linear_head_only:
                    torch.save(reg_head.state_dict(), str(out_model))
                else:
                    torch.save(model2.state_dict(), str(out_model))
                logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ä½³æ¨¡å‹ -> {out_model} (val={avg_val:.6f})")

    # è®­ç»ƒç»“æŸï¼šä¸€æ¬¡æ€§ä¿å­˜å…¨ç¨‹æœ€ä½³ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    if SAVE_ONLY_LAST and best_state is not None:
        if use_linear_head_only and "head" in best_state:
            torch.save(best_state["head"], str(out_model))
        elif not use_linear_head_only and "model" in best_state:
            torch.save(best_state["model"], str(out_model))
        logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜å…¨ç¨‹æœ€ä½³æ¨¡å‹ -> {out_model} (best val={best_val:.6f})")
    else:
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ")


if __name__ == "__main__":
    main(DEFAULT_CONFIG_PATH)
