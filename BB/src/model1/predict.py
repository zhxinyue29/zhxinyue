# predict.py  â€”â€” BBç‰ˆï¼šåœ¨ä¸æ”¹åŠ¨åŸæœ‰å‡½æ•°/ç±»/å¸¸é‡çš„å‰æä¸‹ï¼Œæ–°å¢æ–‡æœ¬ç”Ÿæˆæ¥å£å¹¶è®© main ä¸è½ç›˜
import os
from pathlib import Path
from datetime import datetime
import logging
import json

import torch
import pandas as pd
import yaml

# === æ–°å¢ï¼šä»…æ·»åŠ å¿…è¦çš„ç±»å‹ä¸æ¨¡å‹ä¾èµ–ï¼›ä¸ç§»é™¤åŸæœ‰ import ===
from typing import List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

DEFAULT_CONFIG = "/home/liyakun/twitter-stock-prediction/configs/model1.yaml"
# å¦‚æœä½ å¸Œæœ›ç›´æ¥åœ¨è„šæœ¬é‡Œç»™å‡ æ¡ç¤ºä¾‹æ–‡æœ¬ï¼Œå°±å¡«åˆ°è¿™é‡Œï¼›å¦åˆ™è®¾ä¸º Noneï¼Œä¼šè‡ªåŠ¨ä» test_data_path è¯»å–
SAMPLE_TEXTS = None
# ä»æµ‹è¯•é›†å–å¤šå°‘æ¡åšæ¼”ç¤ºï¼ˆå½“ SAMPLE_TEXTS=None æ—¶ç”Ÿæ•ˆï¼‰
TEST_TAKE_N = 5
# è¾“å‡ºç»“æœä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
DEFAULT_OUTPUT_JSON = "/home/liyakun/twitter-stock-prediction/BB/results/model1_predict_preview.json"

logger = logging.getLogger("predict")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(ch)

# ä½ ä¹‹å‰ç»™çš„æ¨¡å‹ä¸€å ä½ç±»ï¼ˆä¿ç•™ï¼Œæœªæ”¹åŠ¨ï¼‰
class DeepSeekTextModel(torch.nn.Module):
    def __init__(self, model_weights_path: str, device='cuda'):
        super().__init__()
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        # ç¤ºä¾‹ï¼šç®€å•çº¿æ€§å±‚æ›¿ä»£çœŸå®æ¨¡å‹
        self.model = torch.nn.Linear(100, 50).to(self.device)
        # åŠ è½½æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if model_weights_path and Path(model_weights_path).exists():
            try:
                state_dict = torch.load(model_weights_path, map_location=self.device)
                # å…¼å®¹åªå­˜äº† state_dict æˆ–æ•´ä¸ª checkpoint çš„æƒ…å†µ
                if isinstance(state_dict, dict) and "state_dict" in state_dict:
                    state_dict = state_dict["state_dict"]
                self.model.load_state_dict(state_dict, strict=False)
                logger.info(f"âœ… æ¨¡å‹ä¸€æƒé‡å·²åŠ è½½: {model_weights_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½æƒé‡å¤±è´¥ï¼ˆå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰: {model_weights_path} | {e}")
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹æƒé‡ä¸å­˜åœ¨ï¼ˆå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰: {model_weights_path}")

    def forward(self, x):
        return self.model(x)

    def generate_text(self, input_tensor):
        """
        ç”Ÿæˆå¯è¯»æ–‡æœ¬è¾“å‡ºï¼ˆæ¼”ç¤ºç”¨ï¼‰
        """
        summaries = []
        for row in input_tensor:
            text = f"çŸ©é˜µè¡Œå’Œ: {row.sum().item():.4f}"
            summaries.append(text)
        return summaries

def load_config(config_path: str) -> dict:
    cfg_path = Path(config_path)
    if not cfg_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg or {}

def pick_model1_weights_from_config(cfg: dict) -> str:
    # å…¼å®¹ä¸¤ç§å¸¸è§å†™æ³•ï¼špaths.output_model æˆ– models.model1_path
    paths = cfg.get("paths", {})
    if "output_model" in paths:
        return paths["output_model"]
    models = cfg.get("models", {})
    if "model1_path" in models:
        return models["model1_path"]
    raise KeyError("æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°æ¨¡å‹ä¸€æƒé‡è·¯å¾„ï¼ˆæœŸæœ› paths.output_model æˆ– models.model1_pathï¼‰ã€‚")

def pick_test_csv_from_config(cfg: dict) -> str:
    paths = cfg.get("paths", {})
    # å…¼å®¹ test_data_path / test_dir / {"test":[...]}
    if "test_data_path" in paths:
        return paths["test_data_path"]
    if "test_dir" in paths:
        test_src = paths["test_dir"]
        if isinstance(test_src, str):
            return test_src
        if isinstance(test_src, dict) and "test" in test_src and test_src["test"]:
            return test_src["test"][0]
        if isinstance(test_src, (list, tuple)) and len(test_src) > 0:
            return test_src[0]
    raise KeyError("æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆæœŸæœ› paths.test_data_path æˆ– paths.test_dirï¼‰ã€‚")

def take_texts_from_csv(csv_path: str, n: int = 5) -> list:
    df = pd.read_csv(csv_path)
    # å…¼å®¹åˆ—å
    cand_cols = ["text", "tweet", "content", "message", "body"]
    text_col = None
    for c in cand_cols:
        if c in df.columns:
            text_col = c
            break
    if text_col is None:
        raise ValueError(f"æµ‹è¯•CSVä¸­æ‰¾ä¸åˆ°æ–‡æœ¬åˆ—ï¼ˆå°è¯• {cand_cols}ï¼‰ã€‚ç°æœ‰åˆ—={list(df.columns)}")
    texts = df[text_col].astype(str).fillna("").tolist()
    texts = [t for t in texts if t.strip()]
    if not texts:
        raise ValueError("æµ‹è¯•CSVä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ã€‚")
    return texts[:n]

def texts_to_embeddings(texts: list, device: torch.device, dim: int = 100) -> torch.Tensor:
    """
    âš ï¸ æ¼”ç¤ºç”¨ï¼šæŠŠæ–‡æœ¬â€œä¼ªè£…â€ä¸ºå‘é‡ã€‚
    çœŸå®æƒ…å†µè¯·ç”¨ tokenizer + model.encode() å¾—åˆ°å‘é‡ã€‚
    """
    torch.manual_seed(42)
    x = torch.randn(len(texts), dim, device=device)
    return x

def save_preview(results: list, out_path: str):
    os.makedirs(Path(out_path).parent, exist_ok=True)
    payload = []
    for r in results:
        row = {
            "input_text": r["input_text"],
            "output_text": r["output_text"],
            # çŸ©é˜µå¤§å¯¹è±¡ä¸ç›´æ¥å…¨é‡å†™å…¥ï¼Œç»™ä¸ªæ‘˜è¦
            "output_matrix_shape": list(r["output_matrix"].shape),
            "output_matrix_sum": float(r["output_matrix"].sum().item()),
        }
        payload.append(row)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“ ç»“æœé¢„è§ˆå·²ä¿å­˜: {out_path}")

# === æ–°å¢ï¼šä¾›æ¨¡å‹äºŒç›´æ¥è°ƒç”¨çš„æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼ˆä¸è½ç›˜ã€ç›´æ¥è¿”å› List[str]ï¼‰===
@torch.inference_mode()
def predict_texts(
    input_texts: List[str],
    config_path: str = DEFAULT_CONFIG,
    max_new_tokens: int = 64,
    temperature: float = 0.7,
    top_p: float = 0.9,
    do_sample: bool = True,
    eos_token: Optional[str] = None,
) -> List[str]:
    # 0) åŸºæœ¬æ ¡éªŒ
    if not isinstance(input_texts, list) or not all(isinstance(x, str) for x in input_texts):
        raise TypeError("input_texts å¿…é¡»æ˜¯ List[str]")

    # 1) åªä»é…ç½®è¯»å–ç›®å½•ï¼ˆä¸å†ä½¿ç”¨å†™æ­»è·¯å¾„ï¼‰
    cfg = load_config(config_path)
    paths = cfg.get("paths", {}) or {}
    env   = cfg.get("env", {}) or {}
    model1_dir = paths.get("model1_dir") or env.get("tokenizer_dir")
    if not model1_dir:
        raise KeyError("é…ç½®ç¼ºå°‘æ¨¡å‹ä¸€ç›®å½•ï¼šè¯·åœ¨ model1.yaml è®¾ç½® paths.model1_dir æˆ– env.tokenizer_dirã€‚")

    raw = Path(model1_dir)
    if raw.is_absolute():
        candidates = [raw]
    else:
        cfg_path = Path(config_path).resolve()
        cfg_dir  = cfg_path.parent
        cwd      = Path.cwd().resolve()
        # ä¾æ¬¡å°è¯•ï¼šcwdã€cwd/..ã€cwd/../..ã€é…ç½®ç›®å½•ã€é…ç½®ç›®å½•çš„ä¸Šä¸€çº§
        candidates = [
            (cwd / raw).resolve(),
            (cwd.parent / raw).resolve(),
            (cwd.parent.parent / raw).resolve(),
            (cfg_dir / raw).resolve(),
            (cfg_dir.parent / raw).resolve(),
        ]

    p = None
    for cand in candidates:
        if cand.exists() and ((cand / "tokenizer.json").exists() or (cand / "tokenizer_config.json").exists()):
            p = cand
            break

    if p is None:
        tried = "\n  - " + "\n  - ".join(str(c) for c in candidates)
        raise FileNotFoundError(
            "æœªæ‰¾åˆ°æ¨¡å‹ä¸€ç›®å½•ï¼ˆéœ€åŒ…å« tokenizer.json æˆ– tokenizer_config.jsonï¼‰ã€‚å·²å°è¯•ï¼š" + tried
        )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 2) ç¼“å­˜ tokenizer / modelï¼ˆé¦–æ¬¡æ„å»ºï¼Œåç»­å¤ç”¨ï¼›ç›®å½•å˜æ›´åˆ™é‡å»ºï¼‰
    if not hasattr(predict_texts, "_cache"):
        predict_texts._cache = {}
    cache = predict_texts._cache
    need_reload = (cache.get("dir") != str(p)) or ("tok" not in cache) or ("model" not in cache)

    if need_reload:
        from transformers import AutoTokenizer, AutoModelForCausalLM

        tok = AutoTokenizer.from_pretrained(
            str(p), use_fast=True, trust_remote_code=True, local_files_only=True
        )
        if tok.pad_token is None:
            tok.pad_token = tok.eos_token or "<|pad|>"

        # å…³é”®ï¼šFP32 + eager æ³¨æ„åŠ›ï¼ˆæ›´ç¨³ï¼Œé¿å… sdpa æ»‘çª—å†…æ ¸æ–­è¨€ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            str(p),
            trust_remote_code=True,
            torch_dtype=None,                 # â† FP32ï¼ˆæ¯” fp16 æ›´ä¸æ˜“ NaN/Infï¼‰
            local_files_only=True,
            attn_implementation="eager",
        ).to(device).eval()

        cache["tok"], cache["model"], cache["dir"] = tok, model, str(p)

        if not hasattr(predict_texts, "_printed_once"):
            logger.info(f"è®¾å¤‡: {device}")
            logger.info(f"æ¨¡å‹ä¸€ç›®å½•: {p}")
            predict_texts._printed_once = True

    tok, model = cache["tok"], cache["model"]

    # 3) ç¼–ç è¾“å…¥
    enc = tok(
        input_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=int(cfg.get("training", {}).get("max_length", 128)),
    ).to(device)

    # 4) ç»„è£… logits_processorï¼ˆæ¸…æ´— NaN/Infï¼Œé¿å… multinomial è§¦å‘æ–­è¨€ï¼‰
    from transformers import LogitsProcessorList
    logits_processors = LogitsProcessorList()
    try:
        from transformers import InfNanRemoveLogitsProcessor
        logits_processors.append(InfNanRemoveLogitsProcessor())
    except Exception:
        # å…¼å®¹è€ç‰ˆæœ¬ transformersï¼šè‡ªå®šä¹‰ä¸€ä¸ªç®€å•æ¸…æ´—å™¨ï¼ˆå‡½æ•°å†…å®šä¹‰ï¼Œä¸æ–°å¢å…¨å±€ç¬¦å·ï¼‰
        class _SafeLogitsProcessor:
            def __call__(self, input_ids, scores):
                # å°† NaN/Inf æ›¿æ¢ä¸ºæœ‰é™å€¼ï¼Œå¹¶æŠŠæç«¯å€¼è£å‰ªåˆ° [-1e4, 1e4]
                scores = torch.nan_to_num(scores, nan=-1e4, posinf=1e4, neginf=-1e4)
                scores.clamp_(min=-1e4, max=1e4)
                return scores
        logits_processors.append(_SafeLogitsProcessor())

    # 5) ç”Ÿæˆï¼ˆå…³é—­ AMP æ··ç²¾ï¼›è‹¥ä»æ‹…å¿ƒå¯å°† do_sample=False ä¸´æ—¶éªŒè¯ï¼‰
    gen_kwargs = dict(
        max_new_tokens=max_new_tokens,
        do_sample=do_sample,
        temperature=float(temperature),
        top_p=float(top_p),
        pad_token_id=tok.pad_token_id,
        eos_token_id=(tok.convert_tokens_to_ids(eos_token) if eos_token else tok.eos_token_id),
        logits_processor=logits_processors,
    )

    # ç¦ç”¨è‡ªåŠ¨æ··ç²¾ï¼Œè¿›ä¸€æ­¥ç¨³ä½æ•°å€¼
    if device.type == "cuda":
        with torch.autocast(device_type="cuda", enabled=False):
            out = model.generate(**enc, **gen_kwargs)
    else:
        out = model.generate(**enc, **gen_kwargs)

    # 6) ä»…è¿”å›æ–°å¢ tokenï¼ˆä¸å›ä¼ åŸè¾“å…¥ï¼‰
    input_lengths = (enc["input_ids"] != tok.pad_token_id).sum(dim=1).tolist()
    generated_texts: List[str] = []
    for i, seq in enumerate(out):
        start = input_lengths[i]
        new_tokens = seq[start:]
        text = tok.decode(new_tokens, skip_special_tokens=True)
        generated_texts.append(text.strip())

    return generated_texts


def main():
    config_path = DEFAULT_CONFIG
    logger.info(f"ä½¿ç”¨é…ç½®: {config_path}")

    cfg = load_config(config_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"è®¾å¤‡: {device}")

    # 1) æ„é€ è¾“å…¥æ–‡æœ¬ï¼ˆæ²¿ç”¨åŸè„šæœ¬ SAMPLE_TEXTS / æµ‹è¯•CSV é€»è¾‘ï¼‰
    if SAMPLE_TEXTS and len(SAMPLE_TEXTS) > 0:
        input_texts = SAMPLE_TEXTS
        logger.info(f"ä½¿ç”¨è„šæœ¬å†…ç½® SAMPLE_TEXTSï¼Œå…± {len(input_texts)} æ¡")
    else:
        test_csv = pick_test_csv_from_config(cfg)
        logger.info(f"ä»æµ‹è¯•é›†è¯»å–æ–‡æœ¬: {test_csv}ï¼ˆå–å‰ {TEST_TAKE_N} æ¡ï¼‰")
        input_texts = take_texts_from_csv(test_csv, n=TEST_TAKE_N)

    # 2) è°ƒç”¨æ–°å¢çš„ predict_textsï¼šç›´æ¥è¿”å›â€œç”Ÿæˆæ–‡æœ¬â€ï¼ˆä¸å†™ä»»ä½•æ–‡ä»¶ï¼‰
    outputs = predict_texts(
        input_texts=input_texts,
        config_path=config_path,
        max_new_tokens=64,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

    # 3) æ‰“å°ç»“æœï¼ˆä»…æ§åˆ¶å°æŸ¥çœ‹ï¼‰ï¼Œä¸ä¿å­˜æ–‡ä»¶
    for i, (inp, out) in enumerate(zip(input_texts, outputs), 1):
        print("â€”â€”â€”")
        print(f"[{i}] è¾“å…¥: {inp[:120]}{'...' if len(inp) > 120 else ''}")
        print(f"    è¾“å‡º: {out}")

    logger.info("âœ… é¢„æµ‹å®Œæˆï¼ˆæœªç”Ÿæˆä»»ä½•æ–‡ä»¶ï¼‰")
    return outputs

if __name__ == "__main__":
    main()
