import os
from transformers.tokenization_utils_base import BatchEncoding
from venv import logger
import torch
import torch.nn as nn
from torch.utils.data import ConcatDataset, DataLoader, Dataset
from pathlib import Path
import numpy as np
import pandas as pd
import yaml                                                                    
import logging
import json
import argparse
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from torch.utils.data import DataLoader, Dataset, random_split,Subset, DataLoader
import torch.nn.functional as F 
from contextlib import contextmanager
import inspect
from ..model2.inference import DeepSeekInfer
from transformers import AutoModel,  AutoModelForSequenceClassification,AutoConfig
from sklearn.metrics import mean_squared_error, mean_absolute_error
from transformers import AutoModelForCausalLM,AutoTokenizer,PreTrainedTokenizerBase
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
def _auto_amp_dtype():
    import torch
    return torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16
USE_AUTOCAST = True                         # âœ… å…¨å¤§å†™ï¼Œåé¢ä¸€ç›´ç”¨è¿™ä¸ªå
AMP_DTYPE   = _auto_amp_dtype()
USE_SCALER  = (AMP_DTYPE == torch.float16)  # ä»… FP16 éœ€è¦ GradScaler
if torch.cuda.is_available():
    device = torch.device("cuda:0")  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPU")
config_path = '/home/liyakun/twitter-stock-prediction/configs/model1.yaml'
def load_model(config_path):
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if not config:
            print("é…ç½®ä¸ºç©ºæˆ–æ ¼å¼æœ‰è¯¯")
            return None
        return config
    except Exception as e:
        print("è¯»å–é…ç½®å¤±è´¥ï¼š", e)
        return None
train_cfg = load_model(config_path)

try:
    model_path = train_cfg['env']['prediction_model_path']
    print("æ¨¡å‹è·¯å¾„ï¼š", model_path)
except KeyError:
    print("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° prediction_model_path å­—æ®µã€‚")
    # ä½ å¯ä»¥è®¾ç½®é»˜è®¤è·¯å¾„æˆ–æŠ›å‡ºå¼‚å¸¸
if not train_cfg:
    raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥ï¼")

# æå–å‚æ•°å’Œæ¨¡å‹è·¯å¾„

class ConfigLoader:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
    
    def _load_config(self, config_path: str) -> dict:
        p = Path(config_path)

        # 1) ç»å¯¹è·¯å¾„ & å­˜åœ¨ï¼šç›´æ¥ç”¨
        if p.is_absolute() and p.exists():
            with open(p, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
            # å¯é€‰ï¼šè®°å½•ä¸€ä¸‹è§£æåçš„çœŸå®è·¯å¾„ï¼ˆä¸å½±å“å¤–éƒ¨ç”¨æ³•ï¼‰
            self.config_path_resolved = str(p)
            print(f"[INFO] è§£æé…ç½®ï¼š{self.config_path_resolved}")
            return cfg

        # 2) ç›¸å¯¹è·¯å¾„ï¼šæŒ‰å¤šåŸºå‡†å°è¯•
        script_dir = Path(__file__).resolve().parent
        # ä¾æ¬¡å°è¯•ï¼šCWDã€è„šæœ¬ç›®å½•ã€ä»¥åŠè„šæœ¬ç›®å½•å‘ä¸Šæœ€å¤š 5 å±‚çˆ¶ç›®å½•
        bases = [Path.cwd(), script_dir]
        bases.extend(list(script_dir.parents)[:5])

        tried = []
        for base in bases:
            cand = (base / p).resolve()
            tried.append(str(cand))
            if cand.exists():
                with open(cand, "r", encoding="utf-8") as f:
                    cfg = yaml.safe_load(f)
                self.config_path_resolved = str(cand)
                print(f"[INFO] è§£æé…ç½®ï¼š{self.config_path_resolved}")
                return cfg

        # 3) å…¨éƒ½æ²¡å‘½ä¸­ï¼šæŠ¥é”™å¹¶æŠŠå°è¯•è¿‡çš„è·¯å¾„åˆ—å‡ºæ¥ï¼Œæ–¹ä¾¿æ’æŸ¥
        msg = "æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ã€‚å°è¯•è¿‡ï¼š\n" + "\n".join(f"  - {x}" for x in tried)
        raise FileNotFoundError(msg)

def setup_logger(
    name: str,
    log_file: str,
    console_level: int = logging.WARNING,   # å…³é”®ï¼šé»˜è®¤ WARNING èµ·
    file_level: int = logging.DEBUG,        # æ–‡ä»¶é‡Œè®°å½•æ›´è¯¦å°½
    also_timestamp_file: bool = True        # é¢å¤–å†å†™ä¸€ä»½å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
) -> logging.Logger:
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)          # æ€»å¼€å…³ç»™è¶³ï¼Œé  handler æ§åˆ¶å®é™…è¾“å‡º
    logger.propagate = False                # å…³é”®ï¼šä¸æŠŠæ—¥å¿—ç»§ç»­å¾€ root ä¼ 

    # æ¸…æ‰è‡ªå·±å·²æœ‰çš„ handlersï¼Œé˜²æ­¢é‡å¤æ·»åŠ 
    if logger.handlers:
        for h in logger.handlers[:]:
            logger.removeHandler(h)

    Path(log_file).parent.mkdir(parents=True, exist_ok=True)

    # æ–‡ä»¶ handlerï¼ˆå›ºå®šæ–‡ä»¶ï¼‰
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setLevel(file_level)
    fh.setFormatter(logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    ))
    logger.addHandler(fh)

    # å¯é€‰ï¼šæ—¶é—´æˆ³æ–‡ä»¶
    if also_timestamp_file:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        ts_file = str(Path(log_file).with_name(
            f"{Path(log_file).stem}_{ts}{Path(log_file).suffix}"
        ))
        fh2 = logging.FileHandler(ts_file, encoding="utf-8")
        fh2.setLevel(file_level)
        fh2.setFormatter(logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        ))
        logger.addHandler(fh2)

    # æ§åˆ¶å° handlerï¼ˆæŠŠçº§åˆ«æé«˜åˆ° WARNING/ERROR å°±å‡ ä¹ä¸å‡ºä¸œè¥¿äº†ï¼‰
    ch = logging.StreamHandler()
    ch.setLevel(console_level)
    ch.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(ch)

    # å½»åº•ç§»é™¤ root çš„é»˜è®¤ handlerï¼Œé¿å…åˆ«äººç”¨ logging.info() æ‰“è¿›æ§åˆ¶å°
    root = logging.getLogger()
    root.propagate = False
    for h in root.handlers[:]:
        root.removeHandler(h)
    root.setLevel(logging.WARNING)  # æ ¹æ—¥å¿—æé«˜é˜ˆå€¼ï¼Œå…œåº•

    return logger
def create_output_directories(config: Dict[str, Any]) -> None:
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•"""
    paths = config['paths']
    # æ¨¡å‹è¾“å‡ºç›®å½• (model1_output/)
    output_dir = Path(paths.get('processed_output_dir', 'data/processed/model1_output'))
    output_dir.mkdir(parents=True, exist_ok=True)
    # è¯„ä¼°ç»“æœç›®å½• (model1_eval/)
    eval_dir = Path(paths.get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_dir = Path(config['paths']['output_model']).parent
    model_save_dir.mkdir(parents=True, exist_ok=True)
    # æ—¥å¿—æ–‡ä»¶ç›®å½•
    log_dir = Path(config['paths']['log_file']).parent
    log_dir.mkdir(parents=True, exist_ok=True)

def convert_to_serializable(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(obj)
    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.detach().cpu().tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]
    else:
        return obj

def save_structured_data(data: pd.DataFrame, config: Dict[str, Any], filename: str = "processed_data.csv") -> Path:
    """
    ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°model1_outputç›®å½•
    """
    output_dir = Path(config['paths'].get('processed_output_dir', 'data/processed/model1_output'))
    output_path = output_dir / filename 
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    # æ·»åŠ æ—¥æœŸåˆ—ä½œä¸ºç»“æ„åŒ–æ•°æ®çš„ä¸€éƒ¨åˆ†
    if 'date' not in data.columns:
        if 'timestamp' in data.columns:
            data['date'] = pd.to_datetime(data['timestamp']).dt.date
        else:
            data['date'] = pd.Timestamp.now().strftime("%Y-%m-%d")
    data.to_csv(output_path, index=False)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜ç»“æ„åŒ–è¾“å‡ºæ•°æ®åˆ°: {output_path}")
    return output_path

def save_evaluation_results(results: Dict[str, Any], config: Dict[str, Any], filename: str = "evaluation_results.json") -> Tuple[Path, Path]:
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ°model1_evalç›®å½•
    """
    eval_dir = Path(config['paths'].get('eval_output_dir', 'results/model1_eval'))
    eval_dir.mkdir(parents=True, exist_ok=True)
    output_path = eval_dir / filename
    results_serializable = convert_to_serializable(results)
    with open(output_path, 'w') as f:
        json.dump(results_serializable, f, indent=4)
    logger = logging.getLogger("training")
    logger.info(f"âœ… ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ä¾¿äºåˆ†æ
    csv_path = output_path.with_suffix('.csv')
    eval_df = pd.DataFrame([results])
    eval_df.to_csv(csv_path, index=False)
    logger.info(f"âœ… åŒæ—¶ä¿å­˜CSVæ ¼å¼è¯„ä¼°ç»“æœ: {csv_path}")
    
    return output_path, csv_path

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    """è®¡ç®—å¤æ™®æ¯”ç‡"""
    excess_returns = returns - risk_free_rate
    return excess_returns.mean() / (excess_returns.std() + 1e-8)

def max_drawdown(values: np.ndarray) -> float:
    """è®¡ç®—æœ€å¤§å›æ’¤"""
    peak = np.maximum.accumulate(values)
    drawdown = (values - peak) / peak
    return np.min(drawdown)
class SafetyController:
    def __init__(self, model, optimizer, config, logger):
        self.model = model
        self.optimizer = optimizer
        self.config = config['training']['stability']
        self.logger = logger
        self.epoch_backup_count = 0
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        self.backup_dir = Path(f"{config['paths']['log_file']}_safety_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        # å­¦ä¹ ç‡å¤‡ä»½
        self.original_lr = optimizer.param_groups[0]['lr']
    
    def protect_outputs(self, outputs):
        """æ¨¡å‹è¾“å‡ºé˜²æŠ¤"""
        if torch.isnan(outputs).any():
            self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œæ‰§è¡Œä¿®å¤...")
            outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1e4, neginf=-1e4)
          # ä¿®å¤NaN/Inféœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆä½¿ç”¨torch.whereå¯å¯¼ï¼‰
        outputs = torch.where(
       torch.isnan(outputs) | torch.isinf(outputs),
       torch.zeros_like(outputs),
       outputs
   )    
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        outputs = torch.clamp(outputs, 
                              min=self.config['output_clip_range'][0], 
                              max=self.config['output_clip_range'][1]) 
        return outputs
    
    def check_gradients(self):
        """æ¢¯åº¦å¥åº·æ£€æŸ¥å’Œä¿®å¤"""
        max_grad_norm = self.config.get('max_grad_norm', 1.0)
        problematic_layers = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad              
                # ä¿®å¤NaN/Infæ¢¯åº¦
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    problematic_layers.append(name)
                    param.grad = torch.where(
                        torch.isnan(grad) | torch.isinf(grad),
                        torch.zeros_like(grad),
                        grad
                    )            
                # æ¢¯åº¦è£å‰ª
                torch.clamp_(param.grad, -self.config['gradient_clip_value'], 
                            self.config['gradient_clip_value'])
        
        # å…¨å±€æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_grad_norm) 
        if problematic_layers:
            self.logger.warning(f"ä¿®å¤æ¢¯åº¦å¼‚å¸¸å±‚: {problematic_layers[:3]}{'...' if len(problematic_layers)>3 else ''}")
        
        return len(problematic_layers) == 0
    
    def create_backup(self, epoch):
        """åˆ›å»ºå®‰å…¨æ¢å¤ç‚¹"""
        if epoch % self.config.get('backup_interval', 5) == 0:
            backup_path = self.backup_dir / f"epoch_{epoch}_safety_backup.pt"
            torch.save({
                'model_state': self.model.state_dict(),
                'optimizer_state': self.optimizer.state_dict(),
                'epoch': epoch
            }, backup_path)
            
            # ä¿å­˜æœ€è¿‘çš„5ä¸ªå¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if len(backup_files) > 5:
                os.remove(backup_files[0])
    
    def recover(self, current_epoch):
        """ä»é”™è¯¯ä¸­æ¢å¤"""
        recovery_type = self.config.get('recovery_strategy', 'backoff')    
        if recovery_type == 'backoff':
            # å­¦ä¹ ç‡é€€é¿
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= self.config.get('lr_backoff_factor', 0.5)            
            self.logger.warning(f"é‡‡ç”¨å­¦ä¹ ç‡é€€é¿ç­–ç•¥ï¼Œæ–°å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")            
            # å°è¯•æ¢å¤æœ€è¿‘çš„å¤‡ä»½
            backup_files = sorted(self.backup_dir.glob("*.pt"), key=os.path.getmtime)
            if backup_files:
                latest_backup = backup_files[-1]
                checkpoint = torch.load(latest_backup)
                self.model.load_state_dict(checkpoint['model_state'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                self.logger.warning(f"ä»å¤‡ä»½æ¢å¤: {latest_backup.name} (epoch {checkpoint['epoch']})")
                return checkpoint['epoch']  # è¿”å›æ¢å¤åˆ°çš„epoch
            
        elif recovery_type == 'reset':
            # å®Œå…¨é‡ç½®æ¨¡å‹
            self.logger.error("æ‰§è¡Œæ¨¡å‹å®Œå…¨é‡ç½®!")            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            for module in self.model.modules():
                if hasattr(module, 'reset_parameters'):
                    module.reset_parameters()            
            # é‡ç½®ä¼˜åŒ–å™¨
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.original_lr            
            return 0  # ä»å¤´å¼€å§‹è®­ç»ƒ
        
        return current_epoch
    
    @contextmanager
    def safe_forward_context(self):
        """å®‰å…¨å‰å‘ä¼ æ’­ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # å¯ç”¨PyTorchå¼‚å¸¸æ£€æµ‹
            torch.autograd.set_detect_anomaly(True)
            yield
        except RuntimeError as e:
            self.logger.error(f"å‰å‘ä¼ æ’­å¼‚å¸¸: {str(e)}")
            # æå–å¼‚å¸¸ä½ç½®
            stack = inspect.stack()
            caller = stack[1]  # è°ƒç”¨safe_forward_contextçš„å‡½æ•°
            self.logger.error(f"å¼‚å¸¸ä½ç½®: {caller.filename}:{caller.lineno}")
            raise
        finally:
            torch.autograd.set_detect_anomaly(False)

# === æ–°å¢: æ¢¯åº¦ç›‘æ§å™¨ ===
class GradientMonitor:
    def __init__(self, model, layers_to_watch=None):
        self.model = model
        self.layers = layers_to_watch or self._identify_critical_layers()
        self.handles = []
        self.logger = logging.getLogger("gradient_monitor")
        self.reset_stats()
    
    def _identify_critical_layers(self):
        """è¯†åˆ«å…³é”®å±‚ï¼ˆæœ€åä¸€å±‚å’Œå«LayerNormçš„å±‚ï¼‰"""
        critical_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
                critical_layers.append(name)
            if 'out' in name or 'final' in name or 'proj' in name:
                critical_layers.append(name)
        return list(set(critical_layers))
    
    def reset_stats(self):
        self.max_grad = 0.0
        self.max_grad_layer = None
        self.nan_count = 0
    
    def _hook_fn(self, module, grad_input, grad_output, layer_name):
        # æ£€æŸ¥NaNå’ŒInf
        for g in grad_output:
            if g is not None:
                if torch.isnan(g).any():
                    self.nan_count += torch.isnan(g).sum().item()
                if torch.isinf(g).any():
                    self.nan_count += torch.isinf(g).sum().item()        
        # è®°å½•æœ€å¤§æ¢¯åº¦
        for g in grad_output:
            if g is not None:
                grad_norm = torch.norm(g)
                if grad_norm > self.max_grad:
                    self.max_grad = grad_norm.item()
                    self.max_grad_layer = layer_name
    
    def attach(self):
        """é™„åŠ æ¢¯åº¦é’©å­"""
        self.reset_stats()
        for name in self.layers:
            module = self._get_module(name)
            hook = lambda m, gi, go, n=name: self._hook_fn(m, gi, go, n)
            handle = module.register_full_backward_hook(hook)
            self.handles.append(handle)
    
    def detach(self):
        """ç§»é™¤æ¢¯åº¦é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles = []
    
    def _get_module(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å—"""
        names = name.split('.')
        module = self.model
        for n in names:
            module = getattr(module, n)
        return module
    
    def report(self, batch_idx, epoch):
        """ç”Ÿæˆæ¢¯åº¦æŠ¥å‘Š"""
        report = f"æ¢¯åº¦ç›‘æ§ - Epoch {epoch} Batch {batch_idx}:\n"
        report += f"  ğŸš© æœ€å¤§æ¢¯åº¦: {self.max_grad:.2e} ({self.max_grad_layer})\n"
        report += f"  âš ï¸ å¼‚å¸¸æ¢¯åº¦è®¡æ•°: {self.nan_count}"
        
        if self.nan_count > 0:
            self.logger.warning(report)
        elif batch_idx % 10 == 0:
            self.logger.info(report)
# === æ–°å¢: SmoothL1æŸå¤±å‡½æ•° ===
class RLoss(nn.Module):

    def __init__(self, supervised_criterion=None, sup_weight=0.5, entropy_coef=0.01, use_baseline=True):
        super().__init__()
        self.supervised_criterion = supervised_criterion  # e.g. SafeSmoothL1Loss()
        self.sup_weight = sup_weight                      # ç›‘ç£é¡¹æƒé‡ Î»_sup
        self.entropy_coef = entropy_coef                  # ç†µç³»æ•° Î²
        self.use_baseline = use_baseline

        # ç®€å•çš„ç§»åŠ¨å¹³å‡ baselineï¼ˆä¹Ÿå¯ä»¥åœ¨å¤–é¢ç®¡ç†ï¼‰
        self.register_buffer("baseline", torch.tensor(0.0))

    @torch.no_grad()
    def _update_baseline(self, reward, momentum=0.99):
        # æ ‡é‡ baselineï¼Œç”¨æ‰¹æ¬¡å‡å€¼åš EMA
        batch_mean = reward.mean()
        self.baseline = momentum * self.baseline + (1 - momentum) * batch_mean

    def forward(self, model_outputs, targets, reward):
        """
        è¿”å›ä¸€ä¸ª dict:
          total_loss, policy_loss, supervised_loss, entropy, weight, mean_reward
        """
        logits = model_outputs.get("logits", None)
        if logits is None:
            raise ValueError("model_outputs éœ€è¦åŒ…å« 'logits'ï¼ˆç­–ç•¥å¤´è¾“å‡º [B,K]ï¼‰ã€‚")

        # ========= ç­–ç•¥ï¼šç¦»æ•£åŠ¨ä½œ =========
        if logits.dim() == 1:
            logits = logits.unsqueeze(0)  # å…¼å®¹ [K]
        probs = F.softmax(logits, dim=-1)                 # [B,K]
        dist  = torch.distributions.Categorical(probs=probs)
        actions = dist.sample()                            # [B]
        logp = dist.log_prob(actions)                      # [B]
        entropy = dist.entropy().mean()                    # æ ‡é‡

        # ========= baseline & advantage =========
        if self.use_baseline:
            # è®­ç»ƒåˆæœŸ baseline æ˜¯ 0ï¼Œé€æ­¥æ›´æ–°
            self._update_baseline(reward)
            advantage = reward - self.baseline             # [B]
        else:
            # é€€ä¸€æ­¥ï¼Œç”¨ batch å†…å‡å€¼åšä¸­å¿ƒåŒ–
            advantage = reward - reward.mean()

        # æ³¨æ„: advantage ä¸éœ€è¦æ¢¯åº¦
        advantage = advantage.detach()

        # ========= ç­–ç•¥æŸå¤± =========
        policy_loss = -(advantage * logp).mean() - self.entropy_coef * entropy

        # ========= ç›‘ç£é¡¹ï¼ˆå¯é€‰ï¼‰=========
        supervised_loss = torch.tensor(0.0, device=logits.device)
        if (self.supervised_criterion is not None) and ("regression" in model_outputs):
            y1_pred = model_outputs["regression"].view(-1)   # [B]
            supervised_loss = self.supervised_criterion(y1_pred.float(), targets.float())

        # ========= åŠ¨æ€æƒé‡ï¼ˆå¦‚æœä½ å–œæ¬¢ç”¨ä½ çš„æ³¢åŠ¨ç‡é€»è¾‘ï¼‰=========
        # ä¹Ÿå¯ä»¥å›ºå®š self.sup_weight ä¸å˜
        try:
            volatility = torch.abs(targets).mean()
            current_weight = float(torch.clamp(0.65 - volatility * 25, 0.2, 0.7).item())
        except Exception:
            current_weight = self.sup_weight

        total_loss = current_weight * supervised_loss + (1 - current_weight) * policy_loss

        return {
            "total_loss": total_loss,
            "policy_loss": policy_loss,
            "supervised_loss": supervised_loss,
            "entropy": entropy,
            "weight": current_weight,
            "mean_reward": reward.mean()
        }
class SafeSmoothL1Loss(nn.Module):
    """ y_pred, y_true -> æ ‡é‡ loss """
    def __init__(self, beta=1.0):
        super().__init__()
        self.beta = beta

    def forward(self, y_pred, y_true):
        # éƒ½æ˜¯ [B] æˆ– [B,1]
        y_pred = torch.clamp(y_pred, -50, 50)
        y_true = torch.clamp(y_true, -50, 50)

        diff = y_pred - y_true
        diff = torch.where(torch.isfinite(diff), diff, torch.zeros_like(diff))

        abs_diff = torch.abs(diff)
        mask = abs_diff < self.beta

        l2 = 0.5 * (diff ** 2) / self.beta
        l1 = abs_diff - 0.5 * self.beta
        loss = torch.where(mask, l2, l1).mean()

        if not torch.isfinite(loss):
            return torch.zeros((), device=y_pred.device)
        return loss
class CSVDataset(Dataset):
    """
    æ–‡æœ¬ç®¡çº¿ç‰ˆï¼šè¯»å–é¢„å¤„ç†åçš„ CSVï¼ˆå« text/target/dateï¼‰ï¼Œ
    åœ¨ __getitem__ ä¸­ç”¨ tokenizer æŠŠ text -> encï¼ˆinput_ids ç­‰ï¼‰ï¼Œè¿”å› (enc, y, date)
    """
    def __init__(
        self,
        data_path: str,
        tokenizer: PreTrainedTokenizerBase,
        text_col: str = "text",
        target_col: str = "target",
        max_length: int = 128,
        drop_na_target: bool = True,
        logger: Optional[logging.Logger] = None,
    ):
        super().__init__()
        self.logger = logger or logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            # é¿å…é‡å¤ handler
            h = logging.StreamHandler()
            h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
            self.logger.addHandler(h)
            self.logger.setLevel(logging.INFO)

        self.data_path = data_path
        self.tokenizer = tokenizer
        self.text_col = text_col
        self.target_col = target_col
        self.max_length = max_length

        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

        # === è¯»å– CSV ===
        try:
            df = pd.read_csv(data_path)
        except Exception as e:
            raise RuntimeError(f"è¯»å– CSV å¤±è´¥: {data_path} | {e}")

        # æœ‰äº›å¯¼å‡ºä¼šå‡ºç°â€œåªæœ‰ä¸€åˆ— 0â€çš„æƒ…å†µï¼Œè¿™é‡Œåšä¸€æ¬¡å…œåº•
        if df.shape[1] == 1 and df.columns[0] in ("0", "Unnamed: 0"):
            # å¦‚æœæ˜¯è¯¯æŠŠ index æˆ–æ•´è¡Œä¸²è¿›äº†ä¸€åˆ—ï¼Œå°½åŠ›æ¢å¤ï¼›å¦åˆ™ç»™å‡ºæ¸…æ™°æŠ¥é”™
            self.logger.warning(f"æ£€æµ‹åˆ°å•åˆ— CSVï¼ˆåˆ—å={df.columns.tolist()}ï¼‰ï¼Œ"
                                f"è¯·ç¡®è®¤ {data_path} æ˜¯å¦ä¸ºå¸¦è¡¨å¤´çš„æ­£è§„ CSVï¼ˆtext/target/dateï¼‰")
            # ç»§ç»­æŒ‰ä¸€åˆ—å¤„ç†ï¼Œä½†æ²¡æœ‰ text/target ä»ä¼šåœ¨åç»­æŠ›é”™

        # === æ–‡æœ¬åˆ—ï¼šä¼˜å…ˆä½¿ç”¨ textï¼Œå…¶æ¬¡è‡ªåŠ¨å¯»æ‰¾/æ‹¼æ¥å¸¸è§å­—æ®µ ===
        self.text_col = self._pick_or_build_text_column(df, prefer=self.text_col)

        # === ç›®æ ‡åˆ—ï¼šå¿…é¡»è½¬æˆæ•°å€¼ ===
        if self.target_col not in df.columns:
            raise ValueError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ°ç›®æ ‡åˆ—ï¼š{self.target_col}ï¼Œç°æœ‰åˆ—={df.columns.tolist()}")

        df[self.target_col] = pd.to_numeric(df[self.target_col], errors="coerce")
        if drop_na_target:
            before = len(df)
            df = df.dropna(subset=[self.target_col]).reset_index(drop=True)
            dropped = before - len(df)
            if dropped > 0:
                self.logger.info(f"ä¸¢å¼ƒæ— æ•ˆç›®æ ‡è¡Œ: {dropped}")

        # æ–‡æœ¬è½¬å­—ç¬¦ä¸²ï¼ˆç¼ºå¤±ç½®ç©ºï¼‰
        df[self.text_col] = df[self.text_col].fillna("").astype(str)

        # æ—¥æœŸå¯é€‰
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")

        self.df = df.reset_index(drop=True)
        self.logger.info(f"âœ… åŠ è½½å®Œæˆ: æ ·æœ¬æ•°={len(self.df)} | æ–‡æœ¬åˆ—={self.text_col} | ç›®æ ‡åˆ—={self.target_col}")

    # ---------- å†…éƒ¨å·¥å…· ----------

    def _pick_or_build_text_column(self, df: pd.DataFrame, prefer: str) -> str:
        """
        é€‰å‡ºæ–‡æœ¬åˆ—ã€‚å¦‚æœæ²¡æœ‰ preferï¼Œå°±å°è¯•å¸¸è§å‘½åï¼›
        è‹¥ä»æ²¡æœ‰ï¼Œå°è¯•ä»å¤šä¸ªå­—æ®µæ‹¼æ¥ä¸€ä¸ª 'text' åˆ—ï¼›éƒ½æ²¡æœ‰åˆ™æŠ¥é”™ï¼ˆæç¤ºç°æœ‰åˆ—ï¼‰
        """
        if prefer in df.columns:
            return prefer

        # å¸¸è§çš„æ–‡æœ¬å­—æ®µ
        common_text = ["text", "tweet", "content", "message", "body"]
        for c in common_text:
            if c in df.columns:
                if c != "text":
                    # ç»Ÿä¸€å« textï¼Œé¿å…åç»­æ··ä¹±
                    df.rename(columns={c: "text"}, inplace=True)
                return "text"

        # å°è¯•ä»å¤šä¸ªå­—æ®µæ‹¼æ¥ä¸€ä¸ª textï¼ˆé€‚é…ä½ ä¹‹å‰çš„æ•°æ®å¯èƒ½å¸¦æœ‰ mentions/hashtags/url ç­‰ï¼‰
        candidates = [c for c in df.columns if c.lower().startswith("text") or c.lower().startswith("tweet")]
        parts = []
        for c in candidates:
            if df[c].dtype == object:
                parts.append(df[c].astype(str))
        # ä¸€äº›å¯èƒ½çš„è¡¥å……å­—æ®µ
        for c in ["screen_name", "user_name", "symbols", "hashtags"]:
            if c in df.columns and df[c].dtype == object:
                parts.append(df[c].astype(str))

        if parts:
            df["text"] = ""
            for p in parts:
                df["text"] = (df["text"] + " " + p).str.strip()
            # ä¸‡ä¸€å…¨ç©ºï¼Œä»ç„¶æŠ¥é”™
            if (df["text"].fillna("").str.len() > 0).any():
                self.logger.info(f"æœªæ‰¾åˆ° '{prefer}'ï¼Œå·²ä» {len(parts)} ä¸ªå­—æ®µæ‹¼æ¥ç”Ÿæˆ 'text'")
                return "text"

        # èµ°åˆ°è¿™é‡Œè¯´æ˜çœŸçš„æ²¡æœ‰ä»»ä½•æ–‡æœ¬å¯ç”¨
        raise ValueError(
            f"æ•°æ®ä¸­æ‰¾ä¸åˆ°æ–‡æœ¬åˆ—ï¼ˆå°è¯•äº† '{prefer}', {['text','tweet','content','message','body']}ï¼Œä»¥åŠå‰ç¼€ text*/tweet* æ‹¼æ¥ï¼‰ã€‚"
            f"ç°æœ‰åˆ—={df.columns.tolist()}ã€‚è¯·æ£€æŸ¥é¢„å¤„ç†è„šæœ¬æ˜¯å¦æŒ‰çº¦å®šå¯¼å‡º text/target åˆ—ã€‚"
        )

    def __len__(self) -> int:
        return len(self.df)

    def __getitem__(self, idx: int):
        row = self.df.iloc[idx]
        text   = str(row[self.text_col])       # æ–‡æœ¬
        target = float(row[self.target_col])   # çœŸå®æ ‡ç­¾ï¼ˆä½ æ˜¯è‚¡ä»·/æ”¶ç›Šï¼‰
        date   = str(row["date"]) if "date" in row and pd.notna(row["date"]) else ""
        return text, target, date
def make_collate_fn(tokenizer, max_length=128):
    def collate(batch):
        texts, ys, dates = zip(*batch)  # æ¥è‡ª CSVDataset çš„ (text, y, date)
        enc = tokenizer(
            list(texts),
            return_tensors="pt",
            padding="longest",       # æˆ– "max_length"
            truncation=True,
            max_length=max_length,
        )
        y = torch.tensor(ys, dtype=torch.float32)
        return enc, y, list(dates)
    return collate

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def compute_reward(model2_result, targets,lookback=5):
    """æ¨¡æ‹Ÿå®é™…äº¤æ˜“æ•ˆæœ å¥–åŠ±"""
    # è·å–æ¨¡å‹é¢„æµ‹åºåˆ—
    predictions = model2_result[:, :lookback]
    # æ„å»ºæ¨¡æ‹ŸæŒä»“
    portfolio = torch.zeros_like(targets)
    total_return = torch.zeros_like(targets)
    
    # åŸºäºè¿ç»­é¢„æµ‹çš„ä»“ä½å˜åŒ–
    for t in range(1, lookback):
        # åŸºäºé¢„æµ‹ä¿¡å·è°ƒæ•´ä»“ä½
        position_change = predictions[:, t] - predictions[:, t-1]
        portfolio += position_change
        
        # è®¡ç®—æ¯æ—¥æ”¶ç›Š
        daily_return = portfolio * targets[:, t]
        total_return += daily_return
        
        # é£é™©ç®¡ç†: å¼ºåˆ¶å¹³ä»“è§„åˆ™
        portfolio = torch.where(torch.abs(portfolio) > 2, 
                                torch.clamp(portfolio, -1, 1), 
                                portfolio)
    
    # æœ€ç»ˆç»„åˆä»·å€¼ä½œä¸ºå¥–åŠ±
    final_value = 1 + total_return
    reward = final_value - 1  # ç»„åˆæ”¶ç›Šä½œä¸ºå¥–åŠ±
    return reward
class SafetyModule:
    """
    è®­ç»ƒç¨³å®šæ€§ä¿æŠ¤ï¼š
    - create_backup: å¤‡ä»½ LoRA/æ¨¡å‹æƒé‡
    - check_inputs: å…¼å®¹ dict(tensor) / tensorï¼Œæ£€æŸ¥ NaN/Inf/None
    - protect_outputs: ç®€å• logits è£å‰ªï¼Œé¿å…æç«¯æ•°å€¼
    - safe_forward_context: å‰å‘å®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰æ‹©æ˜¯å¦æŠ›å‡ºå¼‚å¸¸ï¼‰
    - check_gradients: æ¸…æ´— NaN/Inf æ¢¯åº¦ +ï¼ˆå¯é€‰ï¼‰é€å…ƒç´ è£å‰ª + å…¨å±€èŒƒæ•°è£å‰ª
    """
    def __init__(self, backup_dir="./backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)

    # --------- å¤‡ä»½ ----------
    def create_backup(self, model, epoch, save_lora_only=True):
        """
        è‹¥æ˜¯ PEFT/LoRA åŒ…è£…çš„æ¨¡å‹ï¼Œmodel.save_pretrained() ä¼šåªä¿å­˜ LoRA é€‚é…å™¨æƒé‡ï¼ˆå–å†³äºåº“ç‰ˆæœ¬ï¼‰ã€‚
        å¦‚æœä½ æƒ³ä¿å­˜å®Œæ•´æ¨¡å‹ï¼Œå¯æ”¹ä¸º torch.save(model.state_dict(), path/xxx.pt)
        """
        try:
            backup_path = self.backup_dir / f"lora_backup_epoch_{epoch}"
            model.save_pretrained(backup_path)
            print(f"ğŸ’¾ LoRA æƒé‡å·²å¤‡ä»½åˆ°: {backup_path}")
        except Exception as e:
            print(f"âš ï¸ å¤‡ä»½å¤±è´¥: {e}")

    # --------- è¾“å…¥æ£€æŸ¥ ----------
    def _is_tensor_bad(self, t: torch.Tensor) -> bool:
        try:
            if not torch.is_tensor(t):
                return True
            if not torch.isfinite(t).all():
                return True
            return False
        except Exception:
            return True

    def _sanitize_tensor_(self, t: torch.Tensor) -> torch.Tensor:
        """
        åŸåœ°æ¸…ç† NaN/Inf -> 0ï¼Œå¹¶é™åˆ¶æç«¯å€¼ï¼ˆé˜²æ­¢æ¢¯åº¦/æ•°å€¼çˆ†ç‚¸ï¼‰ã€‚
        """
        if not torch.is_tensor(t):
            return t
        # å°†éæœ‰é™å€¼ç½®é›¶
        mask = ~torch.isfinite(t)
        if mask.any():
            t[mask] = 0.0
        # å¯é€‰ï¼šå¯¹å¼‚å¸¸å¤§å€¼è¿›è¡Œç¡¬è£å‰ªï¼ˆæ•°å€¼å¯æŒ‰éœ€è°ƒæ•´ï¼‰
        t.clamp_(min=-1e6, max=1e6)
        return t

    def check_inputs(self, inputs, targets):
        """
        æ£€æŸ¥è¾“å…¥æ˜¯å¦å®‰å…¨ï¼Œæ”¯æŒï¼š
          - inputs: Tensor æˆ– dict[str, Tensor]ï¼ˆä¾‹å¦‚ tokenizer çš„è¾“å‡ºï¼‰
          - targets: Tensor
        è¿”å› True è¡¨ç¤ºå®‰å…¨ï¼ŒFalse è¡¨ç¤ºä¸å®‰å…¨ï¼›è‹¥å‘ç°é—®é¢˜ä¼šå°è¯•åŸåœ°ä¿®å¤ã€‚
        """
        try:
            if inputs is None or targets is None:
                print("âš ï¸ inputs æˆ– targets ä¸º None")
                return False

            bad = False
            # å¤„ç† inputs
            if isinstance(inputs, dict):
                for k, v in inputs.items():
                    if not torch.is_tensor(v):
                        print(f"âš ï¸ inputs['{k}'] ä¸æ˜¯å¼ é‡")
                        bad = True
                        continue
                    if self._is_tensor_bad(v):
                        print(f"âš ï¸ inputs['{k}'] å« NaN/Infï¼Œå·²ä¿®å¤")
                        self._sanitize_tensor_(v)
                        bad = True  # æ ‡è®°å‘ç°è¿‡é—®é¢˜ï¼Œä½†å·²ä¿®å¤
            elif torch.is_tensor(inputs):
                if self._is_tensor_bad(inputs):
                    print("âš ï¸ inputs å« NaN/Infï¼Œå·²ä¿®å¤")
                    self._sanitize_tensor_(inputs)
                    bad = True
            else:
                print("âš ï¸ inputs ç±»å‹å¼‚å¸¸ï¼Œæ—¢ä¸æ˜¯ dict ä¹Ÿä¸æ˜¯ tensor")
                return False

            # å¤„ç† targets
            if not torch.is_tensor(targets):
                print("âš ï¸ targets ä¸æ˜¯å¼ é‡")
                return False
            if self._is_tensor_bad(targets):
                print("âš ï¸ targets å« NaN/Infï¼Œå·²ä¿®å¤")
                self._sanitize_tensor_(targets)
                bad = True

            # å‘ç°é—®é¢˜ä½†å·²ä¿®å¤ï¼Œä»ç„¶å…è®¸ç»§ç»­è®­ç»ƒ
            return True

        except Exception as e:
            print(f"âš ï¸ check_inputs å¼‚å¸¸: {e}")
            return False

    # --------- è¾“å‡ºä¿æŠ¤ ----------
    def protect_outputs(self, outputs):
        """
        å¯¹æ¨¡å‹è¾“å‡ºï¼ˆlogits/è¿ç»­å€¼ï¼‰åšä¿åº•è£å‰ªï¼Œé¿å…æç«¯æ•°å€¼ã€‚
        æ”¯æŒ tensor æˆ– dict[str, tensor]
        """
        try:
            if isinstance(outputs, dict):
                new_out = {}
                for k, v in outputs.items():
                    if torch.is_tensor(v):
                        v = torch.clamp(v, -1e4, 1e4)
                        v = torch.nan_to_num(v, nan=0.0, posinf=1e4, neginf=-1e4)
                    new_out[k] = v
                return new_out
            elif torch.is_tensor(outputs):
                v = torch.clamp(outputs, -1e4, 1e4)
                v = torch.nan_to_num(v, nan=0.0, posinf=1e4, neginf=-1e4)
                return v
            else:
                return outputs
        except Exception as e:
            print(f"âš ï¸ protect_outputs å¼‚å¸¸: {e}")
            return outputs

    # --------- å‰å‘å®‰å…¨ä¸Šä¸‹æ–‡ ----------
    @contextmanager
    def safe_forward_context(self, rethrow: bool = True):
        """
        ç”¨æ³•ï¼š
            with safety.safe_forward_context():
                outputs = model(**enc)

        rethrow=Trueï¼šè®°å½•é”™è¯¯åé‡æ–°æŠ›å‡ºï¼Œä¾¿äºä¸Šå±‚é€»è¾‘ä¸­æ–­å¹¶è¿›å…¥ except åˆ†æ”¯ã€‚
        rethrow=Falseï¼šä»…æ‰“å°é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œï¼ˆä¸æ¨èï¼Œå®¹æ˜“äº§ç”Ÿæœªå®šä¹‰å˜é‡ï¼‰ã€‚
        """
        try:
            yield
        except RuntimeError as e:
            print(f"âš ï¸ å‰å‘ä¼ æ’­å‡ºé”™: {e}")
            if rethrow:
                raise
        except Exception as e:
            print(f"âš ï¸ å‰å‘ä¼ æ’­æœªçŸ¥å¼‚å¸¸: {e}")
            if rethrow:
                raise

    # --------- æ¢¯åº¦æ£€æŸ¥ï¼ˆå…³é”®è¡¥å……ï¼‰ ----------
    def check_gradients(self, model: nn.Module, max_grad_norm: float = 1.0, clip_value: float | None = None):
        """
        åœ¨ loss.backward() ä¹‹åã€optimizer.step() ä¹‹å‰è°ƒç”¨ï¼š
            safety.check_gradients(model, max_grad_norm, clip_value)

        åŠŸèƒ½ï¼š
        - å°†å‚æ•°æ¢¯åº¦ä¸­çš„ NaN/Inf ç½®é›¶
        - å¯é€‰ï¼šé€å…ƒç´ è£å‰ªåˆ° [-clip_value, clip_value]
        - å…¨å±€èŒƒæ•°è£å‰ªåˆ° max_grad_norm
        """
        try:
            # 1) æ¸…ç† NaN/Inf
            for name, p in model.named_parameters():
                if p.grad is None:
                    continue
                g = p.grad
                # éæœ‰é™å€¼ â†’ 0
                mask = ~torch.isfinite(g)
                if mask.any():
                    g[mask] = 0.0
                # å¯é€‰ï¼šé€å…ƒç´ è£å‰ª
                if clip_value is not None:
                    g.clamp_(min=-float(clip_value), max=float(clip_value))

            # 2) å…¨å±€èŒƒæ•°è£å‰ª
            if max_grad_norm is not None and max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)

            return True

        except Exception as e:
            print(f"âš ï¸ check_gradients å¼‚å¸¸: {e}")
            return False
def _tofloat(x):
    import torch
    return x.item() if isinstance(x, torch.Tensor) else float(x)
def _to_model_inputs(enc, device):
    # å…è®¸ enc æ˜¯ BatchEncoding æˆ– dict
    if isinstance(enc, BatchEncoding):
        enc = dict(enc)
    elif not isinstance(enc, dict):
        raise TypeError(f"enc åº”æ˜¯ dict/BatchEncodingï¼Œå®é™…æ˜¯ {type(enc)}")

    if "input_ids" not in enc:
        raise KeyError(f"enc ç¼ºå°‘ 'input_ids'ï¼Œç°æœ‰é”®: {list(enc.keys())}")

    # embedding éœ€è¦ long/int
    if enc["input_ids"].dtype != torch.long:
        enc["input_ids"] = enc["input_ids"].long()
    if "attention_mask" in enc and enc["attention_mask"].dtype != torch.long:
        enc["attention_mask"] = enc["attention_mask"].long()

    # æ¬åˆ°è®¾å¤‡
    return {k: v.to(device, non_blocking=True) for k, v in enc.items()}
class TextGenerator:
    """
    ä»…ç”¨äºâ€œæ–‡æœ¬ç”Ÿæˆæ‹¿å¥–åŠ±â€ï¼Œä¸åä¼ æ¢¯åº¦ã€‚
    ä» model1_dir åŠ è½½ Causal LMï¼ˆæœ¬åœ°ï¼‰ï¼Œå¸¸é©»å†…å­˜ï¼Œé¿å…é¢‘ç¹åŠ è½½ã€‚
    """
    def __init__(self, model_dir: str, device: torch.device, max_length: int = 128):
        self.device = device
        self.max_length = max_length

        self.tok = AutoTokenizer.from_pretrained(
            model_dir, use_fast=True, trust_remote_code=True, local_files_only=True
        )
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token or "<|pad|>"

        # åªåšæ¨ç†ï¼Œä½¿ç”¨ Causal LM
        self.lm = AutoModelForCausalLM.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=None,              # FP32 ç¨³å®š
            attn_implementation="eager",   # é¿å… sdpa çš„æ»‘çª— warning/æ–­è¨€
        ).to(device).eval()

    @torch.no_grad()
    def generate(
        self,
        prompts: list[str],
        *,
        max_new_tokens: int = 32,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_p: float = 1.0,
    ) -> list[str]:
        enc = self.tok(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length,
        ).to(self.device)

        gen = self.lm.generate(
            **enc,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=self.tok.pad_token_id,
            eos_token_id=self.tok.eos_token_id,
        )
        # è¿™é‡Œç›´æ¥è§£ç å…¨éƒ¨ï¼ˆåŒ…å« promptï¼‰ï¼Œå¦‚æœä½ åªæƒ³è¦æ–°å¢éƒ¨åˆ†ï¼Œå¯ä»¥é¢å¤–æˆªå–
        return self.tok.batch_decode(gen, skip_special_tokens=True, clean_up_tokenization_spaces=True)

def main(config_path: str):
    # ===== 0. åŸºæœ¬å‡†å¤‡ =====
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    config_loader = ConfigLoader(config_path)
    train_cfg = config_loader.config
    train_cfg['training']['stability'] = train_cfg.get('stability', {
        'max_nan_allowed': 0,
        'output_clip_range': [-5.0, 5.0],
        'max_grad_norm': 1.0,
        'gradient_clip_value': 10000.0,
        'add_output_noise': True,
        'noise_std': 0.01,
        'backup_interval': 5,
        'recovery_strategy': 'backoff',
        'lr_backoff_factor': 0.5
    })

    # æ—¥å¿— & ç›®å½•
    log_file_from_cfg = train_cfg['paths']['log_file'] 
    logger = setup_logger(
        name="training",
        log_file=log_file_from_cfg,
        console_level=logging.WARNING,   # ç»ˆç«¯å¹²å‡€
        file_level=logging.DEBUG,        # æ–‡ä»¶é‡Œå…¨é‡
        also_timestamp_file=True
    )
    # åŠ è½½ YAML é…ç½®
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œè®­ç»ƒè„šæœ¬")
    logger.info(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    create_output_directories(train_cfg)

    # --- è·¯å¾„æ”¹ä¸º Path.exists()ï¼ˆå¿…è¦ä¿®æ­£ï¼‰ ---
    train_dir = Path(train_cfg["paths"]["train_data_path"]).resolve()
    val_dir   = Path(train_cfg["paths"]["val_data_path"]).resolve()
    test_dir  = Path(train_cfg["paths"]["test_data_path"]).resolve()
    if not train_dir.exists(): raise RuntimeError(f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼š{train_dir}")
    if not test_dir.exists():  raise RuntimeError(f"æµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼š{test_dir}")

    # ===== 1. tokenizerï¼ˆåœ¨ DataLoader å‰ï¼‰ =====
    model1_dir = Path(train_cfg["paths"]["model1_dir"]).resolve()
    tokenizer = AutoTokenizer.from_pretrained(
        model1_dir,
        use_fast=True,
        trust_remote_code=True,
        local_files_only=True    # â­ å¼ºåˆ¶åªåŠ è½½æœ¬åœ°
    )
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            tokenizer.add_special_tokens({"pad_token": "[PAD]"})

    # ===== 2. æ„å»º base æ¨¡å‹ + LoRAï¼ˆåªåšä¸€æ¬¡ï¼‰=====
    hf_config = AutoConfig.from_pretrained(model1_dir / "config.json")
    base_model = AutoModelForSequenceClassification.from_config(hf_config)

    peft_config = LoraConfig(
        task_type="SEQ_CLS",
        r=8,
        lora_alpha=16,
        lora_dropout=0.1
    )
    model = get_peft_model(base_model, peft_config).to(device)

    # æ˜¾å­˜å‹å¥½è®¾ç½®ï¼ˆå¿…è¦ä¸”åªå½±å“è®­ç»ƒæœŸç¼“å­˜/æ¿€æ´»ï¼‰
    if hasattr(model, "config"):
        try:
            model.config.use_cache = False
        except Exception:
            pass
    try:
        model.gradient_checkpointing_enable()
    except Exception:
        pass

    # åŒæ­¥ embed è¯è¡¨å¤§å° + pad/eos
    model.resize_token_embeddings(len(tokenizer))
    model.config.pad_token_id = tokenizer.pad_token_id
    if getattr(model.config, "eos_token_id", None) is None and tokenizer.eos_token_id is not None:
        model.config.eos_token_id = tokenizer.eos_token_id

    # ï¼ˆå¯é€‰ï¼‰åŠ è½½å·²æœ‰æƒé‡ï¼ˆLoRA æˆ–å…¨é‡ï¼‰ï¼ŒæŒ‰éœ€ä¿ç•™ï¼š
    weights_path = Path(train_cfg["paths"]["output_model"]).resolve()
    if weights_path.exists():
        state_dict = torch.load(weights_path, map_location=device)
        if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
            state_dict = state_dict['model_state_dict']
        model.load_state_dict(state_dict, strict=False)
        print("âœ… model1æƒé‡åŠ è½½å®Œæˆ")
    else:
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å·²æœ‰æƒé‡ï¼š{weights_path}ï¼Œå°†ä»åˆå§‹åŒ–å‚æ•°å¼€å§‹è®­ç»ƒ")

    # ===== 3. æ„å»ºæ•°æ®é›† & DataLoaderï¼ˆä½¿ç”¨ CSVDataset + collateï¼‰ =====
    logger.info(f"ğŸ” åŠ è½½è®­ç»ƒæ•°æ®é›†: {train_dir}")
    train_dataset = CSVDataset(
        data_path=train_dir,
        tokenizer=tokenizer,
        text_col="text",
        target_col="target",
        max_length=train_cfg['training'].get('max_length', 128),
    )
    logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")

    val_dataset = None
    if val_dir.exists():
        logger.info(f"ğŸ” åŠ è½½éªŒè¯æ•°æ®é›†: {val_dir}")
        val_dataset = CSVDataset(
            data_path=val_dir,
            tokenizer=tokenizer,
            text_col="text",
            target_col="target",
            max_length=train_cfg['training'].get('max_length', 128),
        )
        logger.info(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")

    # collate_fn
    collate = make_collate_fn(tokenizer, max_length=train_cfg['training'].get('max_length', 128))
    # ------- Debug å¼€å…³ï¼ˆä¸æ”¹é…ç½®ï¼Œç”¨ä»£ç æ§åˆ¶ï¼‰-------
    DEBUG_SMALL_TRAIN = True      # æƒ³å…¨é‡è®­ç»ƒå°±æ”¹æˆ False
    DEBUG_TRAIN_SAMPLES = 100     # è®­ç»ƒé›†åªå–å‰ 100 æ¡
    DEBUG_MAX_BATCHES = 50        # æ¯ä¸ª epoch æœ€å¤šè·‘ 50 ä¸ª batchï¼›ä¸é™åˆ¶å°±è®¾ None

    # ------- è®­ç»ƒ DataLoader -------
    train_ds_for_loader = train_dataset
    if DEBUG_SMALL_TRAIN:
        n = min(DEBUG_TRAIN_SAMPLES, len(train_dataset))
        train_ds_for_loader = Subset(train_dataset, range(n))
        print(f"âš ï¸ Debug æ¨¡å¼ï¼šä»…ä½¿ç”¨å‰ {n} æ¡è®­ç»ƒæ ·æœ¬")

    train_loader = DataLoader(
        train_ds_for_loader,
        batch_size=train_cfg['training'].get('batch_size', 16),
        shuffle=True,
        num_workers=0,           # å…ˆè®¾ 0ï¼Œç¨³å®šåå†è°ƒå¤§
        pin_memory=True,
        collate_fn=collate,
    )
    val_loader = None
    if val_dataset is not None:
        val_loader = DataLoader(
            val_dataset,
            batch_size=train_cfg['training'].get('batch_size', 16),
            shuffle=False,
            num_workers=0,
            pin_memory=True,
            collate_fn=collate
        )

    hidden_size = model.config.hidden_size
    model1_dtype = next(model.parameters()).dtype
    reg_head = nn.Linear(hidden_size, 1, device=device, dtype=model1_dtype)

    # ===== 4. æ¨¡å‹2ï¼ˆLoRA æ¨ç†å™¨ï¼‰ =====
    safety = SafetyModule()   # ä½ è‡ªå·±å®ç°çš„å¸¦ check_inputs/protect çš„ç‰ˆæœ¬
    try:
        model2 = DeepSeekInfer()     # ä½ è‡ªå·±çš„ç±»
        model2.model.to(device)
        print("åŠ è½½çš„ model2 ç±»å‹:", type(model2))
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½ model2: {e}")

    # ===== 4.1 å¸¸é©»â€œç”Ÿæˆç”¨â€æ¨¡å‹ä¸€ï¼ˆCausal LMï¼‰â€”â€” ä»…ç”¨äºå¥–åŠ± =====
    text_gen = TextGenerator(
        model_dir=model1_dir,
        device=device,
        max_length=train_cfg['training'].get('max_length', 128)
    )

    # ===== 5. ä¼˜åŒ–å™¨ & æŸå¤± =====
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=float(train_cfg['training']['learning_rate']),
                                 eps=1e-6)
    # AMP: ä»…åœ¨ FP16 æ—¶å¯ç”¨ GradScaler
    scaler = torch.cuda.amp.GradScaler(enabled=USE_SCALER)

    max_grad_norm = train_cfg['training'].get('max_grad_norm', 1.0)
    loss_fn = RLoss(
        supervised_criterion=SafeSmoothL1Loss(beta=1.0).to(device),
        sup_weight=float(train_cfg['training'].get('base_loss_weight', 0.5)),
        entropy_coef=0.01,
        use_baseline=True
    ).to(device)

    grad_monitor = GradientMonitor(model)
    grad_monitor.attach()
    torch.autograd.set_detect_anomaly(True)
    
    # ===== 6. è®­ç»ƒå¾ªç¯ï¼ˆåªç”¨ encï¼Œä¸è¦ç”¨æœªå®šä¹‰çš„ inputsï¼‰=====
    best_val_loss = float("inf")
    for epoch in range(train_cfg['training']['epochs']): 
        model.train()
        epoch_loss = 0.0
        nan_batch_count = 0
        valid_batch_count = 0
        print(f"\nã€è°ƒè¯•ã€‘å¼€å§‹ç¬¬ {epoch+1} è½®è®­ç»ƒ")
        model.train()
        reg_head.train()

        for batch_idx, (enc, targets, date) in enumerate(train_loader):
            enc = _to_model_inputs(enc, device)
            targets = targets.to(device, dtype=torch.float32, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)
            
            # 1) æ¨¡å‹ä¸€å‰å‘ï¼ˆAMPï¼‰
            with torch.cuda.amp.autocast(enabled=USE_AUTOCAST, dtype=AMP_DTYPE):
                outputs = model(**enc, output_hidden_states=True)        # AutoModelForSequenceClassification
                policy_logits = outputs.logits                            # [B, K]
                last_hidden = outputs.hidden_states[-1]                   # [B, L, H]
                cls_vec = last_hidden[:, 0, :].to(reg_head.weight.dtype) # [B, H]
                reg_pred = reg_head(cls_vec).squeeze(-1)                  # [B]

            # 3) æ¨¡å‹äºŒï¼šåªç”¨äºäº§ç”Ÿ rewardï¼Œä¸å›ä¼ æ¢¯åº¦ï¼ˆæ–‡æœ¬â†’æ–‡æœ¬é“¾è·¯ï¼‰
            with torch.no_grad():
                batch_texts = tokenizer.batch_decode(
                    enc["input_ids"],
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                gen_texts = text_gen.generate(
                    prompts=batch_texts,
                    max_new_tokens=32,     # Debug å¯é€‚å½“å‡å°ä»¥åŠ é€Ÿ
                    do_sample=False,
                    temperature=1.0,
                    top_p=1.0,
                )
                model2_out = model2.predict(gen_texts)                # Tensor [B] æˆ– [B,1]
                model2_out = model2_out.view(-1).to(device)

                direction_match = (torch.sign(model2_out) == torch.sign(targets)).float()
                reward = compute_reward(model2_out, targets)
                match_rate = direction_match.mean().item()

            # 4) è®¡ç®—æŸå¤±ï¼ˆAMPï¼‰
            with torch.cuda.amp.autocast(enabled=USE_AUTOCAST, dtype=AMP_DTYPE):
                loss_dict = loss_fn(
                    model_outputs={
                        "logits": policy_logits,
                        "regression": reg_pred
                    },
                    targets=targets,
                    reward=reward
                )
                loss = loss_dict["total_loss"]

            # 5) åå‘ä¼ æ’­ + ä¼˜åŒ–ï¼ˆåŒºåˆ† FP16/BF16ï¼‰
            if USE_SCALER:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                optimizer.step()

            epoch_loss += loss.item()
            valid_batch_count += 1

            if (batch_idx + 1) % 50 == 0:
                logger.info(
                    f"[Epoch {epoch+1} | Batch {batch_idx+1}] "
                    f"loss={_tofloat(loss):.4f} | "
                    f"sup={_tofloat(loss_dict['supervised_loss']):.4f} | "
                    f"rl={_tofloat(loss_dict['policy_loss']):.4f} | "
                    f"match={match_rate:.4f}"
                )

            avg_loss = epoch_loss / valid_batch_count if valid_batch_count > 0 else float('nan')
            logger.info(f"Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")
            
            # è¾“å‡ºæ¯å±‚æ¢¯åº¦æœ€å¤§å€¼
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_max = param.grad.abs().max().item()
                    logger.info(f"å±‚ {name} æ¢¯åº¦æœ€å¤§å€¼: {grad_max}")
            
            # âœ… åˆ é™¤äº†å¤šä½™çš„ç¬¬äºŒæ¬¡ stepï¼ˆå¿…è¦ä¿®æ­£ï¼‰
            safety.check_gradients(
                model,
                max_grad_norm=train_cfg['training'].get('max_grad_norm', 1.0),
                clip_value=None,
            )
            
            batch_reward = reward.mean().item()

            # æ¯10æ‰¹æ¬¡æŠ¥å‘Šæ¢¯åº¦çŠ¶æ€
            if batch_idx % 10 == 0:
                grad_monitor.report(batch_idx, epoch)

            # è‹¥è®¾ç½®äº† DEBUG_MAX_BATCHESï¼Œå¯æå‰è·³å‡º
            if DEBUG_MAX_BATCHES is not None and (batch_idx + 1) >= DEBUG_MAX_BATCHES:
                logger.info(f"âš ï¸ Debugï¼šæœ¬è½®æå‰åœ¨ {DEBUG_MAX_BATCHES} ä¸ª batch å¤„åœæ­¢")
                break

        logger.info(f"ğŸ Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")

        if val_loader:
            model.eval()
            reg_head.eval()
            val_loss_sum = 0.0
            val_steps = 0
            with torch.no_grad():
                for val_enc, val_targets, _ in val_loader:
                    val_enc = _to_model_inputs(val_enc, device)
                    val_targets = val_targets.to(device, dtype=torch.float32, non_blocking=True).view(-1)

                    val_outputs = model(**val_enc, output_hidden_states=True)
                    last_hidden = val_outputs.hidden_states[-1]   # [B, L, H]
                    cls_vec = last_hidden[:, 0, :]                # [B, H]
                    reg_pred = reg_head(cls_vec).squeeze(-1)      # [B]

                    sup_loss = loss_fn.supervised_criterion(
                        reg_pred.to(torch.float32),
                        val_targets.to(torch.float32)
                    )

                    val_loss_sum += sup_loss.item()
                    val_steps += 1

            avg_val_loss = val_loss_sum / max(1, val_steps)
            logger.info(f"Epoch {epoch+1} éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

            # ===== ä¿å­˜æœ€ä¼˜ =====
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                save_path = Path(train_cfg['paths']['output_model'])
                torch.save(model.state_dict(), save_path)
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {save_path} (val_loss: {avg_val_loss:.4f})")
            else:
                logger.info(f"å½“å‰éªŒè¯æŸå¤± {avg_val_loss:.4f} â‰¥ æœ€ä½³ {best_val_loss:.4f}ï¼Œä¸ä¿å­˜")

            model.train()
            reg_head.train()

    # ç§»é™¤æ¢¯åº¦ç›‘æ§é’©å­
    grad_monitor.detach()

    # æŠ¥å‘Šå½“å‰epochçŠ¶æ€
    if valid_batch_count > 0:
        avg_loss = epoch_loss / valid_batch_count
        logger.info(f"ğŸ Epoch å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.6f} | è·³è¿‡æ‰¹æ¬¡: {nan_batch_count}")
    else:
        logger.error(f"â›” æœ¬è½®æ²¡æœ‰æœ‰æ•ˆè®­ç»ƒæ‰¹æ¬¡ï¼Œå°è¯•æ¢å¤...")
        recovered_epoch = safety.recover(epoch)
        logger.warning(f"æ¢å¤è‡³epoch {recovered_epoch}")

    model_save_path = Path(train_cfg['paths']['output_model']) 
    torch.save(model.state_dict(), model_save_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")

    # ===== æµ‹è¯•å’Œè¯„ä¼°é˜¶æ®µï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ =====
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ...")
    print("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ...")
    test_loader = None
    test_src = train_cfg['paths'].get('test_data_path')
    if not test_src:
        logger.info("âš ï¸ æœªé…ç½® test_dirï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
        print("âš ï¸ æœªé…ç½® test_dirï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
    else:
        if isinstance(test_src, dict) and 'test' in test_src:
            test_files = test_src['test']
        elif isinstance(test_src, (list, tuple)):
            test_files = list(test_src)
        else:
            test_files = [test_src]

        test_files = [p for p in test_files if os.path.exists(p)]
        if len(test_files) == 0:
            logger.warning("âš ï¸ test_dir ä¸­æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
            test_loader = None
        else:
            logger.info(f"ğŸ§ª æµ‹è¯•æ–‡ä»¶æ•°: {len(test_files)}")
            test_datasets = []
            for tf in test_files:
                try:
                    ds = CSVDataset(
                        data_path=tf,
                        tokenizer=tokenizer,
                        text_col="text",
                        target_col="target",
                        max_length=train_cfg['training'].get('max_length', 128),
                        logger=logging.getLogger("CSVDataset")
                    )
                    logger.info(f"  âœ” è½½å…¥æµ‹è¯•é›†: {tf} | æ ·æœ¬={len(ds)}")
                    test_datasets.append(ds)
                except Exception as e:
                    logger.error(f"  âŒ åŠ è½½æµ‹è¯•æ–‡ä»¶å¤±è´¥: {tf} | {e}", exc_info=True)

            if len(test_datasets) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®é›†ï¼Œè·³è¿‡æµ‹è¯•é˜¶æ®µ")
                test_loader = None
            else:
                if len(test_datasets) > 1:
                    final_test_dataset = torch.utils.data.ConcatDataset(test_datasets)
                    logger.info(f"âœ… åˆå¹¶æµ‹è¯•é›†å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(final_test_dataset)}")
                else:
                    final_test_dataset = test_datasets[0]

                test_loader = DataLoader(
                    final_test_dataset,
                    batch_size=train_cfg['training'].get('batch_size', 16),
                    shuffle=False,
                    num_workers=train_cfg['training'].get('num_workers', 0),
                    pin_memory=True,
                    collate_fn=collate
                )

    if test_loader is not None:
        model.eval()
        reg_head.eval()
        all_preds, all_targets, all_dates = [], [], []

        with torch.no_grad():
            for bidx, (enc, targets, dates) in enumerate(test_loader):
                enc = _to_model_inputs(enc, device)
                targets = targets.to(device, dtype=torch.float32, non_blocking=True)

                outputs = model(**enc, output_hidden_states=True)
                last_hidden = outputs.hidden_states[-1]      # [B, L, H]
                cls_vec = last_hidden[:, 0, :]
                cls_vec = cls_vec.to(reg_head.weight.dtype)
                reg_pred = reg_head(cls_vec).squeeze(-1)     # [B]

                all_preds.extend(reg_pred.detach().cpu().tolist())
                all_targets.extend(targets.detach().cpu().tolist())
                if isinstance(dates, (list, tuple)):
                    all_dates.extend([str(d) if d is not None else "" for d in dates])
                else:
                    try:
                        all_dates.extend(list(dates))
                    except Exception:
                        all_dates.extend([""] * len(reg_pred))

                if (bidx + 1) % 100 == 0:
                    logger.info(f"ğŸ”„ æµ‹è¯•è¿›åº¦: {bidx+1}/{len(test_loader)} æ‰¹æ¬¡")

        if len(all_targets) > 0:
            preds_np = np.asarray(all_preds, dtype=np.float32)
            tgs_np   = np.asarray(all_targets, dtype=np.float32)

            mae  = float(np.mean(np.abs(preds_np - tgs_np)))
            rmse = float(np.sqrt(np.mean((preds_np - tgs_np) ** 2)))

            pred_sign = np.sign(preds_np)
            tg_sign   = np.sign(tgs_np)
            valid_idx = (pred_sign != 0) & (tg_sign != 0)
            dir_acc   = float(np.mean(pred_sign[valid_idx] == tg_sign[valid_idx])) if valid_idx.any() else float('nan')

            logger.info("ğŸ“Š æµ‹è¯•æŒ‡æ ‡ï¼š")
            logger.info(f"  â€¢ MAE  = {mae:.6f}")
            logger.info(f"  â€¢ RMSE = {rmse:.6f}")
            logger.info(f"  â€¢ æ–¹å‘å‡†ç¡®ç‡ = {dir_acc*100 if np.isfinite(dir_acc) else float('nan'):.2f}%")

            out_df = pd.DataFrame({
                "date": all_dates[:len(all_preds)],
                "prediction": all_preds,
                "target": all_targets
            })
            csv_path = save_structured_data(
                out_df,
                train_cfg,
                filename=f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            )
            logger.info(f"ğŸ’¾ å·²ä¿å­˜æµ‹è¯•é¢„æµ‹åˆ°: {csv_path}")

            eval_dir = Path(train_cfg['paths'].get('eval_output_dir', 'results/model1_eval'))
            eval_dir.mkdir(parents=True, exist_ok=True)
            eval_path = eval_dir / f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(eval_path, "w") as f:
                json.dump(convert_to_serializable({
                    "MAE": mae,
                    "RMSE": rmse,
                    "Direction_Accuracy": dir_acc,
                    "num_samples": len(all_targets),
                    "test_files": test_files
                }), f, indent=4)
            logger.info(f"ğŸ“ å·²ä¿å­˜è¯„ä¼°æ‘˜è¦åˆ°: {eval_path}")
        else:
            logger.warning("âš ï¸ æµ‹è¯•é˜¶æ®µæœªäº§ç”Ÿæœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡æŒ‡æ ‡ä¸æ–‡ä»¶ä¿å­˜")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="RLäº¤æ˜“ç³»ç»Ÿè®­ç»ƒè„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument("--config", 
                        type=str, 
                        default="configs/model1.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: configs/model1.yaml")
    
    args = parser.parse_args()
    main(args.config)
