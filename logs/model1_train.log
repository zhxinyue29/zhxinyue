2025-05-29 13:21:51,512 - INFO - Loading model2 for inference...
2025-05-29 13:34:00,456 - INFO - Loading model2 for inference...
2025-05-29 18:07:13,819 - INFO - Loading model2 for inference...
2025-05-29 18:17:03,446 - INFO - Loading model2 for inference...
2025-05-29 18:18:42,921 - INFO - Loading model2 for inference...
2025-05-29 18:19:20,345 - INFO - Loading model2 for inference...
2025-05-29 18:22:18,988 - INFO - Loading model2 for inference...
2025-05-29 19:04:13,796 - INFO - Loading model2 for inference...
2025-05-29 19:10:18,419 - INFO - Loading model2 for inference...
2025-05-30 08:39:50,153 - INFO - Loading model2 for inference...
2025-06-04 12:26:09,352 - INFO - 使用设备: cuda
2025-06-04 12:26:09,412 - INFO - 设置随机种子: 42
2025-06-04 12:45:40,277 - INFO - 使用设备: cuda
2025-06-04 12:45:40,278 - INFO - 设置随机种子: 42
2025-06-04 12:55:38,088 - INFO - 使用设备: cuda
2025-06-04 12:55:38,089 - INFO - 设置随机种子: 42
2025-06-04 13:04:01,285 - INFO - 开始运行训练脚本
2025-06-04 13:19:09,630 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 13:19:09,661 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 13:19:09,661 - training - INFO - 📁 所有输出目录已创建
2025-06-04 13:19:31,544 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 13:25:27,024 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 13:25:27,043 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 13:25:27,043 - training - INFO - 📁 所有输出目录已创建
2025-06-04 13:25:34,548 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 13:26:10,578 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 13:26:10,888 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 13:26:13,469 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 13:29:24,305 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 13:29:24,305 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 13:29:24,305 - training - INFO - 📁 所有输出目录已创建
2025-06-04 13:29:31,068 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 13:29:33,841 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 13:29:34,150 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 13:29:35,738 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 13:29:58,491 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 13:29:58,491 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 13:29:58,491 - training - INFO - 📁 所有输出目录已创建
2025-06-04 13:30:05,256 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 13:30:08,051 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 13:30:08,361 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 13:30:09,957 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 13:30:09,957 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 13:30:09,957 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 13:30:22,642 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 13:30:22,900 - training - ERROR - 加载数据出错: "['target'] not found in axis"
2025-06-04 17:53:11,759 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 17:53:11,759 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 17:53:11,759 - training - INFO - 📁 所有输出目录已创建
2025-06-04 17:53:18,489 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 17:53:21,282 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 17:53:21,589 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 17:53:23,147 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 17:53:23,147 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 17:53:23,148 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 17:53:30,959 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 17:53:30,960 - training - ERROR - 加载数据出错: "['target'] not found in axis"
2025-06-04 18:05:47,780 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 18:05:47,780 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 18:05:47,780 - training - INFO - 📁 所有输出目录已创建
2025-06-04 18:05:54,543 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 18:05:57,336 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 18:05:57,645 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 18:05:59,227 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 18:05:59,227 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 18:05:59,227 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 18:06:06,937 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 18:06:06,938 - training - ERROR - 加载数据出错: "['target'] not found in axis"
2025-06-04 18:11:51,609 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 18:11:51,609 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 18:11:51,610 - training - INFO - 📁 所有输出目录已创建
2025-06-04 18:11:58,337 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 18:12:01,120 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 18:12:01,426 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 18:12:02,987 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 18:12:02,987 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 18:12:02,987 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 18:12:10,747 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 18:12:12,220 - training - ERROR - 加载数据出错: ' Close'
2025-06-04 18:14:28,600 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 18:14:28,600 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 18:14:28,600 - training - INFO - 📁 所有输出目录已创建
2025-06-04 18:14:35,347 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 18:14:38,133 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 18:14:38,440 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 18:14:40,006 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 18:14:40,006 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 18:14:40,006 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 18:14:47,883 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 18:17:07,188 - training - INFO - 🚀 开始运行训练脚本
2025-06-04 18:17:07,188 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-04 18:17:07,188 - training - INFO - 📁 所有输出目录已创建
2025-06-04 18:17:13,939 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-04 18:17:16,737 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-04 18:17:17,044 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-04 18:17:18,609 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-04 18:17:18,609 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-04 18:17:18,610 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 18:17:26,355 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 18:17:47,005 - training - INFO - 📦 加载测试数据: data/processed/merged_data.csv
2025-06-04 18:17:47,005 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-04 18:17:54,589 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-04 18:19:58,294 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-05 08:40:48,627 - training - INFO - 🚀 开始运行训练脚本
2025-06-05 08:40:48,627 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-05 08:40:48,627 - training - INFO - 📁 所有输出目录已创建
2025-06-05 08:40:55,476 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-05 08:41:43,260 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-05 08:41:43,573 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-05 08:41:45,187 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-05 08:41:45,187 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-05 08:41:45,187 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 08:41:52,992 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-05 08:42:13,704 - training - INFO - 📦 加载测试数据: data/processed/merged_data.csv
2025-06-05 08:42:13,704 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 08:42:21,286 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-05 08:43:04,070 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-05 11:55:38,720 - training - INFO - 🚀 开始运行训练脚本
2025-06-05 11:55:38,720 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-05 11:55:38,720 - training - INFO - 📁 所有输出目录已创建
2025-06-05 11:55:45,614 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-05 11:55:48,412 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-05 11:55:48,721 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-05 11:55:50,604 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-05 11:55:50,604 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-05 11:55:50,604 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 11:55:58,876 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-05 11:56:20,825 - training - INFO - 📦 加载测试数据: data/processed/merged_data.csv
2025-06-05 11:56:20,825 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 11:56:28,455 - training - INFO - 数据加载成功, 形状: (8410659, 8)
2025-06-05 11:58:06,683 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-05 17:51:48,345 - training - INFO - 🚀 开始运行训练脚本
2025-06-05 17:51:48,366 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-05 17:51:48,367 - training - INFO - 📁 所有输出目录已创建
2025-06-05 17:51:55,272 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-05 17:51:58,129 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-05 17:51:58,443 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-05 17:52:00,430 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-05 17:52:00,430 - training - INFO - 📦 加载训练数据: data/processed/merged_data.csv
2025-06-05 17:52:00,430 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 17:52:00,432 - training - INFO - 数据加载成功, 形状: (174, 8)
2025-06-05 17:52:00,434 - training - INFO - 📦 加载测试数据: data/processed/merged_data.csv
2025-06-05 17:52:00,434 - training - INFO - 📊 加载数据: data/processed/merged_data.csv
2025-06-05 17:52:00,435 - training - INFO - 数据加载成功, 形状: (174, 8)
2025-06-05 17:52:00,994 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-09 12:25:51,821 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 12:25:51,863 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 12:25:51,863 - training - INFO - 📁 所有输出目录已创建
2025-06-09 12:25:58,771 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-09 12:26:02,018 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-09 12:26:02,343 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-09 12:26:04,127 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-09 12:26:04,127 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train
2025-06-09 12:26:04,127 - training - INFO - 📊 加载数据: /home/liyakun/twitter-stock-prediction/data/splits/train
2025-06-09 12:26:04,140 - training - ERROR - 加载数据出错: [Errno 21] Is a directory: '/home/liyakun/twitter-stock-prediction/data/splits/train'
2025-06-09 12:38:53,292 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 12:38:53,292 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 12:38:53,292 - training - INFO - 📁 所有输出目录已创建
2025-06-09 12:39:00,040 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-09 12:39:02,965 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-09 12:39:03,274 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-09 12:39:04,821 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-09 12:59:38,109 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 12:59:38,109 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 12:59:38,110 - training - INFO - 📁 所有输出目录已创建
2025-06-09 12:59:44,889 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-09 12:59:47,789 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-09 12:59:48,096 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-09 12:59:49,655 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-09 12:59:49,660 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-09 13:06:29,210 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 13:06:29,210 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 13:06:29,210 - training - INFO - 📁 所有输出目录已创建
2025-06-09 13:06:36,009 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-09 13:06:38,883 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-09 13:06:39,192 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-09 13:06:40,755 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-09 13:06:40,760 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-09 13:06:41,390 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-09 13:23:43,248 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 13:23:43,248 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 13:23:43,248 - training - INFO - 📁 所有输出目录已创建
2025-06-09 13:28:09,345 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 13:28:09,345 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 13:28:09,345 - training - INFO - 📁 所有输出目录已创建
2025-06-09 13:36:06,474 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 13:36:06,474 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 13:36:06,474 - training - INFO - 📁 所有输出目录已创建
2025-06-09 13:39:40,193 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 13:39:40,193 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 13:39:40,193 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:16:48,900 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:16:48,900 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:16:48,900 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:18:58,266 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:18:58,266 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:18:58,266 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:19:44,850 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:19:44,850 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:19:44,850 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:20:15,291 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:20:15,291 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:20:15,292 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:25:47,974 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:25:47,974 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:25:47,974 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:28:31,406 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:28:31,406 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:28:31,406 - training - INFO - 📁 所有输出目录已创建
2025-06-09 18:28:50,207 - training - INFO - 🚀 开始运行训练脚本
2025-06-09 18:28:50,207 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-09 18:28:50,207 - training - INFO - 📁 所有输出目录已创建
2025-06-10 08:44:17,737 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 08:44:17,737 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 08:44:17,737 - training - INFO - 📁 所有输出目录已创建
2025-06-10 08:49:08,541 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 08:49:08,541 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 08:49:08,541 - training - INFO - 📁 所有输出目录已创建
2025-06-10 08:49:14,398 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 08:49:17,025 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 08:49:17,336 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=64, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 08:49:18,681 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 08:49:18,921 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 08:49:19,528 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 08:52:02,491 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 08:52:02,491 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 08:52:02,491 - training - INFO - 📁 所有输出目录已创建
2025-06-10 08:52:08,366 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 08:52:11,270 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 08:52:11,578 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=64, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 08:52:12,927 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 08:52:12,933 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 08:52:13,473 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 11:58:39,972 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 11:58:39,972 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 11:58:39,972 - training - INFO - 📁 所有输出目录已创建
2025-06-10 11:58:45,807 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 11:58:48,532 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 11:58:48,838 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 11:58:50,185 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 11:58:50,191 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 11:58:50,728 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:10:33,651 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:10:33,651 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:10:33,651 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:10:39,467 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:10:42,189 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:10:42,494 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 12:10:43,839 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:10:43,845 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:10:44,393 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:25:38,148 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:25:38,148 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:25:38,148 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:25:43,979 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:25:46,700 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:25:47,004 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 12:25:48,344 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:25:48,350 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:25:48,891 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:31:27,534 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:31:27,534 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:31:27,534 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:31:33,363 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:31:36,085 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:31:36,390 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=10, bias=True)
)
2025-06-10 12:31:37,734 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:31:37,739 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:31:38,276 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:34:51,715 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:34:51,715 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:34:51,715 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:34:57,547 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:35:00,267 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:35:00,572 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=1, bias=True)
)
2025-06-10 12:35:01,907 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:35:01,913 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:35:02,448 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:39:38,847 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:39:38,847 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:39:38,847 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:39:44,688 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:39:47,457 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:39:47,768 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=1, bias=True)
)
2025-06-10 12:39:49,110 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:39:49,116 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:39:49,655 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:41:43,420 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:41:43,420 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:41:43,420 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:41:49,251 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:41:51,969 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:41:52,276 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1536, bias=True)
  (layers): ModuleList(
    (0-31): 32 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
      )
      (linear1): Linear(in_features=1536, out_features=11008, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=11008, out_features=1536, bias=True)
      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1536, out_features=1, bias=True)
)
2025-06-10 12:41:53,668 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:41:53,673 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:41:54,216 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:42:47,182 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 12:42:47,183 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 12:42:47,183 - training - INFO - 📁 所有输出目录已创建
2025-06-10 12:42:48,169 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 12:42:50,757 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 12:42:51,061 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-10 12:42:51,374 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 12:42:51,379 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 12:42:51,919 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 12:42:52,839 - training - INFO - 🏁 训练轮次 1/50, 平均损失: nan
2025-06-10 12:42:53,397 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-10 12:42:53,955 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-10 12:42:54,507 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-10 12:42:55,059 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-10 12:42:55,611 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-10 12:42:56,166 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-10 12:42:56,717 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-10 12:42:57,291 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-10 12:42:57,842 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-10 12:42:58,395 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-10 12:42:58,945 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-10 12:42:59,504 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-10 12:43:00,059 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-10 12:43:00,609 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-10 12:43:01,161 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-10 12:43:01,714 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-10 12:43:02,269 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-10 12:43:02,818 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-10 12:43:03,369 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-10 12:43:03,930 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-10 12:43:04,486 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-10 12:43:05,045 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-10 12:43:05,596 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-10 12:43:06,149 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-10 12:43:06,701 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-10 12:43:07,260 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-10 12:43:07,815 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-10 12:43:08,370 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-10 12:43:08,922 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-10 12:43:09,479 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-10 12:43:10,038 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-10 12:43:10,602 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-10 12:43:11,153 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-10 12:43:11,709 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-10 12:43:12,267 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-10 12:43:12,819 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-10 12:43:13,373 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-10 12:43:13,928 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-10 12:43:14,485 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-10 12:43:15,052 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-10 12:43:15,604 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-10 12:43:16,159 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-10 12:43:16,714 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-10 12:43:17,271 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-10 12:43:17,826 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-10 12:43:18,380 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-10 12:43:18,937 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-10 12:43:19,494 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-10 12:43:20,070 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-10 18:28:40,748 - training - INFO - 🚀 开始运行训练脚本
2025-06-10 18:28:40,748 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-10 18:28:40,748 - training - INFO - 📁 所有输出目录已创建
2025-06-10 18:28:41,736 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-10 18:28:44,336 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-10 18:28:44,642 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-10 18:28:44,947 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-10 18:28:44,951 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-10 18:28:45,489 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-10 18:28:46,440 - training - INFO - 🏁 训练轮次 1/50, 平均损失: nan
2025-06-10 18:28:47,016 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-10 18:28:47,594 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-10 18:28:48,192 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-10 18:28:48,774 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-10 18:28:49,352 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-10 18:28:49,935 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-10 18:28:50,514 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-10 18:28:51,094 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-10 18:28:51,670 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-10 18:28:52,250 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-10 18:28:52,832 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-10 18:28:53,412 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-10 18:28:54,006 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-10 18:28:54,589 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-10 18:28:55,172 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-10 18:28:55,749 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-10 18:28:56,329 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-10 18:28:56,908 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-10 18:28:57,488 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-10 18:28:58,070 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-10 18:28:58,648 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-10 18:28:59,234 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-10 18:28:59,817 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-10 18:29:00,400 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-10 18:29:00,979 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-10 18:29:01,561 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-10 18:29:02,150 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-10 18:29:02,735 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-10 18:29:03,317 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-10 18:29:03,899 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-10 18:29:04,480 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-10 18:29:05,070 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-10 18:29:05,653 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-10 18:29:06,237 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-10 18:29:06,817 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-10 18:29:07,403 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-10 18:29:07,985 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-10 18:29:08,568 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-10 18:29:09,150 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-10 18:29:09,732 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-10 18:29:10,322 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-10 18:29:10,948 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-10 18:29:11,549 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-10 18:29:12,172 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-10 18:29:12,763 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-10 18:29:13,380 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-10 18:29:13,965 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-10 18:29:14,551 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-10 18:29:15,133 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-11 08:38:30,966 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 08:38:30,966 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 08:38:30,966 - training - INFO - 📁 所有输出目录已创建
2025-06-11 08:42:12,942 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 08:42:12,942 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 08:42:12,942 - training - INFO - 📁 所有输出目录已创建
2025-06-11 08:48:08,903 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 08:48:08,903 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 08:48:08,903 - training - INFO - 📁 所有输出目录已创建
2025-06-11 11:57:52,019 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 11:57:52,019 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 11:57:52,019 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:21:27,593 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:21:27,593 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:21:27,593 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:27:36,226 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:27:36,226 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:27:36,226 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:35:34,270 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:35:34,270 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:35:34,270 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:38:54,579 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:38:54,579 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:38:54,579 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:41:12,180 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:41:12,180 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:41:12,180 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:41:36,778 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:41:36,778 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:41:36,778 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:43:57,337 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:43:57,337 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:43:57,338 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:47:57,872 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:47:57,872 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:47:57,872 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:50:34,163 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:50:34,163 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:50:34,163 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:53:13,630 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 12:53:13,630 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 12:53:13,630 - training - INFO - 📁 所有输出目录已创建
2025-06-11 12:53:14,622 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 12:53:17,270 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 12:53:17,574 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 12:53:17,881 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 12:53:17,890 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 12:53:18,437 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 13:06:46,678 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 13:06:46,678 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 13:06:46,678 - training - INFO - 📁 所有输出目录已创建
2025-06-11 13:12:14,201 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 13:12:14,201 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 13:12:14,202 - training - INFO - 📁 所有输出目录已创建
2025-06-11 13:12:15,190 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 13:12:17,842 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 13:12:18,147 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 13:12:18,451 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 13:12:18,460 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 13:12:19,012 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 18:13:10,129 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 18:13:10,129 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 18:13:10,129 - training - INFO - 📁 所有输出目录已创建
2025-06-11 18:13:11,134 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 18:13:13,778 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 18:13:14,052 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 18:13:14,324 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 18:13:14,340 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 18:13:14,883 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 18:31:37,289 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 18:31:37,289 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 18:31:37,289 - training - INFO - 📁 所有输出目录已创建
2025-06-11 18:31:38,292 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 18:31:40,937 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 18:31:41,210 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 18:31:41,467 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 18:31:41,483 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 18:31:42,026 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 18:37:30,051 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 18:37:30,051 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 18:37:30,051 - training - INFO - 📁 所有输出目录已创建
2025-06-11 18:37:31,057 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 18:37:33,712 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 18:37:33,986 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 18:37:34,251 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 18:37:34,266 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 18:37:34,811 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 18:52:02,783 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 18:52:02,783 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 18:52:02,783 - training - INFO - 📁 所有输出目录已创建
2025-06-11 18:52:03,788 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 18:52:06,433 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 18:52:06,706 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 18:52:06,962 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 18:52:06,978 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 18:52:07,520 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-11 18:59:55,501 - training - INFO - 🚀 开始运行训练脚本
2025-06-11 18:59:55,501 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-11 18:59:55,501 - training - INFO - 📁 所有输出目录已创建
2025-06-11 18:59:56,508 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-11 18:59:59,160 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-11 18:59:59,434 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-11 18:59:59,695 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-11 18:59:59,711 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-11 19:00:00,251 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 08:46:14,069 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 08:46:14,069 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 08:46:14,069 - training - INFO - 📁 所有输出目录已创建
2025-06-12 08:46:15,081 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 08:46:17,743 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 08:46:18,019 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 08:46:18,273 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 08:46:18,288 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 08:46:18,828 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 11:50:31,270 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 11:50:31,271 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 11:50:31,271 - training - INFO - 📁 所有输出目录已创建
2025-06-12 11:50:32,275 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 11:50:34,946 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 11:50:35,219 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 11:50:35,475 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 11:50:35,490 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 11:50:36,029 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:05:51,391 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:05:51,391 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:05:51,391 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:05:52,396 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:05:55,070 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:05:55,343 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:05:55,610 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:05:55,626 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:05:56,165 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:09:21,046 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:09:21,046 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:09:21,046 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:09:22,047 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:09:24,715 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:09:24,989 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:09:25,264 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:09:25,280 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:09:25,823 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:09:26,277 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 60057522176.000000
2025-06-12 12:09:26,503 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-12 12:09:26,727 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-12 12:09:26,956 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-12 12:09:27,183 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-12 12:09:27,410 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-12 12:09:27,639 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-12 12:09:27,872 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-12 12:09:28,100 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-12 12:09:28,336 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-12 12:09:28,565 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-12 12:09:28,791 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-12 12:09:29,021 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-12 12:09:31,030 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-12 12:09:31,259 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-12 12:09:31,486 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-12 12:09:31,715 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-12 12:09:31,948 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-12 12:09:32,177 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-12 12:09:32,404 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-12 12:09:32,638 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-12 12:09:32,869 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-12 12:09:33,106 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-12 12:09:33,338 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-12 12:09:33,562 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-12 12:09:33,790 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-12 12:09:34,020 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-12 12:09:34,246 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-12 12:09:34,469 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-12 12:09:34,701 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-12 12:09:34,930 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-12 12:09:35,158 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-12 12:09:35,384 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-12 12:10:02,165 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-12 12:10:02,394 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-12 12:10:02,623 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-12 12:10:02,855 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-12 12:10:03,087 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-12 12:10:03,315 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-12 12:10:03,547 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-12 12:10:03,771 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-12 12:10:04,000 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-12 12:10:04,230 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-12 12:10:04,459 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-12 12:10:04,687 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-12 12:10:04,920 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-12 12:10:05,150 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-12 12:10:05,380 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-12 12:10:05,609 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-12 12:10:05,839 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-12 12:27:07,456 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:27:07,456 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:27:07,456 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:27:08,460 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:27:11,129 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:27:11,403 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:27:11,659 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:27:11,675 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:27:12,216 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:28:10,642 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:28:10,642 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:28:10,642 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:28:11,648 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:28:14,329 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:28:14,603 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:28:14,857 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:28:14,873 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:28:15,415 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:28:15,870 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 59859476480.000000
2025-06-12 12:28:16,097 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-12 12:28:16,333 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-12 12:28:16,567 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-12 12:28:16,799 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-12 12:28:17,030 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-12 12:28:17,258 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-12 12:28:17,486 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-12 12:28:17,721 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-12 12:28:17,949 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-12 12:28:18,179 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-12 12:28:18,412 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-12 12:28:18,647 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-12 12:28:18,878 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-12 12:28:19,111 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-12 12:28:19,338 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-12 12:28:19,573 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-12 12:28:19,809 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-12 12:28:20,046 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-12 12:28:20,282 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-12 12:28:20,512 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-12 12:28:20,744 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-12 12:28:20,976 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-12 12:28:21,213 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-12 12:28:21,446 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-12 12:28:21,676 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-12 12:28:21,914 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-12 12:28:22,151 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-12 12:28:22,383 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-12 12:28:22,615 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-12 12:28:22,843 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-12 12:28:23,085 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-12 12:28:23,315 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-12 12:28:26,857 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-12 12:28:27,095 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-12 12:28:27,327 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-12 12:28:27,557 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-12 12:28:27,787 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-12 12:28:28,018 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-12 12:28:28,258 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-12 12:28:28,492 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-12 12:28:28,725 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-12 12:28:28,956 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-12 12:28:29,186 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-12 12:28:29,418 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-12 12:28:29,651 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-12 12:28:29,888 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-12 12:28:30,896 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-12 12:28:31,125 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-12 12:28:31,356 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-12 12:33:45,545 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:33:45,545 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:33:45,545 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:33:46,550 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:33:49,218 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:33:49,493 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:33:49,748 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:33:49,764 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:33:50,298 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:33:50,759 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 57810026496.000000
2025-06-12 12:33:51,006 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-12 12:33:51,243 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-12 12:33:51,476 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-12 12:33:51,708 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-12 12:33:51,940 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-12 12:33:52,179 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-12 12:33:52,417 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-12 12:33:52,660 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-12 12:33:52,895 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-12 12:33:53,134 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-12 12:33:53,377 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-12 12:33:53,616 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-12 12:33:53,859 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-12 12:33:54,103 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-12 12:33:54,348 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-12 12:33:54,595 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-12 12:33:54,835 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-12 12:33:55,080 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-12 12:33:55,313 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-12 12:33:55,552 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-12 12:33:55,808 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-12 12:33:56,050 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-12 12:33:56,283 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-12 12:33:56,521 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-12 12:33:56,760 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-12 12:33:56,992 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-12 12:33:57,230 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-12 12:33:57,471 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-12 12:33:57,706 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-12 12:33:57,943 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-12 12:33:58,179 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-12 12:33:58,417 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-12 12:33:58,662 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-12 12:33:58,902 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-12 12:33:59,139 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-12 12:33:59,378 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-12 12:33:59,610 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-12 12:33:59,848 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-12 12:34:00,084 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-12 12:34:00,323 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-12 12:34:00,556 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-12 12:34:00,790 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-12 12:34:01,038 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-12 12:34:01,282 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-12 12:34:01,520 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-12 12:34:01,757 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-12 12:34:01,995 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-12 12:34:02,226 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-12 12:34:02,465 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-12 12:37:26,964 - training - INFO - 🚀 开始运行训练脚本
2025-06-12 12:37:26,964 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-12 12:37:26,964 - training - INFO - 📁 所有输出目录已创建
2025-06-12 12:37:27,971 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-12 12:37:30,650 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-12 12:37:30,932 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-12 12:37:31,189 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-12 12:37:31,205 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-12 12:37:31,763 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-12 12:37:32,214 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 61624205312.000000
2025-06-12 12:37:32,447 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-12 12:37:32,682 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-12 12:37:32,918 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-12 12:37:33,162 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-12 12:37:33,399 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-12 12:37:33,634 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-12 12:37:33,872 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-12 12:37:34,109 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-12 12:37:34,351 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-12 12:37:34,584 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-12 12:37:34,811 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-12 12:37:35,044 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-12 12:37:35,282 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-12 12:37:35,520 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-12 12:37:35,763 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-12 12:37:35,996 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-12 12:37:36,239 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-12 12:37:36,478 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-12 12:37:36,720 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-12 12:37:36,953 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-12 12:37:37,183 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-12 12:37:37,414 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-12 12:37:37,664 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-12 12:37:37,903 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-12 12:37:38,139 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-12 12:37:38,378 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-12 12:37:38,742 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-12 12:37:38,983 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-12 12:37:39,225 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-12 12:37:39,472 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-12 12:37:39,711 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-12 12:37:39,946 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-12 12:37:40,184 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-12 12:37:40,417 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-12 12:37:40,655 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-12 12:37:40,894 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-12 12:37:41,138 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-12 12:37:41,381 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-12 12:37:41,615 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-12 12:37:41,876 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-12 12:37:42,111 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-12 12:37:42,350 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-12 12:37:42,591 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-12 12:37:42,833 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-12 12:37:43,079 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-12 12:37:43,317 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-12 12:37:43,557 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-12 12:37:43,797 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-12 12:37:44,041 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-19 12:58:14,882 - training - INFO - 🚀 开始运行训练脚本
2025-06-19 12:58:14,905 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-19 12:58:14,905 - training - INFO - 📁 所有输出目录已创建
2025-06-19 12:58:16,302 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-19 12:58:49,895 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-19 12:58:50,346 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-19 12:58:53,707 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-19 12:58:53,889 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-19 12:59:04,359 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-19 12:59:12,583 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 61127741440.000000
2025-06-19 12:59:12,899 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-19 12:59:13,181 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-19 12:59:13,467 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-19 12:59:13,748 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-19 12:59:14,024 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-19 12:59:14,305 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-19 12:59:14,579 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-19 12:59:14,849 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-19 12:59:15,130 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-19 12:59:15,410 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-19 12:59:15,699 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-19 12:59:15,990 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-19 12:59:16,273 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-19 12:59:17,269 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-19 12:59:17,581 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-19 12:59:17,862 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-19 12:59:18,138 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-19 12:59:18,414 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-19 12:59:18,696 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-19 12:59:18,978 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-19 12:59:19,262 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-19 12:59:19,539 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-19 12:59:19,823 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-19 12:59:20,104 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-19 12:59:37,914 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-19 12:59:38,202 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-19 12:59:38,483 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-19 12:59:38,753 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-19 12:59:39,034 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-19 12:59:39,313 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-19 12:59:39,591 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-19 12:59:39,886 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-19 12:59:40,188 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-19 12:59:40,478 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-19 12:59:40,755 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-19 12:59:41,037 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-19 12:59:52,435 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-19 12:59:52,718 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-19 12:59:52,996 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-19 12:59:53,280 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-19 12:59:53,562 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-19 12:59:53,842 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-19 12:59:54,125 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-19 12:59:54,401 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-19 12:59:54,680 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-19 12:59:54,958 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-19 12:59:55,247 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-19 13:00:14,317 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-19 13:00:14,596 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-19 13:01:58,628 - training - INFO - 🚀 开始运行训练脚本
2025-06-19 13:01:58,628 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-19 13:01:58,629 - training - INFO - 📁 所有输出目录已创建
2025-06-19 13:01:59,648 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-19 13:02:21,501 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-19 13:02:21,802 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-19 13:02:22,268 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-19 13:02:22,410 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-19 13:02:23,244 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-19 13:02:23,824 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 61139406848.000000
2025-06-19 13:02:24,108 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-19 13:02:24,385 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-19 13:02:24,657 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-19 13:02:24,927 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-19 13:02:25,198 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-19 13:02:25,474 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-19 13:02:25,741 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-19 13:02:26,008 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-19 13:02:26,282 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-19 13:02:26,553 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-19 13:02:26,821 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-19 13:02:27,096 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-19 13:02:27,374 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-19 13:02:27,647 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-19 13:02:27,922 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-19 13:02:28,197 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-19 13:02:28,467 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-19 13:02:28,741 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-19 13:02:29,017 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-19 13:02:29,291 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-19 13:02:29,568 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-19 13:02:29,843 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-19 13:02:30,142 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-19 13:02:30,438 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-19 13:02:30,723 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-19 13:02:31,005 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-19 13:02:31,285 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-19 13:02:31,558 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-19 13:02:31,828 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-19 13:02:32,106 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-19 13:02:32,382 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-19 13:02:32,662 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-19 13:02:32,933 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-19 13:02:33,205 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-19 13:02:33,482 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-19 13:02:33,750 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-19 13:02:34,030 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-19 13:02:34,302 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-19 13:02:34,579 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-19 13:02:34,856 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-19 13:02:35,132 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-19 13:02:35,404 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-19 13:02:35,675 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-19 13:02:35,944 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-19 13:02:36,220 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-19 13:02:36,491 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-19 13:02:36,766 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-19 13:02:37,044 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-19 13:02:37,315 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-19 13:10:20,624 - training - INFO - 🚀 开始运行训练脚本
2025-06-19 13:10:20,625 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-19 13:10:20,625 - training - INFO - 📁 所有输出目录已创建
2025-06-19 13:10:21,639 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-19 13:10:51,920 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-19 13:10:52,254 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-19 13:10:52,536 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-19 13:10:52,734 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-19 13:10:58,805 - training - INFO - 🔥 开始训练, 共 10000 步
2025-06-19 13:10:59,590 - training - INFO - 🏁 训练轮次 1/50, 平均损失: 1794.787109
2025-06-19 13:10:59,927 - training - INFO - 🏁 训练轮次 2/50, 平均损失: nan
2025-06-19 13:11:00,193 - training - INFO - 🏁 训练轮次 3/50, 平均损失: nan
2025-06-19 13:11:00,455 - training - INFO - 🏁 训练轮次 4/50, 平均损失: nan
2025-06-19 13:11:00,714 - training - INFO - 🏁 训练轮次 5/50, 平均损失: nan
2025-06-19 13:11:00,979 - training - INFO - 🏁 训练轮次 6/50, 平均损失: nan
2025-06-19 13:11:01,250 - training - INFO - 🏁 训练轮次 7/50, 平均损失: nan
2025-06-19 13:11:01,520 - training - INFO - 🏁 训练轮次 8/50, 平均损失: nan
2025-06-19 13:11:01,787 - training - INFO - 🏁 训练轮次 9/50, 平均损失: nan
2025-06-19 13:11:02,054 - training - INFO - 🏁 训练轮次 10/50, 平均损失: nan
2025-06-19 13:11:02,318 - training - INFO - 🏁 训练轮次 11/50, 平均损失: nan
2025-06-19 13:11:02,576 - training - INFO - 🏁 训练轮次 12/50, 平均损失: nan
2025-06-19 13:11:02,843 - training - INFO - 🏁 训练轮次 13/50, 平均损失: nan
2025-06-19 13:11:03,105 - training - INFO - 🏁 训练轮次 14/50, 平均损失: nan
2025-06-19 13:11:03,364 - training - INFO - 🏁 训练轮次 15/50, 平均损失: nan
2025-06-19 13:11:03,627 - training - INFO - 🏁 训练轮次 16/50, 平均损失: nan
2025-06-19 13:11:03,883 - training - INFO - 🏁 训练轮次 17/50, 平均损失: nan
2025-06-19 13:11:04,151 - training - INFO - 🏁 训练轮次 18/50, 平均损失: nan
2025-06-19 13:11:04,413 - training - INFO - 🏁 训练轮次 19/50, 平均损失: nan
2025-06-19 13:11:04,680 - training - INFO - 🏁 训练轮次 20/50, 平均损失: nan
2025-06-19 13:11:04,943 - training - INFO - 🏁 训练轮次 21/50, 平均损失: nan
2025-06-19 13:11:05,208 - training - INFO - 🏁 训练轮次 22/50, 平均损失: nan
2025-06-19 13:11:05,477 - training - INFO - 🏁 训练轮次 23/50, 平均损失: nan
2025-06-19 13:11:05,740 - training - INFO - 🏁 训练轮次 24/50, 平均损失: nan
2025-06-19 13:11:06,002 - training - INFO - 🏁 训练轮次 25/50, 平均损失: nan
2025-06-19 13:11:06,271 - training - INFO - 🏁 训练轮次 26/50, 平均损失: nan
2025-06-19 13:11:06,532 - training - INFO - 🏁 训练轮次 27/50, 平均损失: nan
2025-06-19 13:11:06,793 - training - INFO - 🏁 训练轮次 28/50, 平均损失: nan
2025-06-19 13:11:07,063 - training - INFO - 🏁 训练轮次 29/50, 平均损失: nan
2025-06-19 13:11:07,333 - training - INFO - 🏁 训练轮次 30/50, 平均损失: nan
2025-06-19 13:11:07,598 - training - INFO - 🏁 训练轮次 31/50, 平均损失: nan
2025-06-19 13:11:07,879 - training - INFO - 🏁 训练轮次 32/50, 平均损失: nan
2025-06-19 13:11:08,164 - training - INFO - 🏁 训练轮次 33/50, 平均损失: nan
2025-06-19 13:11:08,438 - training - INFO - 🏁 训练轮次 34/50, 平均损失: nan
2025-06-19 13:11:08,697 - training - INFO - 🏁 训练轮次 35/50, 平均损失: nan
2025-06-19 13:11:08,956 - training - INFO - 🏁 训练轮次 36/50, 平均损失: nan
2025-06-19 13:11:09,220 - training - INFO - 🏁 训练轮次 37/50, 平均损失: nan
2025-06-19 13:11:09,484 - training - INFO - 🏁 训练轮次 38/50, 平均损失: nan
2025-06-19 13:11:09,750 - training - INFO - 🏁 训练轮次 39/50, 平均损失: nan
2025-06-19 13:11:10,013 - training - INFO - 🏁 训练轮次 40/50, 平均损失: nan
2025-06-19 13:11:10,281 - training - INFO - 🏁 训练轮次 41/50, 平均损失: nan
2025-06-19 13:11:10,540 - training - INFO - 🏁 训练轮次 42/50, 平均损失: nan
2025-06-19 13:11:10,806 - training - INFO - 🏁 训练轮次 43/50, 平均损失: nan
2025-06-19 13:11:11,073 - training - INFO - 🏁 训练轮次 44/50, 平均损失: nan
2025-06-19 13:11:11,334 - training - INFO - 🏁 训练轮次 45/50, 平均损失: nan
2025-06-19 13:11:11,594 - training - INFO - 🏁 训练轮次 46/50, 平均损失: nan
2025-06-19 13:11:11,855 - training - INFO - 🏁 训练轮次 47/50, 平均损失: nan
2025-06-19 13:11:12,124 - training - INFO - 🏁 训练轮次 48/50, 平均损失: nan
2025-06-19 13:11:12,387 - training - INFO - 🏁 训练轮次 49/50, 平均损失: nan
2025-06-19 13:11:12,651 - training - INFO - 🏁 训练轮次 50/50, 平均损失: nan
2025-06-24 12:45:33,005 - training - INFO - 🚀 开始运行训练脚本
2025-06-24 12:45:33,005 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-24 12:45:33,005 - training - INFO - 📁 所有输出目录已创建
2025-06-24 12:45:34,422 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-24 12:45:37,381 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-24 12:45:37,697 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-24 12:45:37,951 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-24 12:45:37,979 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-24 12:52:09,354 - training - INFO - 🚀 开始运行训练脚本
2025-06-24 12:52:09,354 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-24 12:52:09,354 - training - INFO - 📁 所有输出目录已创建
2025-06-24 12:52:10,374 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-24 12:52:13,183 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-24 12:52:13,490 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-24 12:52:13,754 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-24 12:52:13,781 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-24 13:06:11,298 - training - INFO - 🚀 开始运行训练脚本
2025-06-24 13:06:11,298 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-24 13:06:11,298 - training - INFO - 📁 所有输出目录已创建
2025-06-24 13:06:12,322 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-24 13:06:15,101 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-24 13:06:15,424 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-24 13:06:15,690 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-24 13:06:15,716 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-24 13:06:16,278 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-24 13:37:30,856 - training - INFO - 🚀 开始运行训练脚本
2025-06-24 13:37:30,856 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-24 13:37:30,857 - training - INFO - 📁 所有输出目录已创建
2025-06-24 13:37:31,878 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-24 13:37:34,700 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-24 13:37:34,986 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-24 13:37:35,239 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-24 13:37:35,253 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-24 13:37:35,802 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-30 12:20:12,548 - training - INFO - 🚀 开始运行训练脚本
2025-06-30 12:20:12,699 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-30 12:20:12,699 - training - INFO - 📁 所有输出目录已创建
2025-06-30 12:20:14,597 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-30 12:21:02,670 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-30 12:21:02,947 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-30 12:21:04,083 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-30 12:21:04,095 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-30 12:21:13,033 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-30 12:32:06,302 - training - INFO - 🚀 开始运行训练脚本
2025-06-30 12:32:06,302 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-30 12:32:06,302 - training - INFO - 📁 所有输出目录已创建
2025-06-30 12:32:07,311 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-30 12:32:09,896 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-30 12:32:10,171 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-30 12:32:10,436 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-30 12:32:10,447 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-30 12:32:10,995 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-30 12:32:19,522 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,104 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,260 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,420 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,575 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:20,890 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,143 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,210 - training - INFO - 🏁 Epoch 1 | 平均损失: 44.058122 | 跳过批次: 0
2025-06-30 12:32:21,442 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,574 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,705 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:21,970 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,101 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,403 - training - INFO - 🏁 Epoch 2 | 平均损失: 43.039205 | 跳过批次: 0
2025-06-30 12:32:22,636 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,767 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:22,898 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,032 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,162 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,293 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,538 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:23,593 - training - INFO - 🏁 Epoch 3 | 平均损失: 43.199584 | 跳过批次: 0
2025-06-30 12:32:23,825 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,051 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,182 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,315 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,446 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,577 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,708 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:24,883 - training - INFO - 🏁 Epoch 4 | 平均损失: 43.998177 | 跳过批次: 0
2025-06-30 12:32:25,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,253 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,385 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,520 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,651 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,782 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:25,913 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:26,028 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:26,087 - training - INFO - 🏁 Epoch 5 | 平均损失: 43.932054 | 跳过批次: 0
2025-06-30 12:32:28,737 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:28,875 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,142 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,273 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,404 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,534 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:29,704 - training - INFO - 🏁 Epoch 6 | 平均损失: 43.937681 | 跳过批次: 0
2025-06-30 12:32:29,947 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,079 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,344 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,474 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,606 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,737 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,852 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:30,911 - training - INFO - 🏁 Epoch 7 | 平均损失: 44.028507 | 跳过批次: 0
2025-06-30 12:32:31,156 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,290 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,553 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,813 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:31,944 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,057 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,114 - training - INFO - 🏁 Epoch 8 | 平均损失: 43.192728 | 跳过批次: 0
2025-06-30 12:32:32,350 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,480 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,611 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,744 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:32,875 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,137 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,251 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,308 - training - INFO - 🏁 Epoch 9 | 平均损失: 44.028275 | 跳过批次: 0
2025-06-30 12:32:33,548 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,681 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,813 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:33,946 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:34,077 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:34,209 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:34,340 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:34,453 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:34,511 - training - INFO - 🏁 Epoch 10 | 平均损失: 44.061826 | 跳过批次: 0
2025-06-30 12:32:41,372 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:41,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:41,640 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:41,774 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:41,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,036 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,168 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,283 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,339 - training - INFO - 🏁 Epoch 11 | 平均损失: 44.054362 | 跳过批次: 0
2025-06-30 12:32:42,576 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,708 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:42,974 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,237 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,368 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,482 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,539 - training - INFO - 🏁 Epoch 12 | 平均损失: 43.735198 | 跳过批次: 0
2025-06-30 12:32:43,776 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:43,908 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,040 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,176 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,308 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,438 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,569 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:44,739 - training - INFO - 🏁 Epoch 13 | 平均损失: 41.466315 | 跳过批次: 0
2025-06-30 12:32:44,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,107 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,375 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,507 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,638 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,770 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,886 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:45,947 - training - INFO - 🏁 Epoch 14 | 平均损失: 44.083387 | 跳过批次: 0
2025-06-30 12:32:46,187 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,319 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,450 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,584 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,716 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,847 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:46,978 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:47,094 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:32:47,154 - training - INFO - 🏁 Epoch 15 | 平均损失: 44.044491 | 跳过批次: 0
2025-06-30 12:33:01,145 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:01,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:01,497 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:01,631 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:01,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:01,894 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,026 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,143 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,198 - training - INFO - 🏁 Epoch 16 | 平均损失: 43.003422 | 跳过批次: 0
2025-06-30 12:33:02,431 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,566 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,697 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,830 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:02,963 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,229 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,343 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,403 - training - INFO - 🏁 Epoch 17 | 平均损失: 43.740660 | 跳过批次: 0
2025-06-30 12:33:03,634 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,768 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:03,899 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,033 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,165 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,297 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,429 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,544 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,602 - training - INFO - 🏁 Epoch 18 | 平均损失: 42.243429 | 跳过批次: 0
2025-06-30 12:33:04,838 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:04,972 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,369 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,500 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,632 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,745 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:05,803 - training - INFO - 🏁 Epoch 19 | 平均损失: 43.937093 | 跳过批次: 0
2025-06-30 12:33:06,133 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,263 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,395 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,529 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,660 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:06,922 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:07,037 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:07,096 - training - INFO - 🏁 Epoch 20 | 平均损失: 44.067187 | 跳过批次: 0
2025-06-30 12:33:21,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:21,367 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:21,614 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:21,748 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:21,880 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,012 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,143 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,259 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,315 - training - INFO - 🏁 Epoch 21 | 平均损失: 44.004854 | 跳过批次: 0
2025-06-30 12:33:22,553 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,685 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,816 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:22,950 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,082 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,213 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,345 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,462 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,519 - training - INFO - 🏁 Epoch 22 | 平均损失: 43.670383 | 跳过批次: 0
2025-06-30 12:33:23,754 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:23,890 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,021 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,160 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,425 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,557 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,673 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:24,730 - training - INFO - 🏁 Epoch 23 | 平均损失: 42.003020 | 跳过批次: 0
2025-06-30 12:33:24,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,362 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,493 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,625 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,756 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,871 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:25,929 - training - INFO - 🏁 Epoch 24 | 平均损失: 44.063587 | 跳过批次: 0
2025-06-30 12:33:26,165 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,297 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,427 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,580 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,711 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,843 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:26,974 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:27,091 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:27,149 - training - INFO - 🏁 Epoch 25 | 平均损失: 41.433920 | 跳过批次: 0
2025-06-30 12:33:41,586 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:41,762 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:41,896 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,031 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,164 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,295 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,542 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,599 - training - INFO - 🏁 Epoch 26 | 平均损失: 42.613842 | 跳过批次: 0
2025-06-30 12:33:42,834 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:42,966 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,098 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,363 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,742 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:43,799 - training - INFO - 🏁 Epoch 27 | 平均损失: 43.160058 | 跳过批次: 0
2025-06-30 12:33:44,040 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,173 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,305 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,439 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,570 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,700 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,831 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:44,947 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,006 - training - INFO - 🏁 Epoch 28 | 平均损失: 42.356134 | 跳过批次: 0
2025-06-30 12:33:45,244 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,375 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,507 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,644 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,775 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:45,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,037 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,211 - training - INFO - 🏁 Epoch 29 | 平均损失: 43.781841 | 跳过批次: 0
2025-06-30 12:33:46,447 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,579 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,710 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,846 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:46,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:47,108 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:47,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:47,357 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:33:47,414 - training - INFO - 🏁 Epoch 30 | 平均损失: 43.221500 | 跳过批次: 0
2025-06-30 12:34:01,387 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:01,522 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:01,654 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:01,816 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:01,947 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,078 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,208 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,325 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,385 - training - INFO - 🏁 Epoch 31 | 平均损失: 43.569916 | 跳过批次: 0
2025-06-30 12:34:02,625 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,756 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:02,886 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,019 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,149 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,411 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,526 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,585 - training - INFO - 🏁 Epoch 32 | 平均损失: 44.015404 | 跳过批次: 0
2025-06-30 12:34:03,820 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:03,951 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,081 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,478 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,608 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,722 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:04,779 - training - INFO - 🏁 Epoch 33 | 平均损失: 43.649719 | 跳过批次: 0
2025-06-30 12:34:05,014 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,146 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,277 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,411 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,542 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,675 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,805 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,921 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:05,979 - training - INFO - 🏁 Epoch 34 | 平均损失: 42.917056 | 跳过批次: 0
2025-06-30 12:34:06,217 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:06,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:06,478 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:06,613 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:06,744 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:06,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:07,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:07,120 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:07,177 - training - INFO - 🏁 Epoch 35 | 平均损失: 44.014782 | 跳过批次: 0
2025-06-30 12:34:21,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,327 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,459 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,595 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,726 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,856 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:21,989 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,127 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,187 - training - INFO - 🏁 Epoch 36 | 平均损失: 43.998538 | 跳过批次: 0
2025-06-30 12:34:22,422 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,553 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,685 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,817 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:22,948 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,079 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,209 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,326 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,385 - training - INFO - 🏁 Epoch 37 | 平均损失: 40.872230 | 跳过批次: 0
2025-06-30 12:34:23,628 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,761 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:23,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,025 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,156 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,287 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,418 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,532 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,591 - training - INFO - 🏁 Epoch 38 | 平均损失: 42.390221 | 跳过批次: 0
2025-06-30 12:34:24,827 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:24,958 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,089 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,225 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,356 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,487 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,618 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,736 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:25,794 - training - INFO - 🏁 Epoch 39 | 平均损失: 44.062197 | 跳过批次: 0
2025-06-30 12:34:26,035 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,166 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,391 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,656 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,788 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:26,920 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:27,035 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:27,172 - training - INFO - 🏁 Epoch 40 | 平均损失: 44.040950 | 跳过批次: 0
2025-06-30 12:34:41,461 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:41,594 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:41,725 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:41,859 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:41,990 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,253 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,368 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,429 - training - INFO - 🏁 Epoch 41 | 平均损失: 40.820810 | 跳过批次: 0
2025-06-30 12:34:42,664 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,796 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:42,930 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,196 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,328 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,460 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,576 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,632 - training - INFO - 🏁 Epoch 42 | 平均损失: 41.057539 | 跳过批次: 0
2025-06-30 12:34:43,866 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:43,998 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,132 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,266 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,399 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,531 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,663 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,777 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:44,835 - training - INFO - 🏁 Epoch 43 | 平均损失: 44.010775 | 跳过批次: 0
2025-06-30 12:34:45,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,204 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,336 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,471 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,602 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,733 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:45,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,033 - training - INFO - 🏁 Epoch 44 | 平均损失: 44.080058 | 跳过批次: 0
2025-06-30 12:34:46,269 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,402 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,534 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,669 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,800 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:46,933 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:47,065 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:47,181 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:34:47,291 - training - INFO - 🏁 Epoch 45 | 平均损失: 43.771432 | 跳过批次: 0
2025-06-30 12:35:01,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:01,921 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,053 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,187 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,318 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,451 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,584 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,697 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:02,792 - training - INFO - 🏁 Epoch 46 | 平均损失: 43.071172 | 跳过批次: 0
2025-06-30 12:35:03,030 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,162 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,294 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,427 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,560 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,691 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,939 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:03,997 - training - INFO - 🏁 Epoch 47 | 平均损失: 41.693130 | 跳过批次: 0
2025-06-30 12:35:04,234 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:04,367 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:04,500 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:04,635 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:04,767 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:04,898 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,029 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,144 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,201 - training - INFO - 🏁 Epoch 48 | 平均损失: 43.107645 | 跳过批次: 0
2025-06-30 12:35:05,440 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,572 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,704 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,841 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:05,972 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,235 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,350 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,410 - training - INFO - 🏁 Epoch 49 | 平均损失: 44.007634 | 跳过批次: 0
2025-06-30 12:35:06,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,780 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:06,913 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,046 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,311 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,443 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:35:07,617 - training - INFO - 🏁 Epoch 50 | 平均损失: 44.115130 | 跳过批次: 0
2025-06-30 12:35:08,946 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250630_123508.csv
2025-06-30 12:41:14,429 - training - INFO - 🚀 开始运行训练脚本
2025-06-30 12:41:14,429 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-30 12:41:14,429 - training - INFO - 📁 所有输出目录已创建
2025-06-30 12:41:15,483 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-30 12:41:29,821 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-30 12:41:30,107 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-30 12:41:30,388 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-30 12:41:30,399 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-30 12:41:31,066 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-30 12:41:32,636 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:32,842 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:32,999 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,163 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,477 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,635 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,784 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:33,851 - training - INFO - 🏁 Epoch 1 | 平均损失: 43.388355 | 跳过批次: 0
2025-06-30 12:41:34,088 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,221 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,352 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,485 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,616 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,747 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:34,996 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,054 - training - INFO - 🏁 Epoch 2 | 平均损失: 42.232161 | 跳过批次: 0
2025-06-30 12:41:35,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,556 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,690 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,821 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:35,953 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,085 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,199 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,258 - training - INFO - 🏁 Epoch 3 | 平均损失: 43.358130 | 跳过批次: 0
2025-06-30 12:41:36,506 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:36,996 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,127 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,258 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,390 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,503 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,562 - training - INFO - 🏁 Epoch 4 | 平均损失: 40.373555 | 跳过批次: 0
2025-06-30 12:41:37,801 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:37,934 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,065 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,199 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,330 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,461 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,593 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,706 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:38,765 - training - INFO - 🏁 Epoch 5 | 平均损失: 41.962664 | 跳过批次: 0
2025-06-30 12:41:41,784 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:41,921 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,052 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,186 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,317 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,449 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,580 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,695 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:42,756 - training - INFO - 🏁 Epoch 6 | 平均损失: 41.989780 | 跳过批次: 0
2025-06-30 12:41:43,010 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,141 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,273 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,407 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,540 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,672 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,803 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,918 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:43,975 - training - INFO - 🏁 Epoch 7 | 平均损失: 42.546843 | 跳过批次: 0
2025-06-30 12:41:44,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:44,343 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:44,475 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:44,609 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:44,741 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:44,872 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,004 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,177 - training - INFO - 🏁 Epoch 8 | 平均损失: 41.744924 | 跳过批次: 0
2025-06-30 12:41:45,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,691 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,827 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:45,960 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,091 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,223 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,341 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,398 - training - INFO - 🏁 Epoch 9 | 平均损失: 43.282195 | 跳过批次: 0
2025-06-30 12:41:46,643 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,777 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:46,909 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,044 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,176 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,308 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,439 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,555 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:47,613 - training - INFO - 🏁 Epoch 10 | 平均损失: 42.581156 | 跳过批次: 0
2025-06-30 12:41:55,646 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:55,779 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:55,912 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,047 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,311 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,444 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,558 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,616 - training - INFO - 🏁 Epoch 11 | 平均损失: 43.275309 | 跳过批次: 0
2025-06-30 12:41:56,854 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:56,987 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,252 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,383 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,516 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,762 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:57,821 - training - INFO - 🏁 Epoch 12 | 平均损失: 43.370475 | 跳过批次: 0
2025-06-30 12:41:58,056 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,190 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,322 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,456 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,588 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,719 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:58,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,024 - training - INFO - 🏁 Epoch 13 | 平均损失: 41.885258 | 跳过批次: 0
2025-06-30 12:41:59,261 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,394 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,659 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,790 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:41:59,922 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,054 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,168 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,229 - training - INFO - 🏁 Epoch 14 | 平均损失: 43.377245 | 跳过批次: 0
2025-06-30 12:42:00,465 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,598 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,730 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,864 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:00,996 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:01,127 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:01,257 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:01,373 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:01,432 - training - INFO - 🏁 Epoch 15 | 平均损失: 42.523447 | 跳过批次: 0
2025-06-30 12:42:16,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,286 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,420 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,553 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,816 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:16,949 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,122 - training - INFO - 🏁 Epoch 16 | 平均损失: 43.406105 | 跳过批次: 0
2025-06-30 12:42:17,363 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,494 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,760 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:17,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,024 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,271 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,330 - training - INFO - 🏁 Epoch 17 | 平均损失: 43.330265 | 跳过批次: 0
2025-06-30 12:42:18,571 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,702 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,833 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:18,972 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,104 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,234 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,366 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,540 - training - INFO - 🏁 Epoch 18 | 平均损失: 43.136490 | 跳过批次: 0
2025-06-30 12:42:19,773 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:19,906 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,038 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,172 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,305 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,437 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,569 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:20,740 - training - INFO - 🏁 Epoch 19 | 平均损失: 41.729724 | 跳过批次: 0
2025-06-30 12:42:21,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,204 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,335 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,469 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,600 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,732 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,865 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:21,981 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:22,041 - training - INFO - 🏁 Epoch 20 | 平均损失: 43.317433 | 跳过批次: 0
2025-06-30 12:42:37,147 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,282 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,681 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,812 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:37,944 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,061 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,119 - training - INFO - 🏁 Epoch 21 | 平均损失: 43.412988 | 跳过批次: 0
2025-06-30 12:42:38,360 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,492 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,624 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,760 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:38,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,023 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,270 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,330 - training - INFO - 🏁 Epoch 22 | 平均损失: 43.380108 | 跳过批次: 0
2025-06-30 12:42:39,568 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,705 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,837 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:39,972 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,234 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,365 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,541 - training - INFO - 🏁 Epoch 23 | 平均损失: 43.338450 | 跳过批次: 0
2025-06-30 12:42:40,778 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:40,909 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,041 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,175 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,307 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,438 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,570 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,685 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:41,746 - training - INFO - 🏁 Epoch 24 | 平均损失: 41.630822 | 跳过批次: 0
2025-06-30 12:42:41,987 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,249 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,385 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,516 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,648 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,780 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,895 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:42,953 - training - INFO - 🏁 Epoch 25 | 平均损失: 43.269346 | 跳过批次: 0
2025-06-30 12:42:57,952 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,086 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,220 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,352 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,484 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,616 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,750 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:58,921 - training - INFO - 🏁 Epoch 26 | 平均损失: 42.218606 | 跳过批次: 0
2025-06-30 12:42:59,162 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,295 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,561 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,692 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:42:59,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,069 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,128 - training - INFO - 🏁 Epoch 27 | 平均损失: 43.395540 | 跳过批次: 0
2025-06-30 12:43:00,366 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,501 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,633 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,767 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:00,899 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,031 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,163 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,276 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,335 - training - INFO - 🏁 Epoch 28 | 平均损失: 43.318349 | 跳过批次: 0
2025-06-30 12:43:01,575 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,707 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,841 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:01,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,108 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,240 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,371 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,487 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,545 - training - INFO - 🏁 Epoch 29 | 平均损失: 41.463232 | 跳过批次: 0
2025-06-30 12:43:02,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:02,919 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,051 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,184 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,315 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,446 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,580 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,694 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:03,752 - training - INFO - 🏁 Epoch 30 | 平均损失: 42.260044 | 跳过批次: 0
2025-06-30 12:43:19,281 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:19,413 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:19,545 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:19,679 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:19,809 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:19,940 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,072 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,188 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,246 - training - INFO - 🏁 Epoch 31 | 平均损失: 43.396080 | 跳过批次: 0
2025-06-30 12:43:20,485 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,618 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,749 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:20,882 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,013 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,145 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,276 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,393 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,451 - training - INFO - 🏁 Epoch 32 | 平均损失: 42.445873 | 跳过批次: 0
2025-06-30 12:43:21,690 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,822 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:21,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,087 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,219 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,351 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,483 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,597 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:22,657 - training - INFO - 🏁 Epoch 33 | 平均损失: 42.206439 | 跳过批次: 0
2025-06-30 12:43:22,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,025 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,156 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,290 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,422 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,553 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,800 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:23,857 - training - INFO - 🏁 Epoch 34 | 平均损失: 42.960351 | 跳过批次: 0
2025-06-30 12:43:24,093 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,358 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,493 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,625 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,756 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:24,889 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:25,004 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:25,064 - training - INFO - 🏁 Epoch 35 | 平均损失: 42.421299 | 跳过批次: 0
2025-06-30 12:43:40,415 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:40,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:40,680 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:40,814 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:40,945 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,078 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,209 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,323 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,380 - training - INFO - 🏁 Epoch 36 | 平均损失: 43.412320 | 跳过批次: 0
2025-06-30 12:43:41,620 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,752 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:41,885 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,154 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,285 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,530 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,591 - training - INFO - 🏁 Epoch 37 | 平均损失: 43.285115 | 跳过批次: 0
2025-06-30 12:43:42,827 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:42,960 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,093 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,357 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,488 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,620 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,734 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:43,796 - training - INFO - 🏁 Epoch 38 | 平均损失: 43.361629 | 跳过批次: 0
2025-06-30 12:43:44,032 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,163 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,294 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,691 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,821 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,939 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:44,999 - training - INFO - 🏁 Epoch 39 | 平均损失: 43.374897 | 跳过批次: 0
2025-06-30 12:43:45,234 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:45,366 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:45,592 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:45,793 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:45,924 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:46,055 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:46,186 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:46,302 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:43:46,362 - training - INFO - 🏁 Epoch 40 | 平均损失: 42.466572 | 跳过批次: 0
2025-06-30 12:44:01,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:01,866 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:01,999 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,132 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,264 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,395 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,526 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,642 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:02,700 - training - INFO - 🏁 Epoch 41 | 平均损失: 43.369531 | 跳过批次: 0
2025-06-30 12:44:02,938 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,338 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,469 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,600 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,732 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,847 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:03,906 - training - INFO - 🏁 Epoch 42 | 平均损失: 43.317978 | 跳过批次: 0
2025-06-30 12:44:04,140 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,274 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,407 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,541 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,672 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,803 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:04,934 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,048 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,109 - training - INFO - 🏁 Epoch 43 | 平均损失: 41.100318 | 跳过批次: 0
2025-06-30 12:44:05,345 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,478 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,611 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,744 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:05,877 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,139 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,255 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,313 - training - INFO - 🏁 Epoch 44 | 平均损失: 42.910584 | 跳过批次: 0
2025-06-30 12:44:06,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,682 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,815 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:06,948 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:07,080 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:07,211 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:07,342 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:07,456 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:07,514 - training - INFO - 🏁 Epoch 45 | 平均损失: 43.325818 | 跳过批次: 0
2025-06-30 12:44:22,964 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,100 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,365 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,498 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,631 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,879 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:23,939 - training - INFO - 🏁 Epoch 46 | 平均损失: 39.533312 | 跳过批次: 0
2025-06-30 12:44:24,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,312 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,444 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,709 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:24,971 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,086 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,145 - training - INFO - 🏁 Epoch 47 | 平均损失: 43.422725 | 跳过批次: 0
2025-06-30 12:44:25,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,517 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:25,918 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,048 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,293 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,351 - training - INFO - 🏁 Epoch 48 | 平均损失: 41.140401 | 跳过批次: 0
2025-06-30 12:44:26,589 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,722 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,853 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:26,987 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,119 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,382 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,496 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,554 - training - INFO - 🏁 Epoch 49 | 平均损失: 43.311091 | 跳过批次: 0
2025-06-30 12:44:27,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:27,922 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,055 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,188 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,319 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,450 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,583 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,698 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 12:44:28,759 - training - INFO - 🏁 Epoch 50 | 平均损失: 43.436480 | 跳过批次: 0
2025-06-30 12:44:29,021 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250630_124429.csv
2025-06-30 13:25:00,935 - training - INFO - 🚀 开始运行训练脚本
2025-06-30 13:25:00,935 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-30 13:25:00,935 - training - INFO - 📁 所有输出目录已创建
2025-06-30 13:25:01,952 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-30 13:25:12,841 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-30 13:25:13,133 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-30 13:25:13,433 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-30 13:25:13,445 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-06-30 13:25:14,343 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-06-30 13:25:15,953 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,161 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,639 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,797 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:16,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,176 - training - INFO - 🏁 Epoch 1 | 平均损失: 43.508423 | 跳过批次: 0
2025-06-30 13:25:17,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,816 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:17,946 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,078 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,326 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,386 - training - INFO - 🏁 Epoch 2 | 平均损失: 42.998947 | 跳过批次: 0
2025-06-30 13:25:18,625 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,757 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:18,889 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,023 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,154 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,285 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,416 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,529 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:19,595 - training - INFO - 🏁 Epoch 3 | 平均损失: 43.231140 | 跳过批次: 0
2025-06-30 13:25:19,838 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,066 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,331 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,462 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,592 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,723 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,837 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:20,897 - training - INFO - 🏁 Epoch 4 | 平均损失: 43.545978 | 跳过批次: 0
2025-06-30 13:25:21,140 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,272 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,401 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,535 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,664 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,795 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:21,925 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:22,038 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:22,100 - training - INFO - 🏁 Epoch 5 | 平均损失: 43.485529 | 跳过批次: 0
2025-06-30 13:25:25,111 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,245 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,376 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,509 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,641 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,772 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:25,902 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,018 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,077 - training - INFO - 🏁 Epoch 6 | 平均损失: 41.880756 | 跳过批次: 0
2025-06-30 13:25:26,316 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,447 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,579 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,712 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,843 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:26,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,107 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,222 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,283 - training - INFO - 🏁 Epoch 7 | 平均损失: 43.479909 | 跳过批次: 0
2025-06-30 13:25:27,520 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,652 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,783 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:27,916 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,047 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,310 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,425 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,485 - training - INFO - 🏁 Epoch 8 | 平均损失: 43.510432 | 跳过批次: 0
2025-06-30 13:25:28,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:28,995 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,129 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,260 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,392 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,639 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:29,736 - training - INFO - 🏁 Epoch 9 | 平均损失: 43.500760 | 跳过批次: 0
2025-06-30 13:25:29,980 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,111 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,243 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,377 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,639 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,771 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,888 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:30,948 - training - INFO - 🏁 Epoch 10 | 平均损失: 41.174076 | 跳过批次: 0
2025-06-30 13:25:38,494 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:38,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:38,759 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:38,893 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,024 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,287 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,402 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,462 - training - INFO - 🏁 Epoch 11 | 平均损失: 42.659682 | 跳过批次: 0
2025-06-30 13:25:39,703 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:39,983 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,116 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,247 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,379 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,510 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,623 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:40,682 - training - INFO - 🏁 Epoch 12 | 平均损失: 42.497418 | 跳过批次: 0
2025-06-30 13:25:40,941 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,072 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,338 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,469 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,601 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,735 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:41,915 - training - INFO - 🏁 Epoch 13 | 平均损失: 40.093313 | 跳过批次: 0
2025-06-30 13:25:42,161 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,556 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,687 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,818 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:42,948 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,061 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,126 - training - INFO - 🏁 Epoch 14 | 平均损失: 41.898716 | 跳过批次: 0
2025-06-30 13:25:43,381 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,515 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,781 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:43,912 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:44,045 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:44,178 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:44,293 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:44,356 - training - INFO - 🏁 Epoch 15 | 平均损失: 43.534716 | 跳过批次: 0
2025-06-30 13:25:57,775 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:57,911 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,045 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,180 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,311 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,444 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,693 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:58,754 - training - INFO - 🏁 Epoch 16 | 平均损失: 43.504645 | 跳过批次: 0
2025-06-30 13:25:58,993 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,127 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,259 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,392 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,524 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,656 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,788 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:25:59,945 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,007 - training - INFO - 🏁 Epoch 17 | 平均损失: 42.616311 | 跳过批次: 0
2025-06-30 13:26:00,248 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,380 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,511 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,646 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,779 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:00,910 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,042 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,157 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,217 - training - INFO - 🏁 Epoch 18 | 平均损失: 42.333140 | 跳过批次: 0
2025-06-30 13:26:01,458 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,590 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,721 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,856 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:01,988 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:02,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:02,249 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:02,364 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:02,426 - training - INFO - 🏁 Epoch 19 | 平均损失: 43.337674 | 跳过批次: 0
2025-06-30 13:26:02,762 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:02,893 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,025 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,158 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,289 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,419 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,551 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,666 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:03,729 - training - INFO - 🏁 Epoch 20 | 平均损失: 43.277523 | 跳过批次: 0
2025-06-30 13:26:17,373 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:17,510 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:17,641 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:17,775 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:17,906 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,036 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,167 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,343 - training - INFO - 🏁 Epoch 21 | 平均损失: 43.501200 | 跳过批次: 0
2025-06-30 13:26:18,583 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,717 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,849 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:18,984 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,115 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,247 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,378 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,494 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,552 - training - INFO - 🏁 Epoch 22 | 平均损失: 42.520031 | 跳过批次: 0
2025-06-30 13:26:19,795 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:19,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,061 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,329 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,462 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,594 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,710 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:20,771 - training - INFO - 🏁 Epoch 23 | 平均损失: 42.941338 | 跳过批次: 0
2025-06-30 13:26:21,010 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,144 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,276 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,410 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,541 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,672 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,803 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,918 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:21,978 - training - INFO - 🏁 Epoch 24 | 平均损失: 42.549587 | 跳过批次: 0
2025-06-30 13:26:22,217 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:22,348 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:22,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:22,615 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:22,746 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:22,877 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:23,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:23,123 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:23,182 - training - INFO - 🏁 Epoch 25 | 平均损失: 42.425575 | 跳过批次: 0
2025-06-30 13:26:37,190 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,325 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,459 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,592 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,725 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,857 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:37,990 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,165 - training - INFO - 🏁 Epoch 26 | 平均损失: 40.920861 | 跳过批次: 0
2025-06-30 13:26:38,409 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,542 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,675 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,810 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:38,942 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,075 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,206 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,383 - training - INFO - 🏁 Epoch 27 | 平均损失: 43.179545 | 跳过批次: 0
2025-06-30 13:26:39,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,758 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:39,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,026 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,159 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,539 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,599 - training - INFO - 🏁 Epoch 28 | 平均损失: 43.050749 | 跳过批次: 0
2025-06-30 13:26:40,841 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:40,974 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,371 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,503 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,634 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,750 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:41,811 - training - INFO - 🏁 Epoch 29 | 平均损失: 41.772376 | 跳过批次: 0
2025-06-30 13:26:42,057 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,189 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,321 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,455 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,587 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,718 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,849 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:42,964 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:43,025 - training - INFO - 🏁 Epoch 30 | 平均损失: 43.573160 | 跳过批次: 0
2025-06-30 13:26:57,268 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:57,401 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:57,533 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:57,667 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:57,798 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:57,930 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,062 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,176 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,236 - training - INFO - 🏁 Epoch 31 | 平均损失: 43.577094 | 跳过批次: 0
2025-06-30 13:26:58,475 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,608 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,741 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:58,874 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,138 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,271 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,446 - training - INFO - 🏁 Epoch 32 | 平均损失: 42.670309 | 跳过批次: 0
2025-06-30 13:26:59,689 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,821 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:26:59,953 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,086 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,254 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,518 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,632 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:00,692 - training - INFO - 🏁 Epoch 33 | 平均损失: 42.769334 | 跳过批次: 0
2025-06-30 13:27:00,938 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,069 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,202 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,336 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,468 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,599 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,846 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:01,906 - training - INFO - 🏁 Epoch 34 | 平均损失: 43.442685 | 跳过批次: 0
2025-06-30 13:27:02,150 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,283 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,413 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,545 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,676 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,809 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:02,940 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:03,056 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:03,115 - training - INFO - 🏁 Epoch 35 | 平均损失: 43.462690 | 跳过批次: 0
2025-06-30 13:27:17,195 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,330 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,461 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,594 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,727 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,858 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:17,988 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,162 - training - INFO - 🏁 Epoch 36 | 平均损失: 43.633232 | 跳过批次: 0
2025-06-30 13:27:18,405 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,538 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,670 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,804 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:18,936 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,068 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,198 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,314 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,375 - training - INFO - 🏁 Epoch 37 | 平均损失: 41.826216 | 跳过批次: 0
2025-06-30 13:27:19,617 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,750 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:19,881 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,015 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,145 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,277 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,409 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,523 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,584 - training - INFO - 🏁 Epoch 38 | 平均损失: 43.562028 | 跳过批次: 0
2025-06-30 13:27:20,824 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:20,956 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,091 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,224 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,356 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,488 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,619 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,733 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:21,794 - training - INFO - 🏁 Epoch 39 | 平均损失: 42.860012 | 跳过批次: 0
2025-06-30 13:27:22,038 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,170 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,399 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,532 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,663 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,796 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:22,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:23,044 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:23,106 - training - INFO - 🏁 Epoch 40 | 平均损失: 42.720307 | 跳过批次: 0
2025-06-30 13:27:37,015 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,150 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,281 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,416 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,547 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,679 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,811 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,927 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:37,987 - training - INFO - 🏁 Epoch 41 | 平均损失: 42.554592 | 跳过批次: 0
2025-06-30 13:27:38,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:38,361 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:38,492 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:38,627 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:38,759 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:38,891 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,021 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,137 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,203 - training - INFO - 🏁 Epoch 42 | 平均损失: 41.609709 | 跳过批次: 0
2025-06-30 13:27:39,446 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,579 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,709 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,842 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:39,973 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,238 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,353 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,439 - training - INFO - 🏁 Epoch 43 | 平均损失: 43.584320 | 跳过批次: 0
2025-06-30 13:27:40,686 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,819 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:40,951 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,085 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,215 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,346 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,478 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,592 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:41,653 - training - INFO - 🏁 Epoch 44 | 平均损失: 43.401286 | 跳过批次: 0
2025-06-30 13:27:41,894 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,026 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,159 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,293 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,425 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,557 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,688 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,804 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:42,863 - training - INFO - 🏁 Epoch 45 | 平均损失: 41.406442 | 跳过批次: 0
2025-06-30 13:27:56,805 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:56,941 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,072 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,207 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,339 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,471 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,603 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,718 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:57,777 - training - INFO - 🏁 Epoch 46 | 平均损失: 43.258252 | 跳过批次: 0
2025-06-30 13:27:58,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,283 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,548 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,679 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,811 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:58,993 - training - INFO - 🏁 Epoch 47 | 平均损失: 41.794439 | 跳过批次: 0
2025-06-30 13:27:59,233 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:59,364 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:59,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:59,628 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:59,759 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:27:59,890 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,021 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,136 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,197 - training - INFO - 🏁 Epoch 48 | 平均损失: 43.580202 | 跳过批次: 0
2025-06-30 13:28:00,441 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,573 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,705 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,839 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:00,971 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,102 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,233 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,348 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,418 - training - INFO - 🏁 Epoch 49 | 平均损失: 43.493930 | 跳过批次: 0
2025-06-30 13:28:01,664 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,799 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:01,934 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,068 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,200 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,333 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,464 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-06-30 13:28:02,641 - training - INFO - 🏁 Epoch 50 | 平均损失: 43.572997 | 跳过批次: 0
2025-06-30 13:28:03,253 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250630_132803.csv
2025-06-30 13:48:42,309 - training - INFO - 🚀 开始运行训练脚本
2025-06-30 13:48:42,309 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-06-30 13:48:42,309 - training - INFO - 📁 所有输出目录已创建
2025-06-30 13:48:43,312 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-06-30 13:48:46,379 - training - INFO - ✅ 成功加载预训练模型权重
2025-06-30 13:48:46,660 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-06-30 13:48:46,941 - training - INFO - ⚙️ 模型运行在: cuda
2025-06-30 13:48:46,953 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 12:25:26,370 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 12:25:26,421 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 12:25:26,421 - training - INFO - 📁 所有输出目录已创建
2025-07-01 12:25:28,206 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 12:26:15,488 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 12:26:15,761 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 12:26:16,828 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 12:26:16,840 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 12:34:44,429 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 12:34:44,429 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 12:34:44,429 - training - INFO - 📁 所有输出目录已创建
2025-07-01 12:34:45,434 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 12:34:47,987 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 12:34:48,260 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 12:34:48,524 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 12:34:48,535 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 12:34:57,293 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-01 12:46:57,106 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 12:46:57,106 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 12:46:57,106 - training - INFO - 📁 所有输出目录已创建
2025-07-01 12:46:58,099 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 12:47:00,662 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 12:47:00,933 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 12:47:01,186 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 12:47:01,199 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 12:47:01,727 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-01 12:58:28,828 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 12:58:28,828 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 12:58:28,828 - training - INFO - 📁 所有输出目录已创建
2025-07-01 12:58:29,821 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 12:58:32,378 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 12:58:32,650 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 12:58:32,903 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 12:58:32,913 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 12:58:33,446 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-01 13:28:30,417 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 13:28:30,417 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 13:28:30,417 - training - INFO - 📁 所有输出目录已创建
2025-07-01 13:28:31,407 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 13:28:33,962 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 13:28:34,234 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 13:28:34,498 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 13:28:34,510 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 13:28:35,044 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-01 13:28:46,655 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:47,281 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:47,452 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:47,624 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:47,790 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:47,955 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:48,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:48,351 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:48,418 - training - INFO - 🏁 Epoch 1 | 平均损失: 43.422035 | 跳过批次: 0
2025-07-01 13:28:48,663 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:48,803 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:48,943 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,084 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,224 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,364 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,503 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:49,684 - training - INFO - 🏁 Epoch 2 | 平均损失: 43.383829 | 跳过批次: 0
2025-07-01 13:28:49,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,068 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,207 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,348 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,487 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,625 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,764 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,886 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:50,944 - training - INFO - 🏁 Epoch 3 | 平均损失: 41.443282 | 跳过批次: 0
2025-07-01 13:28:51,186 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:51,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:51,561 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:51,704 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:51,842 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:51,980 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,240 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,298 - training - INFO - 🏁 Epoch 4 | 平均损失: 41.541920 | 跳过批次: 0
2025-07-01 13:28:52,545 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,824 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:52,968 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:53,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:53,244 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:53,383 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:53,505 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:53,563 - training - INFO - 🏁 Epoch 5 | 平均损失: 41.728313 | 跳过批次: 0
2025-07-01 13:28:57,178 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:57,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:57,459 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:57,600 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:57,738 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:57,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,015 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,138 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,195 - training - INFO - 🏁 Epoch 6 | 平均损失: 41.108097 | 跳过批次: 0
2025-07-01 13:28:58,438 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,717 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,858 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:58,997 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,136 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,276 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,398 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,454 - training - INFO - 🏁 Epoch 7 | 平均损失: 43.346454 | 跳过批次: 0
2025-07-01 13:28:59,696 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,836 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:28:59,975 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,257 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,535 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,658 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:00,714 - training - INFO - 🏁 Epoch 8 | 平均损失: 41.682788 | 跳过批次: 0
2025-07-01 13:29:00,958 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,099 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,238 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,383 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,663 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,801 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,923 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:01,981 - training - INFO - 🏁 Epoch 9 | 平均损失: 42.535776 | 跳过批次: 0
2025-07-01 13:29:02,226 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:02,365 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:02,504 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:02,646 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:02,784 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:02,922 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:03,062 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:03,184 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:03,239 - training - INFO - 🏁 Epoch 10 | 平均损失: 41.441287 | 跳过批次: 0
2025-07-01 13:29:10,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:10,788 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:10,927 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,068 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,207 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,346 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,486 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,609 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:11,666 - training - INFO - 🏁 Epoch 11 | 平均损失: 43.440219 | 跳过批次: 0
2025-07-01 13:29:11,915 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,056 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,341 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,483 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,621 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,767 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,889 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:12,946 - training - INFO - 🏁 Epoch 12 | 平均损失: 43.390333 | 跳过批次: 0
2025-07-01 13:29:13,192 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:13,333 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:13,472 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:13,615 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:13,754 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:13,893 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,031 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,211 - training - INFO - 🏁 Epoch 13 | 平均损失: 42.067775 | 跳过批次: 0
2025-07-01 13:29:14,454 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,595 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,735 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:14,875 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,014 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,153 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,293 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,416 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,472 - training - INFO - 🏁 Epoch 14 | 平均损失: 42.575005 | 跳过批次: 0
2025-07-01 13:29:15,717 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,856 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:15,996 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,137 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,276 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,415 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,554 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,677 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:16,737 - training - INFO - 🏁 Epoch 15 | 平均损失: 43.330572 | 跳过批次: 0
2025-07-01 13:29:29,842 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:29,983 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,123 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,263 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,402 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,541 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,682 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,804 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:30,862 - training - INFO - 🏁 Epoch 16 | 平均损失: 43.393055 | 跳过批次: 0
2025-07-01 13:29:31,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,247 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,388 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,530 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,668 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,808 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:31,948 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,127 - training - INFO - 🏁 Epoch 17 | 平均损失: 43.405918 | 跳过批次: 0
2025-07-01 13:29:32,395 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,534 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,675 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,814 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:32,953 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,092 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,231 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,354 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,411 - training - INFO - 🏁 Epoch 18 | 平均损失: 43.350341 | 跳过批次: 0
2025-07-01 13:29:33,658 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,798 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:33,937 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,080 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,220 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,361 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,500 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,624 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:34,682 - training - INFO - 🏁 Epoch 19 | 平均损失: 39.374591 | 跳过批次: 0
2025-07-01 13:29:35,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,162 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,301 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,443 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,583 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,726 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,867 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:35,990 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:36,050 - training - INFO - 🏁 Epoch 20 | 平均损失: 41.930484 | 跳过批次: 0
2025-07-01 13:29:49,806 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:49,946 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,085 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,226 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,365 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,505 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,646 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,770 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:50,827 - training - INFO - 🏁 Epoch 21 | 平均损失: 42.027042 | 跳过批次: 0
2025-07-01 13:29:51,069 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,208 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,489 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,627 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,766 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:51,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,028 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,084 - training - INFO - 🏁 Epoch 22 | 平均损失: 41.206879 | 跳过批次: 0
2025-07-01 13:29:52,328 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,483 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,622 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:52,902 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,040 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,181 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,301 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,364 - training - INFO - 🏁 Epoch 23 | 平均损失: 43.384777 | 跳过批次: 0
2025-07-01 13:29:53,609 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,749 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:53,888 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,030 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,169 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,307 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,445 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,568 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:54,625 - training - INFO - 🏁 Epoch 24 | 平均损失: 43.438529 | 跳过批次: 0
2025-07-01 13:29:54,873 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,012 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,294 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,432 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,571 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,710 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,832 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:29:55,888 - training - INFO - 🏁 Epoch 25 | 平均损失: 43.496505 | 跳过批次: 0
2025-07-01 13:30:09,089 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,373 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,515 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,654 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,793 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:09,933 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,057 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,113 - training - INFO - 🏁 Epoch 26 | 平均损失: 43.458550 | 跳过批次: 0
2025-07-01 13:30:10,355 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,496 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,635 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,777 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:10,916 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,055 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,316 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,373 - training - INFO - 🏁 Epoch 27 | 平均损失: 42.272588 | 跳过批次: 0
2025-07-01 13:30:11,618 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,758 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:11,898 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,039 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,464 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,587 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:12,643 - training - INFO - 🏁 Epoch 28 | 平均损失: 43.309310 | 跳过批次: 0
2025-07-01 13:30:12,889 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,075 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,217 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,358 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,497 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,637 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,778 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,900 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:13,958 - training - INFO - 🏁 Epoch 29 | 平均损失: 41.552674 | 跳过批次: 0
2025-07-01 13:30:14,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:14,344 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:14,485 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:14,627 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:14,766 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:14,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:15,044 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:15,167 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:15,226 - training - INFO - 🏁 Epoch 30 | 平均损失: 43.307504 | 跳过批次: 0
2025-07-01 13:30:29,794 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:29,946 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,093 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,242 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,384 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,522 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,662 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,785 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:30,841 - training - INFO - 🏁 Epoch 31 | 平均损失: 43.398098 | 跳过批次: 0
2025-07-01 13:30:31,088 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,220 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,365 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,510 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,650 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,790 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:35,929 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,053 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,110 - training - INFO - 🏁 Epoch 32 | 平均损失: 43.313828 | 跳过批次: 0
2025-07-01 13:30:36,356 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,496 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,636 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,777 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:36,915 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,055 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,321 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,379 - training - INFO - 🏁 Epoch 33 | 平均损失: 42.911984 | 跳过批次: 0
2025-07-01 13:30:37,620 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,762 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:37,902 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,044 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,183 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,322 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,462 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,585 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:38,642 - training - INFO - 🏁 Epoch 34 | 平均损失: 41.214211 | 跳过批次: 0
2025-07-01 13:30:38,890 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,029 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,167 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,312 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,451 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,590 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,729 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:39,909 - training - INFO - 🏁 Epoch 35 | 平均损失: 42.544651 | 跳过批次: 0
2025-07-01 13:30:49,971 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,114 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,254 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,536 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,675 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,815 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,938 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:50,997 - training - INFO - 🏁 Epoch 36 | 平均损失: 43.338965 | 跳过批次: 0
2025-07-01 13:30:51,244 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:51,384 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:51,523 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:51,665 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:51,804 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:51,943 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,081 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,205 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,263 - training - INFO - 🏁 Epoch 37 | 平均损失: 41.569458 | 跳过批次: 0
2025-07-01 13:30:52,509 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,789 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:52,932 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,349 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,490 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,549 - training - INFO - 🏁 Epoch 38 | 平均损失: 43.406504 | 跳过批次: 0
2025-07-01 13:30:53,790 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:53,930 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,070 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,211 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,350 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,488 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,629 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,751 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:54,807 - training - INFO - 🏁 Epoch 39 | 平均损失: 43.450453 | 跳过批次: 0
2025-07-01 13:30:55,053 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,196 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,571 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,710 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,850 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:55,989 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:56,112 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:30:56,171 - training - INFO - 🏁 Epoch 40 | 平均损失: 42.267486 | 跳过批次: 0
2025-07-01 13:31:09,695 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:09,837 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:09,977 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,257 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,395 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,537 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,660 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:10,718 - training - INFO - 🏁 Epoch 41 | 平均损失: 42.272960 | 跳过批次: 0
2025-07-01 13:31:10,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,107 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,245 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,387 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,526 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,665 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,804 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,927 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:11,987 - training - INFO - 🏁 Epoch 42 | 平均损失: 41.722131 | 跳过批次: 0
2025-07-01 13:31:12,232 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:12,375 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:12,514 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:12,657 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:12,796 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:12,935 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,076 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,253 - training - INFO - 🏁 Epoch 43 | 平均损失: 42.264156 | 跳过批次: 0
2025-07-01 13:31:13,499 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,824 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:13,967 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,244 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,509 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,566 - training - INFO - 🏁 Epoch 44 | 平均损失: 43.442871 | 跳过批次: 0
2025-07-01 13:31:14,811 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:14,951 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,090 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,231 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,370 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,509 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,648 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,772 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:15,832 - training - INFO - 🏁 Epoch 45 | 平均损失: 41.576893 | 跳过批次: 0
2025-07-01 13:31:29,575 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:29,715 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:29,855 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:29,998 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,138 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,277 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,540 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,600 - training - INFO - 🏁 Epoch 46 | 平均损失: 43.398062 | 跳过批次: 0
2025-07-01 13:31:30,847 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:30,986 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,126 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,267 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,405 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,544 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,684 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,806 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:31,864 - training - INFO - 🏁 Epoch 47 | 平均损失: 43.337865 | 跳过批次: 0
2025-07-01 13:31:32,110 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,390 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,531 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,670 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,808 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:32,947 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:33,070 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:33,128 - training - INFO - 🏁 Epoch 48 | 平均损失: 41.964664 | 跳过批次: 0
2025-07-01 13:31:33,378 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:33,518 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:33,656 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:33,878 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,017 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,156 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,295 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,476 - training - INFO - 🏁 Epoch 49 | 平均损失: 42.525663 | 跳过批次: 0
2025-07-01 13:31:34,724 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:34,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,003 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,145 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,284 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,562 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,688 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-01 13:31:35,746 - training - INFO - 🏁 Epoch 50 | 平均损失: 41.437701 | 跳过批次: 0
2025-07-01 13:31:36,889 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250701_133136.csv
2025-07-01 13:41:06,301 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 13:41:06,301 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 13:41:06,301 - training - INFO - 📁 所有输出目录已创建
2025-07-01 13:41:07,395 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 13:41:10,256 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 13:41:10,535 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 13:41:10,988 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 13:41:10,999 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 13:41:11,553 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-01 13:51:42,603 - training - INFO - 🚀 开始运行训练脚本
2025-07-01 13:51:42,603 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-01 13:51:42,603 - training - INFO - 📁 所有输出目录已创建
2025-07-01 13:51:43,603 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-01 13:51:55,692 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-01 13:51:55,971 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-01 13:51:56,222 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-01 13:51:56,235 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-01 13:51:56,769 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-03 12:32:50,615 - training - INFO - 🚀 开始运行训练脚本
2025-07-03 12:32:50,667 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-03 12:32:50,667 - training - INFO - 📁 所有输出目录已创建
2025-07-03 12:32:52,509 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-03 12:33:39,707 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-03 12:33:39,989 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-03 12:33:40,977 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-03 12:33:40,987 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-03 12:33:50,351 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-03 12:34:04,128 - training - ERROR - ⛔ Epoch 1 没有有效训练批次，尝试恢复...
2025-07-03 12:34:04,128 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:04,372 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:34:04,372 - training - WARNING - 恢复至epoch 0
2025-07-03 12:34:04,925 - training - ERROR - ⛔ Epoch 2 没有有效训练批次，尝试恢复...
2025-07-03 12:34:04,925 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:05,165 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:34:05,166 - training - WARNING - 恢复至epoch 0
2025-07-03 12:34:05,819 - training - ERROR - ⛔ Epoch 3 没有有效训练批次，尝试恢复...
2025-07-03 12:34:05,819 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:06,060 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:34:06,060 - training - WARNING - 恢复至epoch 0
2025-07-03 12:34:06,614 - training - ERROR - ⛔ Epoch 4 没有有效训练批次，尝试恢复...
2025-07-03 12:34:06,615 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:06,858 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:34:06,859 - training - WARNING - 恢复至epoch 0
2025-07-03 12:34:07,404 - training - ERROR - ⛔ Epoch 5 没有有效训练批次，尝试恢复...
2025-07-03 12:34:07,404 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:07,644 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:34:07,645 - training - WARNING - 恢复至epoch 0
2025-07-03 12:34:09,288 - training - ERROR - ⛔ Epoch 6 没有有效训练批次，尝试恢复...
2025-07-03 12:34:09,288 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:09,527 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:34:09,527 - training - WARNING - 恢复至epoch 5
2025-07-03 12:34:10,080 - training - ERROR - ⛔ Epoch 7 没有有效训练批次，尝试恢复...
2025-07-03 12:34:10,080 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:10,320 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:34:10,321 - training - WARNING - 恢复至epoch 5
2025-07-03 12:34:10,869 - training - ERROR - ⛔ Epoch 8 没有有效训练批次，尝试恢复...
2025-07-03 12:34:10,870 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:11,107 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:34:11,107 - training - WARNING - 恢复至epoch 5
2025-07-03 12:34:11,659 - training - ERROR - ⛔ Epoch 9 没有有效训练批次，尝试恢复...
2025-07-03 12:34:11,660 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:11,897 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:34:11,898 - training - WARNING - 恢复至epoch 5
2025-07-03 12:34:12,443 - training - ERROR - ⛔ Epoch 10 没有有效训练批次，尝试恢复...
2025-07-03 12:34:12,443 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:12,682 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:34:12,682 - training - WARNING - 恢复至epoch 5
2025-07-03 12:34:14,309 - training - ERROR - ⛔ Epoch 11 没有有效训练批次，尝试恢复...
2025-07-03 12:34:14,310 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:14,548 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:34:14,548 - training - WARNING - 恢复至epoch 10
2025-07-03 12:34:15,094 - training - ERROR - ⛔ Epoch 12 没有有效训练批次，尝试恢复...
2025-07-03 12:34:15,094 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:15,332 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:34:15,332 - training - WARNING - 恢复至epoch 10
2025-07-03 12:34:15,883 - training - ERROR - ⛔ Epoch 13 没有有效训练批次，尝试恢复...
2025-07-03 12:34:15,884 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:16,125 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:34:16,125 - training - WARNING - 恢复至epoch 10
2025-07-03 12:34:16,678 - training - ERROR - ⛔ Epoch 14 没有有效训练批次，尝试恢复...
2025-07-03 12:34:16,678 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:16,913 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:34:16,914 - training - WARNING - 恢复至epoch 10
2025-07-03 12:34:17,462 - training - ERROR - ⛔ Epoch 15 没有有效训练批次，尝试恢复...
2025-07-03 12:34:17,462 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:17,697 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:34:17,697 - training - WARNING - 恢复至epoch 10
2025-07-03 12:34:19,468 - training - ERROR - ⛔ Epoch 16 没有有效训练批次，尝试恢复...
2025-07-03 12:34:19,469 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:19,711 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:34:19,712 - training - WARNING - 恢复至epoch 15
2025-07-03 12:34:20,275 - training - ERROR - ⛔ Epoch 17 没有有效训练批次，尝试恢复...
2025-07-03 12:34:20,275 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:20,517 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:34:20,518 - training - WARNING - 恢复至epoch 15
2025-07-03 12:34:21,069 - training - ERROR - ⛔ Epoch 18 没有有效训练批次，尝试恢复...
2025-07-03 12:34:21,070 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:21,303 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:34:21,304 - training - WARNING - 恢复至epoch 15
2025-07-03 12:34:21,848 - training - ERROR - ⛔ Epoch 19 没有有效训练批次，尝试恢复...
2025-07-03 12:34:21,849 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:22,084 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:34:22,084 - training - WARNING - 恢复至epoch 15
2025-07-03 12:34:22,723 - training - ERROR - ⛔ Epoch 20 没有有效训练批次，尝试恢复...
2025-07-03 12:34:22,724 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:22,957 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:34:22,957 - training - WARNING - 恢复至epoch 15
2025-07-03 12:34:24,628 - training - ERROR - ⛔ Epoch 21 没有有效训练批次，尝试恢复...
2025-07-03 12:34:24,629 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:24,870 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:34:24,870 - training - WARNING - 恢复至epoch 20
2025-07-03 12:34:25,425 - training - ERROR - ⛔ Epoch 22 没有有效训练批次，尝试恢复...
2025-07-03 12:34:25,425 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:25,676 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:34:25,676 - training - WARNING - 恢复至epoch 20
2025-07-03 12:34:26,231 - training - ERROR - ⛔ Epoch 23 没有有效训练批次，尝试恢复...
2025-07-03 12:34:26,231 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:26,472 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:34:26,472 - training - WARNING - 恢复至epoch 20
2025-07-03 12:34:27,020 - training - ERROR - ⛔ Epoch 24 没有有效训练批次，尝试恢复...
2025-07-03 12:34:27,021 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:27,257 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:34:27,257 - training - WARNING - 恢复至epoch 20
2025-07-03 12:34:27,811 - training - ERROR - ⛔ Epoch 25 没有有效训练批次，尝试恢复...
2025-07-03 12:34:27,812 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:28,048 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:34:28,048 - training - WARNING - 恢复至epoch 20
2025-07-03 12:34:29,504 - training - ERROR - ⛔ Epoch 26 没有有效训练批次，尝试恢复...
2025-07-03 12:34:29,505 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:29,741 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:34:29,741 - training - WARNING - 恢复至epoch 25
2025-07-03 12:34:30,297 - training - ERROR - ⛔ Epoch 27 没有有效训练批次，尝试恢复...
2025-07-03 12:34:30,297 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:30,531 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:34:30,531 - training - WARNING - 恢复至epoch 25
2025-07-03 12:34:31,081 - training - ERROR - ⛔ Epoch 28 没有有效训练批次，尝试恢复...
2025-07-03 12:34:31,082 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:31,322 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:34:31,322 - training - WARNING - 恢复至epoch 25
2025-07-03 12:34:31,870 - training - ERROR - ⛔ Epoch 29 没有有效训练批次，尝试恢复...
2025-07-03 12:34:31,870 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:32,104 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:34:32,105 - training - WARNING - 恢复至epoch 25
2025-07-03 12:34:32,658 - training - ERROR - ⛔ Epoch 30 没有有效训练批次，尝试恢复...
2025-07-03 12:34:32,659 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:32,892 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:34:32,892 - training - WARNING - 恢复至epoch 25
2025-07-03 12:34:34,358 - training - ERROR - ⛔ Epoch 31 没有有效训练批次，尝试恢复...
2025-07-03 12:34:34,358 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:34,600 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:34:34,601 - training - WARNING - 恢复至epoch 30
2025-07-03 12:34:35,150 - training - ERROR - ⛔ Epoch 32 没有有效训练批次，尝试恢复...
2025-07-03 12:34:35,150 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:35,390 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:34:35,391 - training - WARNING - 恢复至epoch 30
2025-07-03 12:34:35,941 - training - ERROR - ⛔ Epoch 33 没有有效训练批次，尝试恢复...
2025-07-03 12:34:35,942 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:36,187 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:34:36,187 - training - WARNING - 恢复至epoch 30
2025-07-03 12:34:36,737 - training - ERROR - ⛔ Epoch 34 没有有效训练批次，尝试恢复...
2025-07-03 12:34:36,737 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:36,978 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:34:36,979 - training - WARNING - 恢复至epoch 30
2025-07-03 12:34:37,528 - training - ERROR - ⛔ Epoch 35 没有有效训练批次，尝试恢复...
2025-07-03 12:34:37,528 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:37,775 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:34:37,775 - training - WARNING - 恢复至epoch 30
2025-07-03 12:34:39,331 - training - ERROR - ⛔ Epoch 36 没有有效训练批次，尝试恢复...
2025-07-03 12:34:39,332 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:39,574 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:34:39,574 - training - WARNING - 恢复至epoch 35
2025-07-03 12:34:40,128 - training - ERROR - ⛔ Epoch 37 没有有效训练批次，尝试恢复...
2025-07-03 12:34:40,129 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:40,370 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:34:40,371 - training - WARNING - 恢复至epoch 35
2025-07-03 12:34:40,920 - training - ERROR - ⛔ Epoch 38 没有有效训练批次，尝试恢复...
2025-07-03 12:34:40,920 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:41,162 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:34:41,162 - training - WARNING - 恢复至epoch 35
2025-07-03 12:34:41,712 - training - ERROR - ⛔ Epoch 39 没有有效训练批次，尝试恢复...
2025-07-03 12:34:41,712 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:41,956 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:34:41,956 - training - WARNING - 恢复至epoch 35
2025-07-03 12:34:42,514 - training - ERROR - ⛔ Epoch 40 没有有效训练批次，尝试恢复...
2025-07-03 12:34:42,515 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:42,763 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:34:42,763 - training - WARNING - 恢复至epoch 35
2025-07-03 12:34:44,246 - training - ERROR - ⛔ Epoch 41 没有有效训练批次，尝试恢复...
2025-07-03 12:34:44,247 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:44,488 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:34:44,488 - training - WARNING - 恢复至epoch 40
2025-07-03 12:34:45,037 - training - ERROR - ⛔ Epoch 42 没有有效训练批次，尝试恢复...
2025-07-03 12:34:45,037 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:45,281 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:34:45,282 - training - WARNING - 恢复至epoch 40
2025-07-03 12:34:45,830 - training - ERROR - ⛔ Epoch 43 没有有效训练批次，尝试恢复...
2025-07-03 12:34:45,831 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:46,073 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:34:46,074 - training - WARNING - 恢复至epoch 40
2025-07-03 12:34:46,623 - training - ERROR - ⛔ Epoch 44 没有有效训练批次，尝试恢复...
2025-07-03 12:34:46,623 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:46,863 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:34:46,864 - training - WARNING - 恢复至epoch 40
2025-07-03 12:34:47,410 - training - ERROR - ⛔ Epoch 45 没有有效训练批次，尝试恢复...
2025-07-03 12:34:47,411 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:47,652 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:34:47,652 - training - WARNING - 恢复至epoch 40
2025-07-03 12:34:49,245 - training - ERROR - ⛔ Epoch 46 没有有效训练批次，尝试恢复...
2025-07-03 12:34:49,245 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:49,487 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:34:49,487 - training - WARNING - 恢复至epoch 45
2025-07-03 12:34:50,036 - training - ERROR - ⛔ Epoch 47 没有有效训练批次，尝试恢复...
2025-07-03 12:34:50,036 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:50,279 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:34:50,279 - training - WARNING - 恢复至epoch 45
2025-07-03 12:34:50,826 - training - ERROR - ⛔ Epoch 48 没有有效训练批次，尝试恢复...
2025-07-03 12:34:50,826 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:51,068 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:34:51,068 - training - WARNING - 恢复至epoch 45
2025-07-03 12:34:51,616 - training - ERROR - ⛔ Epoch 49 没有有效训练批次，尝试恢复...
2025-07-03 12:34:51,616 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:51,852 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:34:51,853 - training - WARNING - 恢复至epoch 45
2025-07-03 12:34:52,399 - training - ERROR - ⛔ Epoch 50 没有有效训练批次，尝试恢复...
2025-07-03 12:34:52,399 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:34:52,638 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:34:52,639 - training - WARNING - 恢复至epoch 45
2025-07-03 12:34:54,078 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250703_123453.csv
2025-07-03 12:52:49,356 - training - INFO - 🚀 开始运行训练脚本
2025-07-03 12:52:49,356 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-03 12:52:49,356 - training - INFO - 📁 所有输出目录已创建
2025-07-03 12:52:50,349 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-03 12:52:52,921 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-03 12:52:53,210 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-03 12:52:53,472 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-03 12:52:53,482 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-03 12:52:54,052 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-03 12:52:56,050 - training - ERROR - ⛔ Epoch 1 没有有效训练批次，尝试恢复...
2025-07-03 12:52:56,051 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:52:56,325 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:52:56,325 - training - WARNING - 恢复至epoch 0
2025-07-03 12:52:56,911 - training - ERROR - ⛔ Epoch 2 没有有效训练批次，尝试恢复...
2025-07-03 12:52:56,912 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:52:57,200 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:52:57,201 - training - WARNING - 恢复至epoch 0
2025-07-03 12:52:57,874 - training - ERROR - ⛔ Epoch 3 没有有效训练批次，尝试恢复...
2025-07-03 12:52:57,875 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:52:58,225 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:52:58,226 - training - WARNING - 恢复至epoch 0
2025-07-03 12:52:58,791 - training - ERROR - ⛔ Epoch 4 没有有效训练批次，尝试恢复...
2025-07-03 12:52:58,792 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:52:59,068 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:52:59,068 - training - WARNING - 恢复至epoch 0
2025-07-03 12:52:59,634 - training - ERROR - ⛔ Epoch 5 没有有效训练批次，尝试恢复...
2025-07-03 12:52:59,635 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:52:59,904 - training - WARNING - 从备份恢复: epoch_0_safety_backup.pt (epoch 0)
2025-07-03 12:52:59,905 - training - WARNING - 恢复至epoch 0
2025-07-03 12:53:01,368 - training - ERROR - ⛔ Epoch 6 没有有效训练批次，尝试恢复...
2025-07-03 12:53:01,368 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:01,632 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:53:01,632 - training - WARNING - 恢复至epoch 5
2025-07-03 12:53:02,194 - training - ERROR - ⛔ Epoch 7 没有有效训练批次，尝试恢复...
2025-07-03 12:53:02,195 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:02,464 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:53:02,464 - training - WARNING - 恢复至epoch 5
2025-07-03 12:53:03,036 - training - ERROR - ⛔ Epoch 8 没有有效训练批次，尝试恢复...
2025-07-03 12:53:03,036 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:03,304 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:53:03,304 - training - WARNING - 恢复至epoch 5
2025-07-03 12:53:03,872 - training - ERROR - ⛔ Epoch 9 没有有效训练批次，尝试恢复...
2025-07-03 12:53:03,872 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:04,145 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:53:04,145 - training - WARNING - 恢复至epoch 5
2025-07-03 12:53:04,709 - training - ERROR - ⛔ Epoch 10 没有有效训练批次，尝试恢复...
2025-07-03 12:53:04,709 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:04,978 - training - WARNING - 从备份恢复: epoch_5_safety_backup.pt (epoch 5)
2025-07-03 12:53:04,978 - training - WARNING - 恢复至epoch 5
2025-07-03 12:53:06,444 - training - ERROR - ⛔ Epoch 11 没有有效训练批次，尝试恢复...
2025-07-03 12:53:06,445 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:06,713 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:53:06,714 - training - WARNING - 恢复至epoch 10
2025-07-03 12:53:07,284 - training - ERROR - ⛔ Epoch 12 没有有效训练批次，尝试恢复...
2025-07-03 12:53:07,284 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:07,551 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:53:07,552 - training - WARNING - 恢复至epoch 10
2025-07-03 12:53:08,128 - training - ERROR - ⛔ Epoch 13 没有有效训练批次，尝试恢复...
2025-07-03 12:53:08,129 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:08,408 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:53:08,409 - training - WARNING - 恢复至epoch 10
2025-07-03 12:53:08,982 - training - ERROR - ⛔ Epoch 14 没有有效训练批次，尝试恢复...
2025-07-03 12:53:08,983 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:09,250 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:53:09,250 - training - WARNING - 恢复至epoch 10
2025-07-03 12:53:09,816 - training - ERROR - ⛔ Epoch 15 没有有效训练批次，尝试恢复...
2025-07-03 12:53:09,816 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:10,085 - training - WARNING - 从备份恢复: epoch_10_safety_backup.pt (epoch 10)
2025-07-03 12:53:10,086 - training - WARNING - 恢复至epoch 10
2025-07-03 12:53:11,562 - training - ERROR - ⛔ Epoch 16 没有有效训练批次，尝试恢复...
2025-07-03 12:53:11,563 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:11,832 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:53:11,833 - training - WARNING - 恢复至epoch 15
2025-07-03 12:53:12,395 - training - ERROR - ⛔ Epoch 17 没有有效训练批次，尝试恢复...
2025-07-03 12:53:12,396 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:12,663 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:53:12,664 - training - WARNING - 恢复至epoch 15
2025-07-03 12:53:13,235 - training - ERROR - ⛔ Epoch 18 没有有效训练批次，尝试恢复...
2025-07-03 12:53:13,235 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:13,503 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:53:13,503 - training - WARNING - 恢复至epoch 15
2025-07-03 12:53:14,069 - training - ERROR - ⛔ Epoch 19 没有有效训练批次，尝试恢复...
2025-07-03 12:53:14,069 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:14,338 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:53:14,339 - training - WARNING - 恢复至epoch 15
2025-07-03 12:53:15,008 - training - ERROR - ⛔ Epoch 20 没有有效训练批次，尝试恢复...
2025-07-03 12:53:15,008 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:15,275 - training - WARNING - 从备份恢复: epoch_15_safety_backup.pt (epoch 15)
2025-07-03 12:53:15,275 - training - WARNING - 恢复至epoch 15
2025-07-03 12:53:16,740 - training - ERROR - ⛔ Epoch 21 没有有效训练批次，尝试恢复...
2025-07-03 12:53:16,740 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:17,008 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:53:17,008 - training - WARNING - 恢复至epoch 20
2025-07-03 12:53:17,574 - training - ERROR - ⛔ Epoch 22 没有有效训练批次，尝试恢复...
2025-07-03 12:53:17,574 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:17,850 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:53:17,851 - training - WARNING - 恢复至epoch 20
2025-07-03 12:53:18,420 - training - ERROR - ⛔ Epoch 23 没有有效训练批次，尝试恢复...
2025-07-03 12:53:18,420 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:18,686 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:53:18,687 - training - WARNING - 恢复至epoch 20
2025-07-03 12:53:19,259 - training - ERROR - ⛔ Epoch 24 没有有效训练批次，尝试恢复...
2025-07-03 12:53:19,259 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:19,524 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:53:19,524 - training - WARNING - 恢复至epoch 20
2025-07-03 12:53:20,091 - training - ERROR - ⛔ Epoch 25 没有有效训练批次，尝试恢复...
2025-07-03 12:53:20,092 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:20,364 - training - WARNING - 从备份恢复: epoch_20_safety_backup.pt (epoch 20)
2025-07-03 12:53:20,364 - training - WARNING - 恢复至epoch 20
2025-07-03 12:53:21,898 - training - ERROR - ⛔ Epoch 26 没有有效训练批次，尝试恢复...
2025-07-03 12:53:21,898 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:22,162 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:53:22,162 - training - WARNING - 恢复至epoch 25
2025-07-03 12:53:22,724 - training - ERROR - ⛔ Epoch 27 没有有效训练批次，尝试恢复...
2025-07-03 12:53:22,724 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:22,993 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:53:22,993 - training - WARNING - 恢复至epoch 25
2025-07-03 12:53:23,564 - training - ERROR - ⛔ Epoch 28 没有有效训练批次，尝试恢复...
2025-07-03 12:53:23,565 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:23,832 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:53:23,833 - training - WARNING - 恢复至epoch 25
2025-07-03 12:53:24,403 - training - ERROR - ⛔ Epoch 29 没有有效训练批次，尝试恢复...
2025-07-03 12:53:24,403 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:24,670 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:53:24,671 - training - WARNING - 恢复至epoch 25
2025-07-03 12:53:25,234 - training - ERROR - ⛔ Epoch 30 没有有效训练批次，尝试恢复...
2025-07-03 12:53:25,235 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:25,498 - training - WARNING - 从备份恢复: epoch_25_safety_backup.pt (epoch 25)
2025-07-03 12:53:25,499 - training - WARNING - 恢复至epoch 25
2025-07-03 12:53:26,984 - training - ERROR - ⛔ Epoch 31 没有有效训练批次，尝试恢复...
2025-07-03 12:53:26,985 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:27,254 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:53:27,255 - training - WARNING - 恢复至epoch 30
2025-07-03 12:53:27,825 - training - ERROR - ⛔ Epoch 32 没有有效训练批次，尝试恢复...
2025-07-03 12:53:27,826 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:28,102 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:53:28,102 - training - WARNING - 恢复至epoch 30
2025-07-03 12:53:28,674 - training - ERROR - ⛔ Epoch 33 没有有效训练批次，尝试恢复...
2025-07-03 12:53:28,675 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:28,956 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:53:28,956 - training - WARNING - 恢复至epoch 30
2025-07-03 12:53:29,568 - training - ERROR - ⛔ Epoch 34 没有有效训练批次，尝试恢复...
2025-07-03 12:53:29,568 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:29,852 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:53:29,853 - training - WARNING - 恢复至epoch 30
2025-07-03 12:53:30,425 - training - ERROR - ⛔ Epoch 35 没有有效训练批次，尝试恢复...
2025-07-03 12:53:30,425 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:30,703 - training - WARNING - 从备份恢复: epoch_30_safety_backup.pt (epoch 30)
2025-07-03 12:53:30,703 - training - WARNING - 恢复至epoch 30
2025-07-03 12:53:32,757 - training - ERROR - ⛔ Epoch 36 没有有效训练批次，尝试恢复...
2025-07-03 12:53:32,758 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:33,033 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:53:33,033 - training - WARNING - 恢复至epoch 35
2025-07-03 12:53:33,600 - training - ERROR - ⛔ Epoch 37 没有有效训练批次，尝试恢复...
2025-07-03 12:53:33,601 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:33,872 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:53:33,872 - training - WARNING - 恢复至epoch 35
2025-07-03 12:53:34,433 - training - ERROR - ⛔ Epoch 38 没有有效训练批次，尝试恢复...
2025-07-03 12:53:34,434 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:34,695 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:53:34,696 - training - WARNING - 恢复至epoch 35
2025-07-03 12:53:35,270 - training - ERROR - ⛔ Epoch 39 没有有效训练批次，尝试恢复...
2025-07-03 12:53:35,270 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:35,545 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:53:35,546 - training - WARNING - 恢复至epoch 35
2025-07-03 12:53:36,116 - training - ERROR - ⛔ Epoch 40 没有有效训练批次，尝试恢复...
2025-07-03 12:53:36,116 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:36,390 - training - WARNING - 从备份恢复: epoch_35_safety_backup.pt (epoch 35)
2025-07-03 12:53:36,391 - training - WARNING - 恢复至epoch 35
2025-07-03 12:53:38,021 - training - ERROR - ⛔ Epoch 41 没有有效训练批次，尝试恢复...
2025-07-03 12:53:38,021 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:38,297 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:53:38,297 - training - WARNING - 恢复至epoch 40
2025-07-03 12:53:38,861 - training - ERROR - ⛔ Epoch 42 没有有效训练批次，尝试恢复...
2025-07-03 12:53:38,861 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:39,130 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:53:39,131 - training - WARNING - 恢复至epoch 40
2025-07-03 12:53:39,772 - training - ERROR - ⛔ Epoch 43 没有有效训练批次，尝试恢复...
2025-07-03 12:53:39,772 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:40,043 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:53:40,044 - training - WARNING - 恢复至epoch 40
2025-07-03 12:53:40,609 - training - ERROR - ⛔ Epoch 44 没有有效训练批次，尝试恢复...
2025-07-03 12:53:40,609 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:40,876 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:53:40,876 - training - WARNING - 恢复至epoch 40
2025-07-03 12:53:41,444 - training - ERROR - ⛔ Epoch 45 没有有效训练批次，尝试恢复...
2025-07-03 12:53:41,445 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:41,715 - training - WARNING - 从备份恢复: epoch_40_safety_backup.pt (epoch 40)
2025-07-03 12:53:41,715 - training - WARNING - 恢复至epoch 40
2025-07-03 12:53:43,560 - training - ERROR - ⛔ Epoch 46 没有有效训练批次，尝试恢复...
2025-07-03 12:53:43,560 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:43,826 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:53:43,826 - training - WARNING - 恢复至epoch 45
2025-07-03 12:53:44,389 - training - ERROR - ⛔ Epoch 47 没有有效训练批次，尝试恢复...
2025-07-03 12:53:44,390 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:44,659 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:53:44,659 - training - WARNING - 恢复至epoch 45
2025-07-03 12:53:45,227 - training - ERROR - ⛔ Epoch 48 没有有效训练批次，尝试恢复...
2025-07-03 12:53:45,227 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:45,497 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:53:45,498 - training - WARNING - 恢复至epoch 45
2025-07-03 12:53:46,062 - training - ERROR - ⛔ Epoch 49 没有有效训练批次，尝试恢复...
2025-07-03 12:53:46,063 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:46,328 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:53:46,329 - training - WARNING - 恢复至epoch 45
2025-07-03 12:53:46,895 - training - ERROR - ⛔ Epoch 50 没有有效训练批次，尝试恢复...
2025-07-03 12:53:46,896 - training - WARNING - 采用学习率退避策略，新学习率: 5.00e-06
2025-07-03 12:53:47,162 - training - WARNING - 从备份恢复: epoch_45_safety_backup.pt (epoch 45)
2025-07-03 12:53:47,162 - training - WARNING - 恢复至epoch 45
2025-07-03 12:53:47,329 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250703_125347.csv
2025-07-03 13:05:00,553 - training - INFO - 🚀 开始运行训练脚本
2025-07-03 13:05:00,553 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-03 13:05:00,553 - training - INFO - 📁 所有输出目录已创建
2025-07-03 13:05:01,544 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-03 13:05:04,233 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-03 13:05:04,516 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-03 13:05:04,781 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-03 13:05:04,791 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-03 13:05:05,320 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-03 13:05:07,978 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:08,501 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:08,671 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:08,837 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,005 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,170 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,335 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,497 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,568 - training - INFO - 🏁 Epoch 1 | 平均损失: 44.104861 | 跳过批次: 0
2025-07-03 13:05:09,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:09,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,249 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,388 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,527 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,666 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:10,850 - training - INFO - 🏁 Epoch 2 | 平均损失: 43.507843 | 跳过批次: 0
2025-07-03 13:05:11,102 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,242 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,382 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,523 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,662 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,801 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:11,942 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:12,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:12,123 - training - INFO - 🏁 Epoch 3 | 平均损失: 42.987338 | 跳过批次: 0
2025-07-03 13:05:12,379 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:12,614 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:12,754 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:12,896 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,035 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,174 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,313 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,436 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,498 - training - INFO - 🏁 Epoch 4 | 平均损失: 44.449246 | 跳过批次: 0
2025-07-03 13:05:13,753 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:13,895 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,034 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,176 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,315 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,455 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,594 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,717 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:14,781 - training - INFO - 🏁 Epoch 5 | 平均损失: 42.684273 | 跳过批次: 0
2025-07-03 13:05:17,537 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:17,678 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:17,817 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:17,959 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,098 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,378 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,501 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,559 - training - INFO - 🏁 Epoch 6 | 平均损失: 44.551948 | 跳过批次: 0
2025-07-03 13:05:18,815 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:18,955 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,095 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,237 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,376 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,515 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,654 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,777 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:19,836 - training - INFO - 🏁 Epoch 7 | 平均损失: 44.519283 | 跳过批次: 0
2025-07-03 13:05:20,086 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,225 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,368 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,513 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,652 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:20,931 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,055 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,116 - training - INFO - 🏁 Epoch 8 | 平均损失: 44.469683 | 跳过批次: 0
2025-07-03 13:05:21,375 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,517 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,657 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,798 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:21,938 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,077 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,338 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,399 - training - INFO - 🏁 Epoch 9 | 平均损失: 44.397004 | 跳过批次: 0
2025-07-03 13:05:22,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,790 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:22,930 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,072 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,212 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,351 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,490 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,614 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:23,673 - training - INFO - 🏁 Epoch 10 | 平均损失: 43.602141 | 跳过批次: 0
2025-07-03 13:05:32,141 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,563 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,702 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,841 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:32,981 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,166 - training - INFO - 🏁 Epoch 11 | 平均损失: 42.702826 | 跳过批次: 0
2025-07-03 13:05:33,419 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,698 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,839 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:33,979 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,258 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,381 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,440 - training - INFO - 🏁 Epoch 12 | 平均损失: 40.771559 | 跳过批次: 0
2025-07-03 13:05:34,699 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:34,979 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,122 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,261 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,401 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,542 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,668 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:35,727 - training - INFO - 🏁 Epoch 13 | 平均损失: 44.419214 | 跳过批次: 0
2025-07-03 13:05:35,983 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,125 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,265 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,407 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,547 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,686 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,825 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:36,947 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,011 - training - INFO - 🏁 Epoch 14 | 平均损失: 44.544673 | 跳过批次: 0
2025-07-03 13:05:37,265 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,406 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,546 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,690 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,830 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:37,970 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:38,109 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:38,236 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:38,301 - training - INFO - 🏁 Epoch 15 | 平均损失: 44.473227 | 跳过批次: 0
2025-07-03 13:05:54,452 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:54,596 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:54,736 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:54,881 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,160 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,300 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,484 - training - INFO - 🏁 Epoch 16 | 平均损失: 42.415377 | 跳过批次: 0
2025-07-03 13:05:55,731 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:55,870 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,009 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,291 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,430 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,569 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,693 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:56,752 - training - INFO - 🏁 Epoch 17 | 平均损失: 44.433208 | 跳过批次: 0
2025-07-03 13:05:57,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,148 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,288 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,430 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,569 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,708 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,848 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:57,971 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:58,033 - training - INFO - 🏁 Epoch 18 | 平均损失: 43.341025 | 跳过批次: 0
2025-07-03 13:05:58,283 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:58,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:58,657 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:58,799 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:58,938 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,077 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,339 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,400 - training - INFO - 🏁 Epoch 19 | 平均损失: 44.450989 | 跳过批次: 0
2025-07-03 13:05:59,653 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,793 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:05:59,932 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,075 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,215 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,354 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,493 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,616 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:00,680 - training - INFO - 🏁 Epoch 20 | 平均损失: 43.317764 | 跳过批次: 0
2025-07-03 13:06:16,771 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:16,916 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,056 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,198 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,337 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,477 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,618 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,741 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:17,806 - training - INFO - 🏁 Epoch 21 | 平均损失: 44.519600 | 跳过批次: 0
2025-07-03 13:06:18,067 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,207 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,489 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,629 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,768 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:18,976 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,099 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,159 - training - INFO - 🏁 Epoch 22 | 平均损失: 42.498318 | 跳过批次: 0
2025-07-03 13:06:19,410 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,689 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,833 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:19,973 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,113 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,253 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,378 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,439 - training - INFO - 🏁 Epoch 23 | 平均损失: 43.455481 | 跳过批次: 0
2025-07-03 13:06:20,693 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,833 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:20,974 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,116 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,256 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,535 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,661 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:21,724 - training - INFO - 🏁 Epoch 24 | 平均损失: 44.479977 | 跳过批次: 0
2025-07-03 13:06:21,975 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,117 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,256 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,399 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,539 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,679 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,819 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:22,942 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:23,005 - training - INFO - 🏁 Epoch 25 | 平均损失: 43.699771 | 跳过批次: 0
2025-07-03 13:06:38,907 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,048 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,189 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,331 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,471 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,611 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,751 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,875 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:39,937 - training - INFO - 🏁 Epoch 26 | 平均损失: 44.340501 | 跳过批次: 0
2025-07-03 13:06:40,191 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:40,332 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:40,472 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:40,615 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:40,754 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:40,895 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,034 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,158 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,221 - training - INFO - 🏁 Epoch 27 | 平均损失: 44.464067 | 跳过批次: 0
2025-07-03 13:06:41,473 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,617 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,757 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:41,902 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,041 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,181 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,443 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,503 - training - INFO - 🏁 Epoch 28 | 平均损失: 44.498592 | 跳过批次: 0
2025-07-03 13:06:42,756 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:42,899 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,038 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,180 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,320 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,460 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,599 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,722 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:43,782 - training - INFO - 🏁 Epoch 29 | 平均损失: 42.847397 | 跳过批次: 0
2025-07-03 13:06:44,036 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,176 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,315 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,461 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,600 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,741 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:44,880 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:45,003 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:06:45,066 - training - INFO - 🏁 Epoch 30 | 平均损失: 42.720683 | 跳过批次: 0
2025-07-03 13:07:01,637 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:01,781 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:01,921 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,204 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,344 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,485 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,608 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:02,668 - training - INFO - 🏁 Epoch 31 | 平均损失: 44.422047 | 跳过批次: 0
2025-07-03 13:07:02,923 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,063 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,345 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,484 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,624 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,765 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,891 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:03,950 - training - INFO - 🏁 Epoch 32 | 平均损失: 44.545971 | 跳过批次: 0
2025-07-03 13:07:04,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:04,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:04,561 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:04,702 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:04,848 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:04,988 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,128 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,309 - training - INFO - 🏁 Epoch 33 | 平均损失: 43.036176 | 跳过批次: 0
2025-07-03 13:07:05,571 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,711 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:05,995 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,135 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,274 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,414 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,537 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,602 - training - INFO - 🏁 Epoch 34 | 平均损失: 44.464498 | 跳过批次: 0
2025-07-03 13:07:06,858 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:06,998 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,138 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,419 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,698 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:07,885 - training - INFO - 🏁 Epoch 35 | 平均损失: 42.698716 | 跳过批次: 0
2025-07-03 13:07:24,543 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:24,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:24,822 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:24,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,245 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,385 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,567 - training - INFO - 🏁 Epoch 36 | 平均损失: 42.124812 | 跳过批次: 0
2025-07-03 13:07:25,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:25,963 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,245 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,664 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:26,850 - training - INFO - 🏁 Epoch 37 | 平均损失: 42.724442 | 跳过批次: 0
2025-07-03 13:07:27,106 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,247 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,528 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,667 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,807 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:27,946 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:28,070 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:28,135 - training - INFO - 🏁 Epoch 38 | 平均损失: 44.483658 | 跳过批次: 0
2025-07-03 13:07:28,385 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:28,526 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:28,666 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:28,808 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,042 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,182 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,321 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,467 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,529 - training - INFO - 🏁 Epoch 39 | 平均损失: 42.698611 | 跳过批次: 0
2025-07-03 13:07:29,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:29,932 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,072 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,355 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,635 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,757 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:30,819 - training - INFO - 🏁 Epoch 40 | 平均损失: 44.521633 | 跳过批次: 0
2025-07-03 13:07:49,154 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:49,295 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:49,436 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:49,588 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:49,728 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:49,867 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,131 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,192 - training - INFO - 🏁 Epoch 41 | 平均损失: 44.444026 | 跳过批次: 0
2025-07-03 13:07:50,444 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,586 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,726 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:50,868 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,148 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,287 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,413 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,472 - training - INFO - 🏁 Epoch 42 | 平均损失: 43.689185 | 跳过批次: 0
2025-07-03 13:07:51,727 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:51,868 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,434 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,574 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,698 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:52,759 - training - INFO - 🏁 Epoch 43 | 平均损失: 43.700212 | 跳过批次: 0
2025-07-03 13:07:53,016 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,158 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,303 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,446 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,585 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,725 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,867 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:53,989 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:54,049 - training - INFO - 🏁 Epoch 44 | 平均损失: 44.515040 | 跳过批次: 0
2025-07-03 13:07:54,307 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:54,447 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:54,587 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:54,732 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:54,872 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:55,012 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:55,152 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:55,277 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:07:55,338 - training - INFO - 🏁 Epoch 45 | 平均损失: 44.565913 | 跳过批次: 0
2025-07-03 13:08:12,439 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:12,581 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:12,724 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:12,867 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,007 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,146 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,287 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,411 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,470 - training - INFO - 🏁 Epoch 46 | 平均损失: 43.371284 | 跳过批次: 0
2025-07-03 13:08:13,722 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:13,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,003 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,145 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,285 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,563 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,687 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:14,747 - training - INFO - 🏁 Epoch 47 | 平均损失: 42.214425 | 跳过批次: 0
2025-07-03 13:08:14,998 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,139 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,279 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,564 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,705 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,848 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:15,973 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,032 - training - INFO - 🏁 Epoch 48 | 平均损失: 44.222564 | 跳过批次: 0
2025-07-03 13:08:16,284 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,427 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,568 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,710 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,850 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:16,990 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,129 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,253 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,312 - training - INFO - 🏁 Epoch 49 | 平均损失: 43.445516 | 跳过批次: 0
2025-07-03 13:08:17,567 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,707 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,847 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:17,989 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:18,129 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:18,269 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:18,409 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:18,532 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:08:18,593 - training - INFO - 🏁 Epoch 50 | 平均损失: 44.488740 | 跳过批次: 0
2025-07-03 13:08:18,775 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250703_130818.csv
2025-07-03 13:40:27,878 - training - INFO - 🚀 开始运行训练脚本
2025-07-03 13:40:27,878 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-03 13:40:27,878 - training - INFO - 📁 所有输出目录已创建
2025-07-03 13:40:28,869 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-03 13:40:31,577 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-03 13:40:31,869 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-03 13:40:32,150 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-03 13:40:32,161 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-03 13:40:32,689 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-03 13:40:34,287 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:34,504 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:34,675 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:34,844 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,012 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,347 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,504 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,574 - training - INFO - 🏁 Epoch 1 | 平均损失: 44.753640 | 跳过批次: 0
2025-07-03 13:40:35,823 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:35,965 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,104 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,246 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,386 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,525 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,665 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,802 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:36,866 - training - INFO - 🏁 Epoch 2 | 平均损失: 44.699570 | 跳过批次: 0
2025-07-03 13:40:37,118 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,258 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,397 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,549 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,689 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,829 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:37,968 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:38,091 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:38,153 - training - INFO - 🏁 Epoch 3 | 平均损失: 41.339152 | 跳过批次: 0
2025-07-03 13:40:38,400 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:38,636 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:38,775 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:38,918 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,057 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,337 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,462 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,521 - training - INFO - 🏁 Epoch 4 | 平均损失: 42.490014 | 跳过批次: 0
2025-07-03 13:40:39,771 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:39,911 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,052 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,334 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,474 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,613 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,737 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:40,800 - training - INFO - 🏁 Epoch 5 | 平均损失: 44.728353 | 跳过批次: 0
2025-07-03 13:40:43,805 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:43,949 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,089 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,231 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,371 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,511 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,650 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,774 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:44,835 - training - INFO - 🏁 Epoch 6 | 平均损失: 44.752751 | 跳过批次: 0
2025-07-03 13:40:45,085 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,229 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,370 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,513 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,653 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,793 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:45,933 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,057 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,117 - training - INFO - 🏁 Epoch 7 | 平均损失: 44.732481 | 跳过批次: 0
2025-07-03 13:40:46,366 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,507 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,648 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:46,934 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,074 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,214 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,339 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,397 - training - INFO - 🏁 Epoch 8 | 平均损失: 43.964788 | 跳过批次: 0
2025-07-03 13:40:47,648 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,788 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:47,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,358 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,499 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,622 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:48,681 - training - INFO - 🏁 Epoch 9 | 平均损失: 44.015359 | 跳过批次: 0
2025-07-03 13:40:48,932 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,074 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,215 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,357 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,497 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,638 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,778 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,903 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:49,962 - training - INFO - 🏁 Epoch 10 | 平均损失: 43.900218 | 跳过批次: 0
2025-07-03 13:40:58,961 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,104 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,245 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,389 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,530 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,671 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,813 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,940 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:40:59,999 - training - INFO - 🏁 Epoch 11 | 平均损失: 42.611715 | 跳过批次: 0
2025-07-03 13:41:00,251 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:00,392 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:00,533 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:00,676 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:00,817 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:00,957 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,220 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,279 - training - INFO - 🏁 Epoch 12 | 平均损失: 44.848381 | 跳过批次: 0
2025-07-03 13:41:01,528 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,670 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,811 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:01,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,095 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,235 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,376 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,502 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,561 - training - INFO - 🏁 Epoch 13 | 平均损失: 44.681167 | 跳过批次: 0
2025-07-03 13:41:02,813 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:02,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,239 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,379 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,535 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,676 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,800 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:03,860 - training - INFO - 🏁 Epoch 14 | 平均损失: 44.758574 | 跳过批次: 0
2025-07-03 13:41:04,110 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,391 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,534 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,674 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,815 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:04,955 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:05,081 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:05,144 - training - INFO - 🏁 Epoch 15 | 平均损失: 44.833866 | 跳过批次: 0
2025-07-03 13:41:21,359 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:21,502 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:21,644 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:21,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:21,926 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,066 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,207 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,333 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,392 - training - INFO - 🏁 Epoch 16 | 平均损失: 43.973984 | 跳过批次: 0
2025-07-03 13:41:22,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:22,927 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,070 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,351 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,491 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,614 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:23,675 - training - INFO - 🏁 Epoch 17 | 平均损失: 44.796676 | 跳过批次: 0
2025-07-03 13:41:23,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,069 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,209 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,352 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,492 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,632 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,771 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,895 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:24,955 - training - INFO - 🏁 Epoch 18 | 平均损失: 44.730561 | 跳过批次: 0
2025-07-03 13:41:25,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:25,351 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:25,584 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:25,727 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:25,868 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,148 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,273 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,335 - training - INFO - 🏁 Epoch 19 | 平均损失: 44.719724 | 跳过批次: 0
2025-07-03 13:41:26,587 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,729 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:26,870 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,014 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,154 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,294 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,434 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,558 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:27,620 - training - INFO - 🏁 Epoch 20 | 平均损失: 42.930317 | 跳过批次: 0
2025-07-03 13:41:44,068 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,352 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,636 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,776 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:44,917 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,041 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,100 - training - INFO - 🏁 Epoch 21 | 平均损失: 44.818929 | 跳过批次: 0
2025-07-03 13:41:45,353 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,493 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,634 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,781 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:45,921 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,062 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,202 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,325 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,389 - training - INFO - 🏁 Epoch 22 | 平均损失: 44.763896 | 跳过批次: 0
2025-07-03 13:41:46,640 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,781 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:46,920 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,063 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,342 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,482 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,606 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:47,666 - training - INFO - 🏁 Epoch 23 | 平均损失: 43.839478 | 跳过批次: 0
2025-07-03 13:41:47,919 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,059 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,201 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,345 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,488 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,632 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,775 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,914 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:48,973 - training - INFO - 🏁 Epoch 24 | 平均损失: 44.706678 | 跳过批次: 0
2025-07-03 13:41:49,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:49,367 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:49,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:49,651 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:49,791 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:49,932 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:50,073 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:50,197 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:41:50,256 - training - INFO - 🏁 Epoch 25 | 平均损失: 44.764385 | 跳过批次: 0
2025-07-03 13:42:05,904 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,049 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,193 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,339 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,621 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,887 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:06,945 - training - INFO - 🏁 Epoch 26 | 平均损失: 43.917175 | 跳过批次: 0
2025-07-03 13:42:07,199 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:07,339 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:07,480 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:07,623 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:07,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:07,903 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,043 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,167 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,226 - training - INFO - 🏁 Epoch 27 | 平均损失: 43.802195 | 跳过批次: 0
2025-07-03 13:42:08,481 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,622 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:08,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,046 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,186 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,326 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,451 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,511 - training - INFO - 🏁 Epoch 28 | 平均损失: 44.829688 | 跳过批次: 0
2025-07-03 13:42:09,765 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:09,905 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,045 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,189 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,329 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,469 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,610 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,733 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:10,792 - training - INFO - 🏁 Epoch 29 | 平均损失: 44.774238 | 跳过批次: 0
2025-07-03 13:42:11,047 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,187 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,328 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,471 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,612 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,753 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:11,893 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:12,017 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:12,075 - training - INFO - 🏁 Epoch 30 | 平均损失: 43.865126 | 跳过批次: 0
2025-07-03 13:42:28,682 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:28,824 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:28,969 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,114 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,427 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,569 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,709 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,832 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:29,897 - training - INFO - 🏁 Epoch 31 | 平均损失: 42.554752 | 跳过批次: 0
2025-07-03 13:42:30,150 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,291 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,434 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,577 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,717 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,857 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:30,997 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:31,122 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:31,184 - training - INFO - 🏁 Epoch 32 | 平均损失: 43.674483 | 跳过批次: 0
2025-07-03 13:42:31,437 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:31,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:31,719 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:31,861 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,001 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,141 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,404 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,463 - training - INFO - 🏁 Epoch 33 | 平均损失: 43.690108 | 跳过批次: 0
2025-07-03 13:42:32,713 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,856 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:32,997 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,141 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,281 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,561 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,685 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:33,750 - training - INFO - 🏁 Epoch 34 | 平均损失: 44.656582 | 跳过批次: 0
2025-07-03 13:42:34,005 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,146 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,379 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,523 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,663 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,805 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:34,945 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:35,069 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:35,131 - training - INFO - 🏁 Epoch 35 | 平均损失: 43.058917 | 跳过批次: 0
2025-07-03 13:42:51,390 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:51,532 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:51,673 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:51,816 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:51,956 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,238 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,361 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,422 - training - INFO - 🏁 Epoch 36 | 平均损失: 43.891800 | 跳过批次: 0
2025-07-03 13:42:52,677 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,820 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:52,961 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,103 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,244 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,384 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,524 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:53,715 - training - INFO - 🏁 Epoch 37 | 平均损失: 44.805218 | 跳过批次: 0
2025-07-03 13:42:53,970 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,111 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,252 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,537 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,677 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,817 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,941 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:54,999 - training - INFO - 🏁 Epoch 38 | 平均损失: 44.764511 | 跳过批次: 0
2025-07-03 13:42:55,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:55,393 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:55,534 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:55,677 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:55,912 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,054 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,319 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,379 - training - INFO - 🏁 Epoch 39 | 平均损失: 44.796277 | 跳过批次: 0
2025-07-03 13:42:56,634 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,780 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:56,923 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,070 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,212 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,353 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,620 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:42:57,678 - training - INFO - 🏁 Epoch 40 | 平均损失: 41.432518 | 跳过批次: 0
2025-07-03 13:43:14,037 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,179 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,321 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,464 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,605 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,745 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:14,886 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,011 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,072 - training - INFO - 🏁 Epoch 41 | 平均损失: 43.290694 | 跳过批次: 0
2025-07-03 13:43:15,324 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,466 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,607 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,751 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:15,891 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,031 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,171 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,295 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,353 - training - INFO - 🏁 Epoch 42 | 平均损失: 43.616046 | 跳过批次: 0
2025-07-03 13:43:16,608 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,750 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:16,889 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,032 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,173 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,313 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,453 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,577 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:17,638 - training - INFO - 🏁 Epoch 43 | 平均损失: 44.822299 | 跳过批次: 0
2025-07-03 13:43:17,886 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,027 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,168 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,311 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,451 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,591 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,730 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,854 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:18,915 - training - INFO - 🏁 Epoch 44 | 平均损失: 44.716698 | 跳过批次: 0
2025-07-03 13:43:19,171 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:19,483 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:19,626 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:19,770 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:19,913 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:20,053 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:20,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:20,319 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:20,379 - training - INFO - 🏁 Epoch 45 | 平均损失: 44.780951 | 跳过批次: 0
2025-07-03 13:43:36,934 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,078 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,218 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,363 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,504 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,644 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,909 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:37,970 - training - INFO - 🏁 Epoch 46 | 平均损失: 44.834665 | 跳过批次: 0
2025-07-03 13:43:38,219 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:38,360 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:38,501 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:38,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:38,787 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:38,928 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:39,071 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:39,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:39,253 - training - INFO - 🏁 Epoch 47 | 平均损失: 44.832400 | 跳过批次: 0
2025-07-03 13:43:39,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:39,648 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:39,789 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,116 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,256 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,538 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,662 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:40,723 - training - INFO - 🏁 Epoch 48 | 平均损失: 44.823875 | 跳过批次: 0
2025-07-03 13:43:40,985 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,125 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,265 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,407 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,547 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,688 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,828 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:41,952 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,012 - training - INFO - 🏁 Epoch 49 | 平均损失: 44.873584 | 跳过批次: 0
2025-07-03 13:43:42,267 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,408 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,550 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,693 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,834 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:42,974 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:43,115 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:43,238 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-03 13:43:43,297 - training - INFO - 🏁 Epoch 50 | 平均损失: 44.689134 | 跳过批次: 0
2025-07-03 13:43:43,472 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250703_134343.csv
2025-07-03 13:43:43,473 - training - INFO - ✅ 保存评估结果到: results/model1_eval/evaluation_20250703_134343.json
2025-07-03 13:43:43,473 - training - INFO - ✅ 同时保存CSV格式评估结果: results/model1_eval/evaluation_20250703_134343.csv
2025-07-03 13:43:43,517 - training - INFO - ✅ 保存评估结果到: results/model1_eval/evaluation_20250703_134343.json
2025-07-03 13:43:43,518 - training - INFO - ✅ 同时保存CSV格式评估结果: results/model1_eval/evaluation_20250703_134343.csv
2025-07-03 13:43:43,518 - training - INFO - 🔮 开始预测, 步数: 1000
2025-07-04 12:09:34,670 - training - INFO - 🚀 开始运行训练脚本
2025-07-04 12:09:34,670 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-04 12:09:34,670 - training - INFO - 📁 所有输出目录已创建
2025-07-04 12:09:35,721 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-04 12:09:38,544 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-04 12:09:38,846 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-04 12:09:39,132 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-04 12:09:39,149 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-04 12:09:40,114 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-04 12:09:42,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:42,283 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:42,449 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:42,616 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:42,782 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:42,948 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:43,114 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:43,273 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:43,347 - training - INFO - 🏁 Epoch 1 | 平均损失: 43.646069 | 跳过批次: 0
2025-07-04 12:09:43,612 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:43,753 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:43,893 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,034 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,173 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,313 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,453 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,576 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:44,644 - training - INFO - 🏁 Epoch 2 | 平均损失: 43.622089 | 跳过批次: 0
2025-07-04 12:09:44,902 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,042 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,183 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,325 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,464 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,603 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,742 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,864 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:45,928 - training - INFO - 🏁 Epoch 3 | 平均损失: 42.677231 | 跳过批次: 0
2025-07-04 12:09:46,187 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:46,423 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:46,564 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:46,707 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:46,847 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:46,987 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:47,127 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:47,250 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:47,317 - training - INFO - 🏁 Epoch 4 | 平均损失: 42.494679 | 跳过批次: 0
2025-07-04 12:09:47,580 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:47,723 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:47,863 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,005 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,144 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,284 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,547 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:48,610 - training - INFO - 🏁 Epoch 5 | 平均损失: 43.642095 | 跳过批次: 0
2025-07-04 12:09:53,164 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:53,307 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:53,447 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:53,589 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:53,729 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:53,869 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,131 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,197 - training - INFO - 🏁 Epoch 6 | 平均损失: 40.519645 | 跳过批次: 0
2025-07-04 12:09:54,457 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,597 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,737 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:54,879 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,018 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,159 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,298 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,486 - training - INFO - 🏁 Epoch 7 | 平均损失: 41.089285 | 跳过批次: 0
2025-07-04 12:09:55,743 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:55,885 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,025 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,167 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,307 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,446 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,585 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,708 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:56,772 - training - INFO - 🏁 Epoch 8 | 平均损失: 43.625926 | 跳过批次: 0
2025-07-04 12:09:57,032 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,173 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,313 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,457 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,597 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,737 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:57,998 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:58,063 - training - INFO - 🏁 Epoch 9 | 平均损失: 43.595549 | 跳过批次: 0
2025-07-04 12:09:58,322 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:58,464 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:58,605 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:58,747 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:58,887 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:59,026 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:59,166 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:59,288 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:09:59,352 - training - INFO - 🏁 Epoch 10 | 平均损失: 43.663815 | 跳过批次: 0
2025-07-04 12:10:09,210 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:09,353 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:09,492 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:09,634 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:09,773 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:09,912 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,052 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,175 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,240 - training - INFO - 🏁 Epoch 11 | 平均损失: 43.243448 | 跳过批次: 0
2025-07-04 12:10:10,502 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,642 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,782 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:10,924 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,064 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,203 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,342 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,466 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,531 - training - INFO - 🏁 Epoch 12 | 平均损失: 41.929205 | 跳过批次: 0
2025-07-04 12:10:11,796 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:11,935 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,074 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,356 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,495 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,635 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,758 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:12,823 - training - INFO - 🏁 Epoch 13 | 平均损失: 43.670926 | 跳过批次: 0
2025-07-04 12:10:13,087 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,227 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,367 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,647 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,786 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:13,925 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,047 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,117 - training - INFO - 🏁 Epoch 14 | 平均损失: 42.554903 | 跳过批次: 0
2025-07-04 12:10:14,381 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,522 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,661 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,805 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:14,944 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:15,084 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:15,223 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:15,344 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:15,409 - training - INFO - 🏁 Epoch 15 | 平均损失: 43.588236 | 跳过批次: 0
2025-07-04 12:10:27,153 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,296 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,435 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,716 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,855 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:27,995 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:28,119 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:28,185 - training - INFO - 🏁 Epoch 16 | 平均损失: 41.194525 | 跳过批次: 0
2025-07-04 12:10:28,449 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:28,589 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:28,729 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:28,871 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,009 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,149 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,288 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,410 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,476 - training - INFO - 🏁 Epoch 17 | 平均损失: 42.865576 | 跳过批次: 0
2025-07-04 12:10:29,730 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:29,870 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,010 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,152 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,291 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,430 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,570 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,692 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:30,754 - training - INFO - 🏁 Epoch 18 | 平均损失: 43.260463 | 跳过批次: 0
2025-07-04 12:10:31,014 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,155 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,394 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,538 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,676 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,815 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:31,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,077 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,140 - training - INFO - 🏁 Epoch 19 | 平均损失: 43.165681 | 跳过批次: 0
2025-07-04 12:10:32,403 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,543 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,825 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:32,968 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:33,108 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:33,247 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:33,370 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:33,441 - training - INFO - 🏁 Epoch 20 | 平均损失: 43.668377 | 跳过批次: 0
2025-07-04 12:10:48,251 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:48,393 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:48,532 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:48,673 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:48,813 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:48,952 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,092 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,216 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,288 - training - INFO - 🏁 Epoch 21 | 平均损失: 42.461030 | 跳过批次: 0
2025-07-04 12:10:49,543 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,824 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:49,966 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,105 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,246 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,385 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,508 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,571 - training - INFO - 🏁 Epoch 22 | 平均损失: 42.781299 | 跳过批次: 0
2025-07-04 12:10:50,831 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:50,970 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,113 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,257 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,396 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,540 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,679 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,801 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:51,867 - training - INFO - 🏁 Epoch 23 | 平均损失: 41.618892 | 跳过批次: 0
2025-07-04 12:10:52,129 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,270 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,410 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,554 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,694 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,833 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:52,973 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,096 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,158 - training - INFO - 🏁 Epoch 24 | 平均损失: 43.608312 | 跳过批次: 0
2025-07-04 12:10:53,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,557 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,698 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:53,979 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:54,119 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:54,259 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:54,434 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:10:54,500 - training - INFO - 🏁 Epoch 25 | 平均损失: 41.286693 | 跳过批次: 0
2025-07-04 12:11:08,721 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:08,865 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,008 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,151 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,431 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,571 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,695 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:09,759 - training - INFO - 🏁 Epoch 26 | 平均损失: 43.584019 | 跳过批次: 0
2025-07-04 12:11:10,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,162 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,303 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,445 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,586 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,728 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,868 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:10,992 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:11,056 - training - INFO - 🏁 Epoch 27 | 平均损失: 43.379852 | 跳过批次: 0
2025-07-04 12:11:11,315 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:11,456 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:11,599 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:11,744 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:11,884 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,024 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,163 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,286 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,351 - training - INFO - 🏁 Epoch 28 | 平均损失: 43.049901 | 跳过批次: 0
2025-07-04 12:11:12,611 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,752 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:12,892 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,034 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,173 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,312 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,451 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,574 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:13,639 - training - INFO - 🏁 Epoch 29 | 平均损失: 43.203268 | 跳过批次: 0
2025-07-04 12:11:13,901 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,043 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,183 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,325 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,466 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,606 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,746 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,868 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:14,933 - training - INFO - 🏁 Epoch 30 | 平均损失: 43.474876 | 跳过批次: 0
2025-07-04 12:11:29,770 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:29,926 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,080 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,231 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,371 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,510 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,649 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,772 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:30,876 - training - INFO - 🏁 Epoch 31 | 平均损失: 43.501543 | 跳过批次: 0
2025-07-04 12:11:31,139 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,284 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,568 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,708 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,848 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:31,988 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:32,110 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:32,173 - training - INFO - 🏁 Epoch 32 | 平均损失: 40.775549 | 跳过批次: 0
2025-07-04 12:11:32,437 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:32,579 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:32,719 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:32,861 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,000 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,140 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,279 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,402 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,466 - training - INFO - 🏁 Epoch 33 | 平均损失: 43.675432 | 跳过批次: 0
2025-07-04 12:11:33,727 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:33,867 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,006 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,149 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,289 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,428 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,566 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,688 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:34,752 - training - INFO - 🏁 Epoch 34 | 平均损失: 43.766817 | 跳过批次: 0
2025-07-04 12:11:35,012 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,152 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,292 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,433 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,573 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,713 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:35,852 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:36,029 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:36,095 - training - INFO - 🏁 Epoch 35 | 平均损失: 42.204663 | 跳过批次: 0
2025-07-04 12:11:50,732 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:50,875 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,015 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,181 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,321 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,461 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,604 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,728 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:51,793 - training - INFO - 🏁 Epoch 36 | 平均损失: 42.526928 | 跳过批次: 0
2025-07-04 12:11:52,053 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,194 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,334 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,476 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,616 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,755 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:52,894 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,017 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,085 - training - INFO - 🏁 Epoch 37 | 平均损失: 43.310006 | 跳过批次: 0
2025-07-04 12:11:53,346 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,487 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,628 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,773 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:53,912 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:54,051 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:54,284 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:54,407 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:54,473 - training - INFO - 🏁 Epoch 38 | 平均损失: 42.525903 | 跳过批次: 0
2025-07-04 12:11:54,736 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:54,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,016 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,158 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,297 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,438 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,578 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,700 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:55,763 - training - INFO - 🏁 Epoch 39 | 平均损失: 42.715706 | 跳过批次: 0
2025-07-04 12:11:56,024 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,201 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,341 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,483 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,622 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,763 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:56,903 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:57,025 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:11:57,090 - training - INFO - 🏁 Epoch 40 | 平均损失: 43.722609 | 跳过批次: 0
2025-07-04 12:12:11,694 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:11,836 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:11,977 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,260 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,399 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,539 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,662 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:12,730 - training - INFO - 🏁 Epoch 41 | 平均损失: 43.601504 | 跳过批次: 0
2025-07-04 12:12:13,000 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,140 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,280 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,421 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,562 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,701 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,840 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:13,962 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:14,025 - training - INFO - 🏁 Epoch 42 | 平均损失: 43.717623 | 跳过批次: 0
2025-07-04 12:12:14,296 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:14,436 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:14,575 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:14,725 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:14,864 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,003 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,142 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,266 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,330 - training - INFO - 🏁 Epoch 43 | 平均损失: 43.605258 | 跳过批次: 0
2025-07-04 12:12:15,596 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,736 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:15,876 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,020 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,160 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,299 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,500 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,622 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:16,690 - training - INFO - 🏁 Epoch 44 | 平均损失: 43.622583 | 跳过批次: 0
2025-07-04 12:12:16,954 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,095 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,234 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,376 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,516 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,655 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,794 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,917 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:17,981 - training - INFO - 🏁 Epoch 45 | 平均损失: 42.717979 | 跳过批次: 0
2025-07-04 12:12:32,839 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:32,981 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,121 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,263 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,403 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,543 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,683 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,806 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:33,871 - training - INFO - 🏁 Epoch 46 | 平均损失: 42.948012 | 跳过批次: 0
2025-07-04 12:12:34,132 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,273 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,417 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,559 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,699 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,839 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:34,978 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,101 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,165 - training - INFO - 🏁 Epoch 47 | 平均损失: 42.490894 | 跳过批次: 0
2025-07-04 12:12:35,424 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,565 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,705 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,851 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:35,991 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:36,131 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:36,271 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:36,394 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:36,459 - training - INFO - 🏁 Epoch 48 | 平均损失: 42.466738 | 跳过批次: 0
2025-07-04 12:12:36,721 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:36,862 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,003 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,146 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,286 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,426 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,565 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,689 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:37,754 - training - INFO - 🏁 Epoch 49 | 平均损失: 41.406852 | 跳过批次: 0
2025-07-04 12:12:38,015 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,156 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,298 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,440 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,579 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,720 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,860 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:38,983 - training - WARNING - 修复梯度异常层: ['input_fc.weight', 'input_fc.bias', 'layers.0.self_attn.in_proj_weight']...
2025-07-04 12:12:39,049 - training - INFO - 🏁 Epoch 50 | 平均损失: 43.663775 | 跳过批次: 0
2025-07-04 12:12:39,449 - training - INFO - ✅ 保存结构化输出数据到: data/processed/model1_output/predictions_20250704_121239.csv
2025-07-04 12:12:39,450 - training - INFO - ✅ 保存评估结果到: results/model1_eval/evaluation_20250704_121239.json
2025-07-04 12:12:39,451 - training - INFO - ✅ 同时保存CSV格式评估结果: results/model1_eval/evaluation_20250704_121239.csv
2025-07-04 12:12:39,517 - training - INFO - ✅ 保存评估结果到: results/model1_eval/evaluation_20250704_121239.json
2025-07-04 12:12:39,526 - training - INFO - ✅ 同时保存CSV格式评估结果: results/model1_eval/evaluation_20250704_121239.csv
2025-07-04 12:12:39,526 - training - INFO - 🔮 开始预测, 步数: 1000
2025-07-04 12:12:39,534 - training - INFO - ✅ 保存预测结果到: data/processed/model1_output/predictions.csv
2025-07-04 12:12:39,534 - training - INFO - 🎉 训练和评估完成!
2025-07-08 12:24:34,395 - training - INFO - 🚀 开始运行训练脚本
2025-07-08 12:24:34,448 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-08 12:24:34,448 - training - INFO - 📁 所有输出目录已创建
2025-07-08 12:24:36,231 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-08 12:24:41,154 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-08 12:24:41,437 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-08 12:24:42,806 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-08 12:24:42,820 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-08 12:24:50,167 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-08 12:30:29,479 - training - INFO - 🚀 开始运行训练脚本
2025-07-08 12:30:29,479 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-08 12:30:29,479 - training - INFO - 📁 所有输出目录已创建
2025-07-08 12:30:30,470 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-08 12:30:33,019 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-08 12:30:33,304 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-08 12:30:33,555 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-08 12:30:33,567 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-08 12:30:34,102 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-08 12:33:34,813 - training - INFO - 🚀 开始运行训练脚本
2025-07-08 12:33:34,813 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-08 12:33:34,814 - training - INFO - 📁 所有输出目录已创建
2025-07-08 12:33:35,806 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-08 12:33:38,353 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-08 12:33:38,635 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-08 12:33:38,886 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-08 12:33:38,897 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-08 12:33:39,438 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-08 12:35:46,569 - training - INFO - 🚀 开始运行训练脚本
2025-07-08 12:35:46,569 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-08 12:35:46,569 - training - INFO - 📁 所有输出目录已创建
2025-07-08 12:35:47,561 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-08 12:35:50,103 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-08 12:35:50,387 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-08 12:35:50,638 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-08 12:35:50,648 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-08 12:35:51,179 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-08 12:38:11,909 - training - INFO - 🚀 开始运行训练脚本
2025-07-08 12:38:11,909 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-08 12:38:11,909 - training - INFO - 📁 所有输出目录已创建
2025-07-08 12:38:12,905 - training - INFO - ⚙️ 加载预训练模型: models/model2/best_model.pt
2025-07-08 12:38:15,443 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-08 12:38:15,727 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-08 12:38:15,989 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-08 12:38:16,001 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-08 12:38:16,532 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-09 13:23:59,142 - training - INFO - 🚀 开始运行训练脚本
2025-07-09 13:23:59,169 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-09 13:23:59,169 - training - INFO - 📁 所有输出目录已创建
2025-07-09 13:24:00,518 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-09 13:24:03,087 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-09 13:24:03,370 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-09 13:24:04,161 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-09 13:24:04,172 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-09 13:24:10,735 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-09 13:32:49,787 - training - INFO - 🚀 开始运行训练脚本
2025-07-09 13:32:49,787 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-09 13:32:49,787 - training - INFO - 📁 所有输出目录已创建
2025-07-09 13:32:50,779 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-09 13:32:53,383 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-09 13:32:53,665 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-09 13:32:53,918 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-09 13:32:53,930 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-09 13:32:54,455 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-09 13:35:45,619 - training - INFO - 🚀 开始运行训练脚本
2025-07-09 13:35:45,619 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-09 13:35:45,619 - training - INFO - 📁 所有输出目录已创建
2025-07-09 13:35:46,604 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-09 13:35:49,145 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-09 13:35:49,428 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-09 13:35:49,683 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-09 13:35:49,695 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-09 13:35:50,228 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-09 13:43:01,953 - training - INFO - 🚀 开始运行训练脚本
2025-07-09 13:43:01,953 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-09 13:43:01,954 - training - INFO - 📁 所有输出目录已创建
2025-07-09 13:43:02,939 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-09 13:43:05,480 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-09 13:43:05,763 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-09 13:43:06,024 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-09 13:43:06,035 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-09 13:43:06,564 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 11:58:23,726 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 11:58:23,759 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 11:58:23,759 - training - INFO - 📁 所有输出目录已创建
2025-07-22 11:58:25,324 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 11:58:56,246 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 11:58:56,532 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 11:58:57,498 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 11:58:57,510 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 11:59:04,102 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 12:30:46,758 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 12:30:46,758 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 12:30:46,758 - training - INFO - 📁 所有输出目录已创建
2025-07-22 12:30:47,745 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 12:30:50,355 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 12:30:50,639 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 12:30:50,890 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 12:30:50,902 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 12:30:51,429 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 12:41:03,546 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 12:41:03,994 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 12:41:04,146 - training - INFO - 📁 所有输出目录已创建
2025-07-22 12:41:12,103 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 12:43:09,030 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 12:43:11,146 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 12:43:25,867 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 12:43:27,906 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 12:44:37,001 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 12:44:56,677 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
2025-07-22 12:44:56,736 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-22 12:50:18,960 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 12:50:18,984 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 12:50:18,984 - training - INFO - 📁 所有输出目录已创建
2025-07-22 12:50:21,046 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 12:54:00,593 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 12:54:03,099 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 12:54:31,887 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 12:54:34,440 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 12:55:19,179 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 12:55:34,241 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
2025-07-22 12:55:34,277 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-22 13:14:32,791 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 13:14:32,808 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 13:14:32,831 - training - INFO - 📁 所有输出目录已创建
2025-07-22 13:14:33,865 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 13:19:24,463 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 13:19:24,487 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 13:19:24,487 - training - INFO - 📁 所有输出目录已创建
2025-07-22 13:19:26,310 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 13:25:45,796 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 13:25:51,677 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 13:28:08,859 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 13:28:14,568 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 13:29:51,359 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 13:30:11,757 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
2025-07-22 13:30:11,776 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-22 13:36:19,910 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 13:36:19,938 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 13:36:19,961 - training - INFO - 📁 所有输出目录已创建
2025-07-22 13:36:25,483 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 13:40:04,969 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 13:40:06,385 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 13:40:18,013 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 13:40:18,331 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 13:40:37,117 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 13:40:57,316 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
2025-07-22 13:40:57,383 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-22 13:48:54,184 - training - INFO - 🚀 开始运行训练脚本
2025-07-22 13:48:54,207 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-22 13:48:54,230 - training - INFO - 📁 所有输出目录已创建
2025-07-22 13:48:56,772 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-22 13:51:23,483 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-22 13:51:23,883 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-22 13:51:28,249 - training - INFO - ⚙️ 模型运行在: cuda
2025-07-22 13:51:28,636 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-22 13:51:45,106 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-22 13:52:01,298 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
2025-07-22 13:52:01,338 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 09:08:06,609 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 09:08:07,690 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 09:08:07,691 - training - INFO - 📁 所有输出目录已创建
2025-07-24 09:08:25,299 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 09:14:35,761 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 09:14:36,132 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 09:14:38,408 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 09:14:38,557 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 09:14:53,182 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 09:15:06,168 - training - ERROR - 前向传播异常: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)
2025-07-24 09:15:06,202 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 09:19:08,172 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 09:19:08,203 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 09:19:08,255 - training - INFO - 📁 所有输出目录已创建
2025-07-24 09:19:31,180 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 09:22:02,588 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 09:22:03,367 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 09:22:04,454 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 09:22:05,290 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 09:22:40,453 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 09:44:32,580 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 09:44:32,608 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 09:44:32,608 - training - INFO - 📁 所有输出目录已创建
2025-07-24 09:45:18,291 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 09:47:17,442 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 09:47:18,414 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 09:47:20,191 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 09:47:21,203 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 09:48:02,486 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 09:48:17,304 - training - ERROR - 前向传播异常: The size of tensor a (32000) must match the size of tensor b (16) at non-singleton dimension 2
2025-07-24 09:48:17,338 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 09:56:25,860 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 09:56:25,871 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 09:56:25,871 - training - INFO - 📁 所有输出目录已创建
2025-07-24 09:56:27,132 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 10:01:21,293 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 10:01:30,785 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 10:01:48,515 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 10:01:54,555 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 10:03:46,320 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 10:04:06,667 - training - ERROR - 前向传播异常: The size of tensor a (32000) must match the size of tensor b (16) at non-singleton dimension 2
2025-07-24 10:04:06,701 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 12:19:04,701 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 12:19:04,701 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 12:19:04,701 - training - INFO - 📁 所有输出目录已创建
2025-07-24 12:19:05,703 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 12:19:35,861 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 12:19:36,175 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 12:19:36,246 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 12:19:36,270 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 12:19:39,841 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 12:19:48,719 - training - ERROR - 前向传播异常: The size of tensor a (32000) must match the size of tensor b (16) at non-singleton dimension 1
2025-07-24 12:19:48,785 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 12:22:02,262 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 12:22:02,262 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 12:22:02,290 - training - INFO - 📁 所有输出目录已创建
2025-07-24 12:22:03,306 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 12:22:32,881 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 12:22:33,210 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 12:22:33,253 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 12:22:33,273 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 12:22:37,702 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 12:22:46,411 - training - ERROR - 前向传播异常: The size of tensor a (32000) must match the size of tensor b (16) at non-singleton dimension 1
2025-07-24 12:22:46,447 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 21:17:46,908 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 21:17:46,931 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 21:17:46,931 - training - INFO - 📁 所有输出目录已创建
2025-07-24 21:17:47,942 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 21:18:20,241 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 21:18:20,645 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 21:18:21,030 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 21:18:21,244 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 21:18:27,014 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 21:18:35,769 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (16x1 and 773x1024)
2025-07-24 21:18:35,804 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 21:28:55,167 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 21:28:55,167 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 21:28:55,167 - training - INFO - 📁 所有输出目录已创建
2025-07-24 21:28:56,181 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 21:29:26,799 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 21:29:27,167 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 21:29:27,236 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 21:29:27,258 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 21:29:30,752 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 21:29:39,042 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (16x1 and 773x1024)
2025-07-24 21:29:39,084 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-24 22:11:45,939 - training - INFO - 🚀 开始运行训练脚本
2025-07-24 22:11:45,940 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-24 22:11:45,940 - training - INFO - 📁 所有输出目录已创建
2025-07-24 22:11:46,987 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-24 22:12:16,820 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-24 22:12:17,156 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-24 22:12:17,322 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-24 22:12:17,367 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-24 22:12:21,185 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-24 22:12:28,958 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x1024)
2025-07-24 22:12:28,999 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-28 12:24:42,081 - training - INFO - 🚀 开始运行训练脚本
2025-07-28 12:24:42,115 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-28 12:24:42,115 - training - INFO - 📁 所有输出目录已创建
2025-07-28 12:24:43,494 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-28 12:25:18,937 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-28 12:25:19,254 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-28 12:25:19,402 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-28 12:25:19,690 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-28 12:25:28,778 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-28 12:25:37,202 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x1024)
2025-07-28 12:25:37,239 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-28 12:48:14,462 - training - INFO - 🚀 开始运行训练脚本
2025-07-28 12:48:14,481 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-28 12:48:14,482 - training - INFO - 📁 所有输出目录已创建
2025-07-28 12:48:15,734 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-28 12:48:37,093 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-28 12:48:37,444 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-28 12:48:37,510 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-28 12:48:37,888 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-28 12:48:42,759 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-28 12:48:50,384 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x1024)
2025-07-28 12:48:50,447 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 12:43:02,124 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 12:43:02,199 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 12:43:02,200 - training - INFO - 📁 所有输出目录已创建
2025-07-29 12:43:03,499 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 12:43:06,047 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 12:43:06,323 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-29 12:43:06,403 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 12:43:06,416 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 12:43:12,815 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 12:43:20,063 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x1024)
2025-07-29 12:43:20,087 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 12:49:52,043 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 12:49:52,043 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 12:49:52,043 - training - INFO - 📁 所有输出目录已创建
2025-07-29 12:49:53,034 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 12:49:55,600 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 12:49:55,876 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=1, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-29 12:49:55,891 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 12:49:55,904 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 12:49:56,438 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 12:50:02,041 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x773 and 1x1024)
2025-07-29 12:50:02,066 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 12:53:05,952 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 12:53:05,952 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 12:53:05,952 - training - INFO - 📁 所有输出目录已创建
2025-07-29 12:53:06,951 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 12:53:09,509 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 12:53:09,786 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-29 12:53:09,802 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 12:53:09,815 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 12:53:10,344 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 12:53:16,819 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x30)
2025-07-29 12:53:16,839 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 13:01:02,176 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 13:01:02,176 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 13:01:02,177 - training - INFO - 📁 所有输出目录已创建
2025-07-29 13:01:03,166 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 13:01:05,723 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 13:01:05,997 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-29 13:01:06,013 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 13:01:06,024 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 13:01:06,561 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 13:01:13,107 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x30)
2025-07-29 13:01:13,127 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 13:06:58,298 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 13:06:58,298 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 13:06:58,298 - training - INFO - 📁 所有输出目录已创建
2025-07-29 13:06:59,283 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 13:07:01,839 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 13:07:02,115 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=1, bias=True)
)
2025-07-29 13:07:02,131 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 13:07:02,144 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 13:07:02,674 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 13:07:09,207 - training - ERROR - 前向传播异常: mat1 and mat2 shapes cannot be multiplied (480x1 and 773x30)
2025-07-29 13:07:09,229 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-29 13:11:56,687 - training - INFO - 🚀 开始运行训练脚本
2025-07-29 13:11:56,687 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-29 13:11:56,687 - training - INFO - 📁 所有输出目录已创建
2025-07-29 13:11:57,673 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-29 13:12:00,232 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-29 13:12:00,505 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-29 13:12:00,521 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-29 13:12:00,533 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-29 13:12:01,060 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-29 13:12:07,688 - training - ERROR - 前向传播异常: shape '[16, 30, 773]' is invalid for input of size 16
2025-07-29 13:12:07,708 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-30 12:12:19,093 - training - INFO - 🚀 开始运行训练脚本
2025-07-30 12:12:19,123 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-30 12:12:19,123 - training - INFO - 📁 所有输出目录已创建
2025-07-30 12:12:20,358 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-30 12:12:22,892 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-30 12:12:23,166 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-30 12:12:23,236 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-30 12:12:23,251 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-30 12:12:29,315 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-30 12:12:36,681 - training - ERROR - 前向传播异常: shape '[16, 30, 773]' is invalid for input of size 16
2025-07-30 12:12:36,701 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-07-30 12:25:30,145 - training - INFO - 🚀 开始运行训练脚本
2025-07-30 12:25:30,145 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-30 12:25:30,145 - training - INFO - 📁 所有输出目录已创建
2025-07-30 12:25:31,173 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-30 12:25:33,722 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-30 12:25:33,997 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-30 12:25:34,013 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-30 12:25:34,028 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-30 12:25:34,568 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-30 12:28:14,556 - training - INFO - 🚀 开始运行训练脚本
2025-07-30 12:28:14,556 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-30 12:28:14,556 - training - INFO - 📁 所有输出目录已创建
2025-07-30 12:28:15,575 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-30 12:28:18,147 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-30 12:28:18,423 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-30 12:28:18,439 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-30 12:28:18,454 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-30 12:28:18,991 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-31 12:11:00,883 - training - INFO - 🚀 开始运行训练脚本
2025-07-31 12:11:00,914 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-31 12:11:00,914 - training - INFO - 📁 所有输出目录已创建
2025-07-31 12:11:02,052 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-31 12:11:04,615 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-31 12:11:04,893 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-31 12:11:04,984 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-31 12:11:04,996 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-31 12:11:11,703 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-31 12:22:44,777 - training - INFO - 🚀 开始运行训练脚本
2025-07-31 12:22:44,777 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-31 12:22:44,777 - training - INFO - 📁 所有输出目录已创建
2025-07-31 12:22:45,770 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-31 12:22:50,065 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-31 12:22:50,353 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-31 12:22:50,397 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-31 12:22:50,410 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-31 12:22:55,564 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-31 12:39:38,732 - training - INFO - 🚀 开始运行训练脚本
2025-07-31 12:39:38,732 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-31 12:39:38,732 - training - INFO - 📁 所有输出目录已创建
2025-07-31 12:39:39,724 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-31 12:39:42,601 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-31 12:39:42,880 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-31 12:39:42,896 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-31 12:39:42,908 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-31 12:39:43,441 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-31 12:46:41,901 - training - INFO - 🚀 开始运行训练脚本
2025-07-31 12:46:41,901 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-31 12:46:41,901 - training - INFO - 📁 所有输出目录已创建
2025-07-31 12:46:42,890 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-31 12:46:48,003 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-31 12:46:48,284 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-31 12:46:48,299 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-31 12:46:48,313 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-31 12:46:48,852 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-07-31 13:22:32,152 - training - INFO - 🚀 开始运行训练脚本
2025-07-31 13:22:32,152 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-07-31 13:22:32,152 - training - INFO - 📁 所有输出目录已创建
2025-07-31 13:22:33,140 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-07-31 13:22:35,938 - training - INFO - ✅ 成功加载预训练模型权重
2025-07-31 13:22:36,218 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-07-31 13:22:36,234 - training - INFO - ⚙️ 模型运行在: cpu
2025-07-31 13:22:36,248 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-07-31 13:22:36,798 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-01 13:52:48,224 - training - INFO - 🚀 开始运行训练脚本
2025-08-01 13:52:48,255 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-01 13:52:48,255 - training - INFO - 📁 所有输出目录已创建
2025-08-01 13:52:49,534 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-01 13:53:22,730 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-01 13:53:23,052 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-01 13:53:23,283 - training - INFO - ⚙️ 模型运行在: cpu
2025-08-01 13:53:23,295 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-01 13:53:29,036 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-02 17:15:34,943 - training - INFO - 🚀 开始运行训练脚本
2025-08-02 17:15:34,984 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-02 17:15:34,984 - training - INFO - 📁 所有输出目录已创建
2025-08-02 17:15:36,391 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-02 17:16:18,422 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-02 17:16:18,833 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-02 17:16:19,701 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-02 17:16:20,171 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-02 17:16:27,231 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-02 17:40:29,290 - training - INFO - 🚀 开始运行训练脚本
2025-08-02 17:40:29,290 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-02 17:40:29,290 - training - INFO - 📁 所有输出目录已创建
2025-08-02 17:40:30,462 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-02 17:41:04,016 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-02 17:41:04,377 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-02 17:41:04,967 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-02 17:41:04,982 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-02 17:41:05,636 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 11:58:13,490 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 11:58:13,522 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 11:58:13,522 - training - INFO - 📁 所有输出目录已创建
2025-08-04 11:58:14,812 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 11:58:48,257 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 11:58:48,580 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 11:58:49,556 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 11:58:49,566 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 11:58:55,356 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:33:28,045 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:33:28,045 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:33:28,045 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:33:29,176 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:33:32,010 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:33:32,334 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:33:32,592 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:33:32,604 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:33:33,217 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:34:32,133 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:34:32,133 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:34:32,134 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:34:33,281 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:34:36,116 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:34:36,438 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:34:36,697 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:34:36,709 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:34:37,325 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:35:09,103 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:35:09,103 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:35:09,103 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:35:10,243 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:35:13,103 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:35:13,425 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:35:13,682 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:35:13,693 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:35:14,302 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:38:36,096 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:38:36,097 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:38:36,097 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:38:37,234 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:38:40,062 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:38:40,384 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:38:40,657 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:38:40,673 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:38:41,312 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:39:49,591 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:39:49,592 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:39:49,592 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:39:50,733 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:39:53,613 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:39:53,935 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:39:54,191 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:39:54,202 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:39:54,824 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 12:42:14,047 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 12:42:14,048 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 12:42:14,048 - training - INFO - 📁 所有输出目录已创建
2025-08-04 12:42:15,187 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 12:42:18,054 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 12:42:18,376 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 12:42:18,640 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 12:42:18,651 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 12:42:19,269 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 13:01:48,204 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 13:01:48,204 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 13:01:48,204 - training - INFO - 📁 所有输出目录已创建
2025-08-04 13:01:49,344 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 13:01:52,264 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 13:01:52,585 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 13:01:52,843 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 13:01:52,854 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 13:01:53,465 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-04 13:10:12,460 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 13:10:12,460 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 13:10:12,460 - training - INFO - 📁 所有输出目录已创建
2025-08-04 13:17:08,449 - training - INFO - 🚀 开始运行训练脚本
2025-08-04 13:17:08,449 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-04 13:17:08,449 - training - INFO - 📁 所有输出目录已创建
2025-08-04 13:17:09,590 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-04 13:17:12,554 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-04 13:17:12,876 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-04 13:17:13,134 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-04 13:17:13,146 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-04 13:17:13,765 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-05 12:17:06,285 - training - INFO - 🚀 开始运行训练脚本
2025-08-05 12:17:06,316 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-05 12:17:06,316 - training - INFO - 📁 所有输出目录已创建
2025-08-05 12:17:07,593 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-05 12:17:41,016 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-05 12:17:41,337 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-05 12:17:42,343 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-05 12:17:42,358 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-05 13:24:02,009 - training - INFO - 🚀 开始运行训练脚本
2025-08-05 13:24:02,010 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-05 13:24:02,010 - training - INFO - 📁 所有输出目录已创建
2025-08-05 13:24:03,149 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-05 13:24:06,061 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-05 13:24:06,382 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-05 13:24:06,642 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-05 13:24:06,649 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-05 13:40:37,329 - training - INFO - 🚀 开始运行训练脚本
2025-08-05 13:40:37,329 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-05 13:40:37,329 - training - INFO - 📁 所有输出目录已创建
2025-08-05 13:40:38,467 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-05 13:40:41,375 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-05 13:40:41,696 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-05 13:40:41,963 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-05 13:40:41,973 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-05 13:43:29,140 - training - INFO - 🚀 开始运行训练脚本
2025-08-05 13:43:29,141 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-05 13:43:29,141 - training - INFO - 📁 所有输出目录已创建
2025-08-05 13:43:30,279 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-05 13:43:33,215 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-05 13:43:33,536 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-05 13:43:33,796 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-05 13:43:33,805 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-05 13:43:39,432 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 11:53:27,314 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 11:53:27,345 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 11:53:27,345 - training - INFO - 📁 所有输出目录已创建
2025-08-06 11:53:28,623 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 11:54:01,488 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 11:54:01,810 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 11:54:02,787 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 11:54:02,795 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 11:54:08,862 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 11:57:30,294 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 11:57:30,294 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 11:57:30,294 - training - INFO - 📁 所有输出目录已创建
2025-08-06 11:57:31,427 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 11:57:34,252 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 11:57:34,574 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 11:57:34,841 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 11:57:34,849 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 11:57:35,456 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 12:01:36,169 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 12:01:36,169 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 12:01:36,169 - training - INFO - 📁 所有输出目录已创建
2025-08-06 12:01:37,305 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 12:01:40,152 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 12:01:40,474 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 12:01:40,744 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 12:01:40,751 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 12:01:41,368 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 12:01:48,514 - training - ERROR - 前向传播异常: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
2025-08-06 12:01:48,536 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-08-06 12:27:33,181 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 12:27:33,181 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 12:27:33,182 - training - INFO - 📁 所有输出目录已创建
2025-08-06 12:30:23,667 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 12:30:23,667 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 12:30:23,667 - training - INFO - 📁 所有输出目录已创建
2025-08-06 12:30:24,994 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 12:30:27,697 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 12:30:28,024 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 12:30:28,025 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 12:30:28,034 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 12:30:28,644 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 12:44:18,205 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 12:44:18,205 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 12:44:18,205 - training - INFO - 📁 所有输出目录已创建
2025-08-06 12:44:19,524 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 12:44:22,224 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 12:44:22,541 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 12:44:22,542 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 12:44:22,550 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 12:44:23,155 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 13:15:44,130 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 13:15:44,130 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 13:15:44,130 - training - INFO - 📁 所有输出目录已创建
2025-08-06 13:15:45,456 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 13:15:48,167 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 13:15:48,494 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 13:15:48,495 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 13:15:48,504 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 13:15:49,124 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 13:31:18,338 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 13:31:18,339 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 13:31:18,339 - training - INFO - 📁 所有输出目录已创建
2025-08-06 13:31:19,683 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 13:31:22,469 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 13:31:22,827 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 13:31:22,828 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 13:31:22,837 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 13:31:23,457 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-06 13:41:15,329 - training - INFO - 🚀 开始运行训练脚本
2025-08-06 13:41:15,329 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-06 13:41:15,329 - training - INFO - 📁 所有输出目录已创建
2025-08-06 13:41:16,657 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-06 13:41:19,355 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-06 13:41:19,679 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-06 13:41:19,680 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-06 13:41:19,690 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-06 13:41:20,301 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 11:56:37,021 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 11:56:37,080 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 11:56:37,080 - training - INFO - 📁 所有输出目录已创建
2025-08-07 11:56:38,617 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 11:57:11,462 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 11:57:11,818 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 11:57:11,819 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 11:57:11,830 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 11:57:18,424 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:10:23,553 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:10:23,553 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:10:23,553 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:10:24,890 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:10:27,633 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:10:27,963 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:10:27,965 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:10:27,976 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:10:28,767 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:13:03,351 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:13:03,351 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:13:03,351 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:13:04,682 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:13:07,510 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:13:07,840 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:13:07,841 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:13:07,850 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:13:08,622 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:17:13,380 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:17:13,380 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:17:13,383 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:17:14,709 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:17:17,427 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:17:17,789 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:17:17,790 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:17:17,800 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:17:18,409 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:21:10,933 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:21:10,933 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:21:10,933 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:21:12,261 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:21:15,116 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:21:15,446 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:21:15,447 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:21:15,459 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:21:16,074 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:26:39,165 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:26:39,165 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:26:39,165 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:26:40,518 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:26:43,361 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:26:43,719 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:26:43,720 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:26:43,731 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:26:44,346 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:30:55,903 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:30:55,903 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:30:55,903 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:30:57,237 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:30:59,981 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:31:00,315 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:31:00,316 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:31:00,327 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:31:00,934 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:34:20,689 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:34:20,689 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:34:20,689 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:34:22,015 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:34:24,762 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:34:25,091 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:34:25,092 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:34:25,103 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:34:25,716 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:36:45,158 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:36:45,158 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:36:45,158 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:36:46,482 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:36:49,185 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:36:49,511 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:36:49,512 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:36:49,523 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:36:50,130 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:46:27,925 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:46:27,925 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:46:27,925 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:46:29,249 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:46:31,959 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:46:32,286 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:46:32,287 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:46:32,298 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:46:32,904 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 12:46:56,656 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 12:46:56,656 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 12:46:56,656 - training - INFO - 📁 所有输出目录已创建
2025-08-07 12:46:57,983 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 12:47:00,722 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 12:47:01,050 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 12:47:01,051 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 12:47:01,062 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 12:47:01,675 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:01:28,433 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:01:28,433 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:01:28,433 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:01:29,763 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:01:32,579 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:01:32,904 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:01:32,906 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:01:32,916 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:01:33,532 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:01:52,719 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:01:52,719 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:01:52,719 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:01:54,044 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:01:56,869 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:01:57,194 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:01:57,195 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:01:57,206 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:01:57,817 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:09:47,562 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:09:47,562 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:09:47,562 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:09:48,908 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:09:51,706 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:09:52,072 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:09:52,073 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:09:52,083 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:09:52,690 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:12:48,557 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:12:48,557 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:12:48,557 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:12:49,883 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:12:52,638 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:12:52,967 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:12:52,968 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:12:52,978 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:12:53,590 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:16:48,731 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:16:48,731 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:16:48,731 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:16:50,059 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:16:52,932 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:16:53,280 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:16:53,281 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:16:53,292 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:16:53,909 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:17:56,111 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:17:56,111 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:17:56,111 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:17:57,425 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:18:00,173 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:18:00,485 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:18:00,486 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:18:00,497 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:18:01,104 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:19:43,839 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:19:43,839 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:19:43,839 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:19:45,192 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:19:48,143 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:19:48,506 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:19:48,507 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:19:48,517 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:19:49,126 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:25:06,210 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:25:06,210 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:25:06,210 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:25:07,537 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:25:10,342 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:25:10,668 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:25:10,669 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:25:10,680 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:25:11,316 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:25:18,510 - training - ERROR - 前向传播异常: a Tensor with 16 elements cannot be converted to Scalar
2025-08-07 13:25:18,531 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-08-07 13:27:06,359 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:27:06,359 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:27:06,359 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:27:07,711 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:27:10,659 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:27:11,015 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:27:11,017 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:27:11,027 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:27:11,651 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:27:18,715 - training - ERROR - 前向传播异常: a Tensor with 16 elements cannot be converted to Scalar
2025-08-07 13:27:18,738 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-08-07 13:36:53,568 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:36:53,568 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:36:53,568 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:36:54,894 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:36:57,696 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:36:58,052 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:36:58,053 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:36:58,063 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:36:58,671 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:37:05,623 - training - ERROR - 前向传播异常: a Tensor with 16 elements cannot be converted to Scalar
2025-08-07 13:37:05,645 - training - ERROR - 异常位置: /home/liyakun/miniconda3/envs/llama_factory/lib/python3.12/contextlib.py:158
2025-08-07 13:50:16,605 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:50:16,605 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:50:16,605 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:50:17,932 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:50:20,742 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:50:21,096 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:50:21,097 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:50:21,106 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:50:21,718 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:51:52,923 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:51:52,923 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:51:52,923 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:51:54,255 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:51:57,072 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:51:57,430 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:51:57,431 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:51:57,442 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:51:58,046 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
2025-08-07 13:52:44,015 - training - INFO - 🚀 开始运行训练脚本
2025-08-07 13:52:44,015 - training - INFO - 📄 使用配置文件: configs/model1.yaml
2025-08-07 13:52:44,015 - training - INFO - 📁 所有输出目录已创建
2025-08-07 13:52:45,341 - training - INFO - ⚙️ 加载预训练模型: models/model1/best_model.pt
2025-08-07 13:52:48,207 - training - INFO - ✅ 成功加载预训练模型权重
2025-08-07 13:52:48,538 - training - INFO - 🔄 模型初始化完成, 结构: TransformerModel(
  (input_fc): Linear(in_features=773, out_features=1024, bias=True)
  (layers): ModuleList(
    (0-15): 16 x TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=5012, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=5012, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (out_proj): Linear(in_features=1024, out_features=773, bias=True)
)
2025-08-07 13:52:48,539 - training - INFO - ⚙️ 模型运行在: cuda
2025-08-07 13:52:48,550 - training - INFO - 📦 加载训练数据: /home/liyakun/twitter-stock-prediction/data/splits/train/
2025-08-07 13:52:49,160 - training - INFO - ℹ️ 使用SmoothL1Loss作为损失函数，对异常值更鲁棒
